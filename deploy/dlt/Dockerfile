FROM python:3.11.11-alpine

# Metadata
LABEL org.label-schema.vendor="dltHub" \
    org.label-schema.url="https://dlthub.com" \
    org.label-schema.name="dlt" \
    org.label-schema.description="**data load tool (dlt)** is a simple, open source Python library that makes data loading easy."

# prepare dirs to install dlt
RUN mkdir -p /tmp/pydlt

WORKDIR /tmp/pydlt

# generated by make recreate-compiled-deps to install packages requiring compiler
# recreate only when you have new deps requiring compilation - step below is very slow
ADD compiled_requirements.txt .

# install alpine deps
RUN apk update &&\
    apk add --no-cache ca-certificates curl postgresql git &&\
    apk add --no-cache --virtual build-deps build-base automake autoconf libtool python3-dev postgresql-dev libffi-dev linux-headers gcc musl-dev cmake &&\
    curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py && python3 get-pip.py &&\
    rm get-pip.py &&\
    pip install --upgrade setuptools wheel pip &&\
    rm -r /usr/lib/python*/ensurepip

# install arrow 17.0.0, usually we would need apache-arrow-dev=17.0.0 but it is not available in alpine 3.20
# adapt this version to the arrow version you need
RUN git clone --no-checkout https://github.com/apache/arrow.git /arrow \
    && cd /arrow \
    && git checkout tags/apache-arrow-17.0.0 \
    && cd cpp \
    && mkdir build \
    && cd build \
    && cmake -DARROW_CSV=ON -DARROW_JSON=ON -DARROW_FILESYSTEM=ON .. \
    && make -j$(nproc) \
    && make install

RUN pip install -r compiled_requirements.txt &&\
    apk del --purge build-deps


# add build labels and envs
ARG COMMIT_SHA=""
ARG IMAGE_VERSION=""
LABEL commit_sha=${COMMIT_SHA}
LABEL version=${IMAGE_VERSION}
ENV COMMIT_SHA=${COMMIT_SHA}
ENV IMAGE_VERSION=${IMAGE_VERSION}

# install exactly the same version of the library we used to build
COPY dist/dlt-${IMAGE_VERSION}.tar.gz .
RUN pip install /tmp/pydlt/dlt-${IMAGE_VERSION}.tar.gz[gcp,redshift,duckdb]

WORKDIR /
RUN rm -r /tmp/pydlt
