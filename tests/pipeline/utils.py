import posixpath
from typing import Any, Dict, List, Tuple
import pytest
import random
from os import environ
import io

import dlt
from dlt.common import json, sleep
from dlt.common.pipeline import LoadInfo
from dlt.common.schema.typing import LOADS_TABLE_NAME
from dlt.common.typing import DictStrAny
from dlt.destinations.impl.filesystem.filesystem import FilesystemClient
from dlt.pipeline.exceptions import SqlClientNotAvailable
from dlt.common.storages import FileStorage
from tests.utils import TEST_STORAGE_ROOT

PIPELINE_TEST_CASES_PATH = "./tests/pipeline/cases/"


@pytest.fixture(autouse=True)
def drop_dataset_from_env() -> None:
    if "DATASET_NAME" in environ:
        del environ["DATASET_NAME"]


def assert_load_info(info: LoadInfo, expected_load_packages: int = 1) -> None:
    """Asserts that expected number of packages was loaded and there are no failed jobs"""
    assert len(info.loads_ids) == expected_load_packages
    # all packages loaded
    assert all(p.completed_at is not None for p in info.load_packages) is True
    # no failed jobs in any of the packages
    assert all(len(p.jobs["failed_jobs"]) == 0 for p in info.load_packages) is True


def json_case_path(name: str) -> str:
    return f"{PIPELINE_TEST_CASES_PATH}{name}.json"


def load_json_case(name: str) -> DictStrAny:
    with open(json_case_path(name), "rb") as f:
        return json.load(f)


def load_table_counts(p: dlt.Pipeline, *table_names: str) -> DictStrAny:
    """Returns row counts for `table_names` as dict"""

    # try sql, could be other destination though
    try:
        with p.sql_client() as c:
            qualified_names = [c.make_qualified_table_name(name) for name in table_names]
            query = "\nUNION ALL\n".join(
                [
                    f"SELECT '{name}' as name, COUNT(1) as c FROM {q_name}"
                    for name, q_name in zip(table_names, qualified_names)
                ]
            )
            with c.execute_query(query) as cur:
                rows = list(cur.fetchall())
                return {r[0]: r[1] for r in rows}
    except SqlClientNotAvailable:
        pass

    # try filesystem
    file_tables = load_files(p, *table_names)
    result = {}
    for table_name, items in file_tables.items():
        result[table_name] = len(items)
    return result


def load_data_table_counts(p: dlt.Pipeline) -> DictStrAny:
    tables = [table["name"] for table in p.default_schema.data_tables()]
    return load_table_counts(p, *tables)


def assert_data_table_counts(p: dlt.Pipeline, expected_counts: DictStrAny) -> None:
    table_counts = load_data_table_counts(p)
    assert (
        table_counts == expected_counts
    ), f"Table counts do not match, expected {expected_counts}, got {table_counts}"


def load_file(fs_client, path: str, file: str) -> Tuple[str, List[Dict[str, Any]]]:
    """
    util function to load a filesystem destination file and return parsed content
    values may not be cast to the right type, especially for insert_values, please
    make sure to do conversions and casting if needed in your tests
    """
    result: List[Dict[str, Any]] = []

    # check if this is a file we want to read
    file_name_items = file.split(".")
    ext = file_name_items[-1]
    if ext not in ["jsonl", "insert_values", "parquet"]:
        return "skip", []

    # table name will be last element of path
    table_name = path.split("/")[-1]
    full_path = posixpath.join(path, file)

    # load jsonl
    if ext == "jsonl":
        file_text = fs_client.read_text(full_path)
        for line in file_text.split("\n"):
            if line:
                result.append(json.loads(line))

    # load insert_values (this is a bit volatile if the exact format of the source file changes)
    elif ext == "insert_values":
        file_text = fs_client.read_text(full_path)
        lines = file_text.split("\n")
        cols = lines[0][15:-2].split(",")
        for line in lines[2:]:
            if line:
                values = line[1:-3].split(",")
                result.append(dict(zip(cols, values)))

    # load parquet
    elif ext == "parquet":
        import pyarrow.parquet as pq

        file_bytes = fs_client.read_bytes(full_path)
        table = pq.read_table(io.BytesIO(file_bytes))
        cols = table.column_names
        count = 0
        for column in table:
            column_name = cols[count]
            item_count = 0
            for item in column.to_pylist():
                if len(result) <= item_count:
                    result.append({column_name: item})
                else:
                    result[item_count][column_name] = item
                item_count += 1
            count += 1

    return table_name, result


def load_files(p: dlt.Pipeline, *table_names: str) -> Dict[str, List[Dict[str, Any]]]:
    """For now this will expect the standard layout in the filesystem destination, if changed the results will not be correct"""
    client: FilesystemClient = p.destination_client()  # type: ignore[assignment]
    result: Dict[str, Any] = {}
    for basedir, _dirs, files in client.fs_client.walk(
        client.dataset_path, detail=False, refresh=True
    ):
        for file in files:
            table_name, items = load_file(client.fs_client, basedir, file)
            if table_name not in table_names:
                continue
            if table_name in result:
                result[table_name] = result[table_name] + items
            else:
                result[table_name] = items

    return result


def load_tables_to_dicts(p: dlt.Pipeline, *table_names: str) -> Dict[str, List[Dict[str, Any]]]:
    # try sql, could be other destination though
    try:
        result = {}
        for table_name in table_names:
            table_rows = []
            columns = p.default_schema.get_table_columns(table_name).keys()
            query_columns = ",".join(map(p.sql_client().capabilities.escape_identifier, columns))

            with p.sql_client() as c:
                query_columns = ",".join(map(c.escape_column_name, columns))
                f_q_table_name = c.make_qualified_table_name(table_name)
                query = f"SELECT {query_columns} FROM {f_q_table_name}"
                with c.execute_query(query) as cur:
                    for row in list(cur.fetchall()):
                        table_rows.append(dict(zip(columns, row)))
            result[table_name] = table_rows
        return result

    except SqlClientNotAvailable:
        pass

    # try files
    return load_files(p, *table_names)


def load_table_distinct_counts(
    p: dlt.Pipeline, distinct_column: str, *table_names: str
) -> DictStrAny:
    """Returns counts of distinct values for column `distinct_column` for `table_names` as dict"""
    query = "\nUNION ALL\n".join(
        [
            f"SELECT '{name}' as name, COUNT(DISTINCT {distinct_column}) as c FROM {name}"
            for name in table_names
        ]
    )
    with p.sql_client() as c:
        with c.execute_query(query) as cur:
            rows = list(cur.fetchall())
            return {r[0]: r[1] for r in rows}


@dlt.source
def airtable_emojis():
    @dlt.resource(name="ðŸ“† Schedule")
    def schedule():
        yield [1, 2, 3]

    @dlt.resource(name="ðŸ’°Budget", primary_key=("ðŸ”‘book_id", "asset_id"))
    def budget():
        # return empty
        yield

    @dlt.resource(name="ðŸ¦šPeacock", selected=False, primary_key="ðŸ”‘id")
    def peacock():
        dlt.current.resource_state()["ðŸ¦šðŸ¦šðŸ¦š"] = "ðŸ¦š"
        yield [{"peacock": [1, 2, 3], "ðŸ”‘id": 1}]

    @dlt.resource(name="ðŸ¦šWidePeacock", selected=False)
    def wide_peacock():
        yield [{"peacock": [1, 2, 3]}]

    return budget, schedule, peacock, wide_peacock


def run_deferred(iters):
    @dlt.defer
    def item(n):
        sleep(random.random() / 2)
        return n

    for n in range(iters):
        yield item(n)


@dlt.source
def many_delayed(many, iters):
    for n in range(many):
        yield dlt.resource(run_deferred(iters), name="resource_" + str(n))
