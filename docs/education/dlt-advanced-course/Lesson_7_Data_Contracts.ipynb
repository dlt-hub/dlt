{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wat0fkM3BHwn"
      },
      "source": [
        "# **Introduction** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/6be4aaac807414ae6100691174c5babcd6a87736/docs/education/dlt-advanced-course/Lesson_7_Data_Contracts.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/6be4aaac807414ae6100691174c5babcd6a87736/docs/education/dlt-advanced-course/Lesson_7_Data_Contracts.ipynb)\n",
        "\n",
        "`dlt` offers powerful tools for schema configuration, giving you control over your data processing. You can export and import schemas for easy adjustments and apply specific settings directly to resources for precise data normalization. Plus, you can set data contracts to ensure your data meets your expectations... 👀\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Lesson_7_Data_Contracts_img1](https://storage.googleapis.com/dlt-blog-images/dlt-advanced-course/Lesson_7_Data_Contracts_img1.webp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhu0CzNENgvx"
      },
      "source": [
        "# [Refresher] **Understanding schema**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUPl9BiAGKVA"
      },
      "source": [
        "When you run a pipeline, `dlt` internally generates a `<>.schema.json` file. You can export this file to a specific location in YAML format by specifying `export_schema_path=\"schemas/export\"` in your pipeline.\n",
        "\n",
        "See [dlt Fundamentals: Lesson 7](https://colab.research.google.com/drive/1LokUcM5YSazdq5jfbkop-Z5rmP-39y4r#forceEdit=true&sandboxMode=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S26RGt8YMzJZ"
      },
      "source": [
        "This YAML file will look something like:\n",
        "\n",
        "```yaml\n",
        "version: 2 # version of the schema\n",
        "version_hash: xmTG0tOmE40LvzY2DbPBOnRaNNK8YlLpVP1PMO0YgyE= # hash of the actual schema content\n",
        "engine_version: 9. # shema engine version of dlt\n",
        "name: quick_start\n",
        "tables:\n",
        "  _dlt_version:\n",
        "    ...\n",
        "  _dlt_loads:\n",
        "    ...\n",
        "  _dlt_pipeline_state:\n",
        "    ...\n",
        "  issues:\n",
        "    columns:\n",
        "      url:\n",
        "        data_type: text\n",
        "        nullable: true\n",
        "      repository_url:\n",
        "        data_type: text\n",
        "        nullable: true\n",
        "      labels_url:\n",
        "        data_type: text\n",
        "        nullable: true\n",
        "      ...\n",
        "    write_disposition: append\n",
        "    resource: get_issues\n",
        "    x-normalizer:\n",
        "      seen-data: true\n",
        "  issues__assignees:\n",
        "    columns:\n",
        "      ...\n",
        "    parent: issues\n",
        "\n",
        "settings:\n",
        "  detections:\n",
        "  - iso_timestamp\n",
        "  default_hints:\n",
        "    not_null:\n",
        "    - _dlt_id\n",
        "    - _dlt_root_id\n",
        "    - _dlt_parent_id\n",
        "    - _dlt_list_idx\n",
        "    - _dlt_load_id\n",
        "    foreign_key:\n",
        "    - _dlt_parent_id\n",
        "    root_key:\n",
        "    - _dlt_root_id\n",
        "    unique:\n",
        "    - _dlt_id\n",
        "normalizers:\n",
        "  names: snake_case # naming convention\n",
        "  json:\n",
        "    module: dlt.common.normalizers.json.relational\n",
        "previous_hashes:\n",
        "- O4M6U4KA32Xz4Vrdcqo4XPBPFVcK1FZbgRu5qcMfjn4=\n",
        "- 0DQRnVWANYV21yD0T5nsoUtdTeq0/jIOYMUxpPE6Fcc=\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkS2uDrg0FXh"
      },
      "source": [
        "## **Tables and columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ou9Ns3h2cjj"
      },
      "source": [
        "A `table schema` may have the following properties:\n",
        "\n",
        "- `name`\n",
        "- `description`\n",
        "- `parent`: The name of the parent table if this is a child table.\n",
        "- `columns`: A list of column schemas defining the table's structure.\n",
        "- `write_disposition`: A hint telling `dlt` how new data coming into the table should be loaded.\n",
        "\n",
        "\n",
        "A `column schema` may have the following properties:\n",
        "\n",
        "- `name`\n",
        "- `description`\n",
        "- `data_type`\n",
        "- `precision`: Defines the precision for text, timestamp, time, bigint, binary, and decimal types.\n",
        "- `scale`: Defines the scale for the decimal type.\n",
        "- `is_variant`: Indicates that the column was generated as a variant of another column.\n",
        "\n",
        "A `column schema` may have the following basic hints:\n",
        "\n",
        "- `nullable`\n",
        "- `primary_key`\n",
        "- `merge_key`: Marks the column as part of the merge key used for incremental loads.\n",
        "- `foreign_key`\n",
        "- `root_key`: Marks the column as part of a root key, a type of foreign key that always refers to the root table.\n",
        "- `unique`\n",
        "\n",
        "\n",
        "A `column schema` may have the following performance hints:\n",
        "\n",
        "- `partition`: Marks the column to be used for partitioning data.\n",
        "- `cluster`: Marks the column to be used for clustering data.\n",
        "- `sort`: : Marks the column as sortable or ordered; on some destinations, this may generate an index, even if the column is not unique.\n",
        "\n",
        "> Each destination can interpret these performance hints in its own way. For example, the `cluster` hint is used by Redshift to define table distribution, by BigQuery to specify a cluster column, and is ignored by DuckDB and Postgres when creating tables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffESiO5DRHM1"
      },
      "source": [
        "# **Data contracts**\n",
        "\n",
        "Data contracts are rules that help control how your data schema changes over time. They are particularly useful for maintaining the integrity and consistency of your data as it evolves.\n",
        "\n",
        "`dlt` allows you to implement these data contracts at various levels, including the [table level](#scrollTo=zzVNMHgqNEYr), [column level](#scrollTo=Bq_9SNOMQGk_), and [data type level](#scrollTo=H9eMPvlOQHrJ). This provides granular control over how different parts of your schema evolve.\n",
        "\n",
        "> **Note**: This Colab is based on `dlt`'s [schema contracts doc page](https://dlthub.com/docs/general-usage/schema-contracts) and includes additional code examples. It's still a good idea to check out the doc page for all the details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2XDHclpusOU"
      },
      "source": [
        "To get started with data contracts, first install `dlt`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EnMwnGgmd2-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Install dlt\n",
        "!pip install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzVNMHgqNEYr"
      },
      "source": [
        "###**Table level**\n",
        "\n",
        "On the table level, you can specify `evolve` or `freeze` as part of the schema contract.\n",
        "\n",
        "- `evolve`: Allows the creation of new tables within the schema.\n",
        "- `freeze`: Prevents any changes to the schema, ensuring no new tables can be added."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23S-mVEG5AlP"
      },
      "source": [
        "Before diving into the modes above, let's load some sample data into a DuckDB database.\n",
        "  > You'll find the database stored in the `Files` section on the left sidebar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krkIwLm6o33M"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "# Sample data to be loaded\n",
        "data = [\n",
        "    {\"id\": 1, \"name\": \"Alice\"},\n",
        "    {\"id\": 2, \"name\": \"Bob\"}]\n",
        "\n",
        "# Create a dlt pipeline\n",
        "table_pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"data_contracts_table_level\", destination=\"duckdb\", dataset_name=\"mydata\"\n",
        ")\n",
        "\n",
        "# Load the data to the \"users\" table\n",
        "load_info = table_pipeline.run(data, table_name=\"users\")\n",
        "print(load_info)\n",
        "\n",
        "# Print the row counts for each table that was loaded in the last run of the pipeline\n",
        "print(\"\\nNumber of new rows loaded into each table: \", table_pipeline.last_trace.last_normalize_info.row_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyVN-Nz95_9Z"
      },
      "source": [
        "Now, try out the `evolve` mode at the table level by loading the same sample data into the same database, but this time into a new table called `new_users`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0pCFrsepLdN"
      },
      "outputs": [],
      "source": [
        "# Define a dlt resource that allows the creation of new tables\n",
        "@dlt.resource(schema_contract={\"tables\": \"evolve\"})\n",
        "def allow_new_tables(input_data):\n",
        "  yield input_data\n",
        "\n",
        "# Run the pipeline again with the above dtl resource to load the same data into a new table \"new_users\"\n",
        "load_info = table_pipeline.run(allow_new_tables(data), table_name=\"new_users\")\n",
        "print(load_info)\n",
        "\n",
        "# Print the row counts for each table that was loaded in the last run of the pipeline\n",
        "print(\"\\nNumber of new rows loaded into each table: \", table_pipeline.last_trace.last_normalize_info.row_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS3aEw5r67aO"
      },
      "source": [
        "The `freeze` mode at the table level, as mentioned earlier, won't allow any changes to the schema, so the pipeline run below that tries to create another table with the name `newest_users` will fail 👇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaEsbtMQpUbJ"
      },
      "outputs": [],
      "source": [
        "# Define a dlt resource that prevents any changes to the schema at the table level (no new tables can be added)\n",
        "@dlt.resource(schema_contract={\"tables\": \"freeze\"})\n",
        "def no_new_tables(input_data):\n",
        "  yield input_data\n",
        "\n",
        "# Now, run the pipeline with the resource above, attempting to load the same data into \"newest_users\".\n",
        "# This will fail, as new tables can't be added.\n",
        "load_info = table_pipeline.run(no_new_tables(data), table_name=\"newest_users\")\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq_9SNOMQGk_"
      },
      "source": [
        "###**Column level**\n",
        "At the column level, you can specify:\n",
        "- `evolve`: Allows for the addition of new columns or changes in the existing ones.\n",
        "- `freeze`: Prevents any changes to the existing columns.\n",
        "- `discard_row`: Skips rows that have new columns but loads those that follow the existing schema.\n",
        "- `discard_value`: Doesn't skip entire rows. Instead, it only skips the values of new columns, loading the rest of the row data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drfSWPyZ8Ghy"
      },
      "source": [
        "Just like we did in the previous section, let's first load some sample data into a new database using a new pipeline.\n",
        "\n",
        "> After you run the following code snippet, a new `data_contracts_column_level.duckdb` file should appear in `Files`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-2-KkNm7fTo"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "# Create a new pipeline\n",
        "column_pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"data_contracts_column_level\", destination=\"duckdb\", dataset_name=\"mydata\"\n",
        ")\n",
        "\n",
        "# Load the initial data containing columns \"id\" and \"name\" into the \"users\" table\n",
        "load_info = column_pipeline.run([{\"id\": 1, \"name\": \"Alice\"}], table_name=\"users\")\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avUxl3V28ghz"
      },
      "source": [
        "View the loaded data using `dlt`'s `sql_client()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cypli_T6RM1h"
      },
      "source": [
        "Alternatively, you can simply use the DuckDB client."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy2igQz_REbt"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "conn = duckdb.connect(f\"{column_pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "conn.sql(\"SELECT * FROM mydata.users\").df()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9GMElv9_Blt"
      },
      "source": [
        "Assume that Alice ☝️ is the first user at your imaginary company, and you have now decided to collect users' ages as well.\n",
        "\n",
        "When you load the information for your second user, Bob, who also provided his age 👇, the schema contract at the column level set to `evolve` will allow `dlt` to automatically adjust the schema in the destination database by adding a new column for \"age\".\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KAmFHew_B3e"
      },
      "outputs": [],
      "source": [
        "# Define dlt resource that allows new columns in the data\n",
        "@dlt.resource(schema_contract={\"columns\": \"evolve\"})\n",
        "def allow_new_columns(input_data):\n",
        "    yield input_data\n",
        "\n",
        "# Now, load a new row into the same table, \"users\", which includes an additional column \"age\"\n",
        "load_info = column_pipeline.run(allow_new_columns([{\"id\": 2, \"name\": \"Bob\", \"age\": 35}]), table_name=\"users\")\n",
        "print(load_info)\n",
        "\n",
        "# View the data that has been loaded\n",
        "print(\"\\n\")\n",
        "conn = duckdb.connect(f\"{column_pipeline.pipeline_name}.duckdb\")\n",
        "conn.sql(\"SELECT * FROM mydata.users\").df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOvsaGQWA8-S"
      },
      "source": [
        "Now, imagine your business partner, with whom you started the company, began requiring phone numbers from users. However, you weren't informed of this requriement and want to first load the data of users who provided their info before this change, i.e., users who did NOT provide their phone numbers.\n",
        "\n",
        "In this case, you would use the `discard_row` mode - which will only load Sam's data 👇 because he didn't provide a phone number, and therefore his data complies with the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-C9_YiRqA-8U"
      },
      "outputs": [],
      "source": [
        "# Define a dlt resource that skips rows that have new columns but loads those that follow the existing schema\n",
        "@dlt.resource(schema_contract={\"columns\": \"discard_row\"})\n",
        "def discard_row(input_data):\n",
        "   yield input_data\n",
        "\n",
        "# Attempt to load two additional rows. Only the row that follows the existing schema will be loaded\n",
        "load_info = column_pipeline.run(\n",
        "    discard_row([\n",
        "        {\"id\": 3, \"name\": \"Sam\", \"age\": 30}, # This row will be loaded\n",
        "        {\"id\": 4, \"name\": \"Kate\", \"age\": 79, \"phone\": \"123-456-7890\"} # This row will not be loaded\n",
        "    ]),\n",
        "    table_name=\"users\"\n",
        ")\n",
        "print(load_info)\n",
        "\n",
        "# View the data that has been loaded\n",
        "print(\"\\n\")\n",
        "conn = duckdb.connect(f\"{column_pipeline.pipeline_name}.duckdb\")\n",
        "conn.sql(\"SELECT * FROM mydata.users\").df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZnhGIgvCqN8"
      },
      "source": [
        "Due to some unknown reasons, you've suddenly decided that phone numbers are irrelevant altogether. From now on, you want to load all new data but without the \"phone\" column.\n",
        "\n",
        "To achieve this, you can use the `discard_value` mode - which will load both Sarah's and Violetta's data 👇, regardless of whether either of them provided a phone number. However, the phone number column itself will be discarded.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfE78ObkCqdQ"
      },
      "outputs": [],
      "source": [
        "# Define a dlt resource that only skips the values of new columns, loading the rest of the row data\n",
        "@dlt.resource(schema_contract={\"columns\": \"discard_value\"})\n",
        "def discard_value(input_data):\n",
        "   yield input_data\n",
        "\n",
        "# Load two additional rows. Since we're using the \"discard_value\" resource, both rows will be added\n",
        "# However, the \"phone\" column in the second row will be ignored and not loaded\n",
        "load_info = column_pipeline.run(\n",
        "    discard_value([\n",
        "        {\"id\": 5, \"name\": \"Sarah\", \"age\": \"23\"},\n",
        "        {\"id\": 6, \"name\": \"Violetta\", \"age\": \"22\", \"phone\": \"666-513-4510\"}\n",
        "    ]),\n",
        "    table_name=\"users\"\n",
        ")\n",
        "print(load_info)\n",
        "\n",
        "# View the data that has been loaded\n",
        "print(\"\\n\")\n",
        "conn = duckdb.connect(f\"{column_pipeline.pipeline_name}.duckdb\")\n",
        "conn.sql(\"SELECT * FROM mydata.users\").df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSfpW8NEOHUL"
      },
      "source": [
        "Eventually you decide that users' id, name and age are the only things you need for your obscure business...\n",
        "\n",
        "So, you set the mode to `freeze`, forbidding any changes to the table schema. The attempt to violate the schema contract, as shown below 👇, will fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is1tTWaLOHqZ"
      },
      "outputs": [],
      "source": [
        "# Define a dlt resource that does not allow new columns in the data\n",
        "@dlt.resource(schema_contract={\"columns\": \"freeze\"})\n",
        "def no_new_columns(input_data):\n",
        "  yield input_data\n",
        "\n",
        "# Attempt to load a row with additional columns when the column contract is set to freeze\n",
        "# This will fail as no new columns are allowed.\n",
        "load_info = column_pipeline.run(\n",
        "    no_new_columns([\n",
        "        {\"id\": 7, \"name\": \"Lisa\", \"age\": 40, \"phone\": \"098-765-4321\"}\n",
        "    ]),\n",
        "    table_name=\"users\"\n",
        ")\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9eMPvlOQHrJ"
      },
      "source": [
        "### **Data type level**\n",
        "At this level, you can choose:\n",
        "- `evolve`: Allows any data type. This may result with variant columns upstream.\n",
        "- `freeze`: Prevents any changes to the existing data types.\n",
        "- `discard_row`: Omits rows with unverifiable data types.\n",
        "- `discard_value`: Replaces unverifiable values with None, but retains the rest of the row data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTbCuvArn3GX"
      },
      "source": [
        "(*No imaginary situations in this section for the sake of variety and ease* ... 👀)\n",
        "\n",
        "Load a sample row entry into a new database using a new pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V4WLhhGlZZQ"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "# Create a pipeline for loading data\n",
        "data_type_pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"data_contracts_data_type\", destination=\"duckdb\", dataset_name=\"mydata\"\n",
        ")\n",
        "\n",
        "# Load the initial data containing a column \"age\" of type int\n",
        "load_info = data_type_pipeline.run([{\"id\": 1, \"name\": \"Alice\", \"age\": 24}], table_name=\"users\")\n",
        "print(load_info)\n",
        "\n",
        "# View the data that has been loaded\n",
        "print(\"\\n\")\n",
        "conn = duckdb.connect(f\"{data_type_pipeline.pipeline_name}.duckdb\")\n",
        "conn.sql(\"SELECT * FROM mydata.users\").df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKUc_2RsrltC"
      },
      "source": [
        "Before trying out the `evolve` mode at the data type level 👇, take a moment to understand how variant columns mentioned earlier are created:\n",
        "- **TLDR:** `dlt` creates a new column when the data type of a field in the incoming data can't be validated against the existing data type in the destination table.\n",
        "- These variant columns will be named following the pattern `<original name>__v_<type>`, where `original_name` is the existing column name (with the data type clash) and `type` is the name of the new data type stored in the variant column.\n",
        "\n",
        "In the example below, even though Bob's age is passed as a string, it can be validated as an integer, so it won't cause any problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMY6yoxMl1KM"
      },
      "outputs": [],
      "source": [
        "# Define dlt resource that accepts all data types\n",
        "@dlt.resource(schema_contract={\"data_type\": \"evolve\"})\n",
        "def allow_any_data_type(input_data):\n",
        "    yield input_data\n",
        "\n",
        "# Now, load a new row where the \"age\" column is passed as a string but will be validated and stored as an integer\n",
        "load_info = data_type_pipeline.run(allow_any_data_type([{\"id\": 2, \"name\": \"Bob\", \"age\": \"35\"}]), table_name=\"users\")\n",
        "print(load_info)\n",
        "\n",
        "# If you pass the age as \"thirty-five\", a new variant column will be added\n",
        "# Note: Running the uncommented code below may affect subsequent steps, so proceed with caution\n",
        "#load_info = data_type_pipeline.run(allow_any_data_type([{\"id\": 2, \"name\": \"Bob\", \"age\": \"thirty-five\"}]), table_name=\"users\")\n",
        "#print(load_info)\n",
        "\n",
        "# View the data that has been loaded\n",
        "print(\"\\n\")\n",
        "conn = duckdb.connect(f\"{data_type_pipeline.pipeline_name}.duckdb\")\n",
        "conn.sql(\"SELECT * FROM mydata.users\").df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goR1zSoThwtp"
      },
      "source": [
        "But if we ran the commented-out pipeline, this would be the outcome with an additional variant column:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Lesson_7_Data_Contracts_img2](https://storage.googleapis.com/dlt-blog-images/dlt-advanced-course/Lesson_7_Data_Contracts_img2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xez3JQdXsmGN"
      },
      "source": [
        "The `discard_row` mode at the data type level functions similarly to how it does at the column level. The only difference is that it discards rows with diverging data types instead of columns. As a result, you will see that Kate's data will not be loaded 👇."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPYDmrBMl-WT"
      },
      "outputs": [],
      "source": [
        "# Define dlt resource that omits rows with unverifiable data types\n",
        "@dlt.resource(schema_contract={\"data_type\": \"discard_row\"})\n",
        "def discard_row(input_data):\n",
        "   yield input_data\n",
        "\n",
        "# Attempt to load two additional rows. Only the row where all column types can be validated will be loaded\n",
        "load_info = data_type_pipeline.run(\n",
        "    discard_row([\n",
        "        {\"id\": 3, \"name\": \"Sam\", \"age\": \"35\"}, # This row will be loaded\n",
        "        {\"id\": 4, \"name\": \"Kate\", \"age\": \"seventy\"} # This row will not be loaded\n",
        "    ]),\n",
        "    table_name=\"users\"\n",
        ")\n",
        "print(load_info)\n",
        "\n",
        "# View the data that has been loaded\n",
        "print(\"\\n\")\n",
        "conn = duckdb.connect(f\"{data_type_pipeline.pipeline_name}.duckdb\")\n",
        "conn.sql(\"SELECT * FROM mydata.users\").df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMQtARoZtJqy"
      },
      "source": [
        "The same goes for the `discard_value` mode. However, note that when applied at the data type level, it will replace non-validating row items with `None`. So, in this example, Violetta's age will be set to `None` 👇."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_svh8fcemFWM"
      },
      "outputs": [],
      "source": [
        "# Define a dlt resource that replaces unverifiable values with None, but retains the rest of the row data\n",
        "@dlt.resource(schema_contract={\"data_type\": \"discard_value\"})\n",
        "def discard_value(input_data):\n",
        "   yield input_data\n",
        "\n",
        "# Load two additional rows. Since we're using the \"discard_value\" resource, both rows will be added\n",
        "# However, the \"age\" value \"twenty-eight\" in the second row will be ignored and not loaded\n",
        "load_info = data_type_pipeline.run(\n",
        "    discard_value([\n",
        "        {\"id\": 5, \"name\": \"Sarah\", \"age\": 23},\n",
        "        {\"id\": 6, \"name\": \"Violetta\", \"age\": \"twenty-eight\"}\n",
        "    ]),\n",
        "    table_name=\"users\"\n",
        ")\n",
        "print(load_info)\n",
        "\n",
        "# View the data that has been loaded\n",
        "print(\"\\n\")\n",
        "conn = duckdb.connect(f\"{data_type_pipeline.pipeline_name}.duckdb\")\n",
        "conn.sql(\"SELECT * FROM mydata.users\").df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9peZC2aauRST"
      },
      "source": [
        "The `freeze` mode prohibits any changes to the data types of existing columns and will result in an error if there is a \"breach in contract\". The example below will fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n_fH2muxXmX"
      },
      "outputs": [],
      "source": [
        "# Define dlt resource that prevents any changes to the existing data types\n",
        "@dlt.resource(schema_contract={\"data_type\": \"freeze\"})\n",
        "def no_data_type_changes(input_data):\n",
        "  yield input_data\n",
        "\n",
        "# Attempt to load a row with a column value that can't be validated, in this case \"forty\"\n",
        "# This will fail as no data type changes are allowed with the \"no_data_type_changes\" resource\n",
        "load_info = data_type_pipeline.run(no_data_type_changes([{\"id\": 7, \"name\": \"Lisa\", \"age\": \"forty\"}]), table_name=\"users\")\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hw8-UV08eQK"
      },
      "source": [
        "# **Pydantic Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAVX-VPq7XQA"
      },
      "source": [
        "Pydantic models can also be used to [define table schemas and validate incoming data](https://dlthub.com/docs/general-usage/resource#define-a-schema-with-pydantic).\n",
        "They can be passed directly to the \"columns\" argument of a `dlt` resource:\n",
        "```python\n",
        "class User(BaseModel):\n",
        "    id: int\n",
        "    name: str\n",
        "    tags: List[str]\n",
        "    email: Optional[str]\n",
        "    address: Address\n",
        "    status: Union[int, str]\n",
        "\n",
        "@dlt.resource(name=\"user\", columns=User)\n",
        "def get_users():\n",
        "    ...\n",
        "```\n",
        "This will set the schema contract to align with the default Pydantic behavior:\n",
        "```python\n",
        "{\n",
        "  \"tables\": \"evolve\",\n",
        "  \"columns\": \"discard_value\",\n",
        "  \"data_type\": \"freeze\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1eXIou2BG9H"
      },
      "source": [
        "If you happen to pass a `schema_contract` explicitly along with the `columns` argument to a `dlt` resource, the following happens:\n",
        "\n",
        "- `tables`: The contract will not impact the Pydantic model and will be applied when a new table is created.\n",
        "- `columns`: The modes for columns are mapped into the `extra` modes of Pydantic. If your models contain other models, `dlt` will apply this setting recursively. The contract for columns is applied when a new column is created on an existing table.\n",
        "\n",
        "<center>\n",
        "\n",
        "| Column Mode     | Pydantic Extra |\n",
        "|-----------------|----------------|\n",
        "| evolve          | allow          |\n",
        "| freeze          | forbid         |\n",
        "| discard_value   | ignore         |\n",
        "| discard_row     | forbid         |\n",
        "\n",
        "</center>\n",
        "\n",
        "- `data_type`: This supports the following modes for Pydantic:\n",
        "  1. `evolve` will synthesize a lenient model that allows for any data type. It may result in variant columns upstream.\n",
        "  2. `freeze` will re-raise a ValidationException.\n",
        "  3. `discard_row` will remove the non-validating data items.\n",
        "  4. `discard_value` is not currently supported.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRE2s2CtEau1"
      },
      "source": [
        "# **Good to Know**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B-snaXbJDUr"
      },
      "source": [
        "- Unless you specify a schema contract, settings will default to `evolve` on all levels.\n",
        "\n",
        "- The `schema_contract` argument accepts two forms:\n",
        "  1. Full form: A detailed mapping of schema entities to their respective contract modes.\n",
        "  ```python\n",
        "  schema_contract={\"tables\": \"freeze\", \"columns\": \"freeze\", \"data_type\": \"freeze\"}\n",
        "  ```\n",
        "  2. Shorthand form: A single contract mode that will be uniformly applied to all schema entities.\n",
        "  ```python\n",
        "  schema_contract=\"freeze\"\n",
        "  ```\n",
        "\n",
        "- Schema contracts can be defined for:\n",
        "  1. `dlt` resources: The contract applies to the corresponding table and any child tables.\n",
        "  ```python\n",
        "  @dlt.resource(schema_contract={\"columns\": \"evolve\"})\n",
        "def items():\n",
        "        ...\n",
        "  ```\n",
        "  2. `dlt` sources: The contract serves as a default for all resources within that source.\n",
        "  ```python\n",
        "  @dlt.source(schema_contract=\"freeze\")\n",
        "def source():\n",
        "        ...\n",
        "  ```\n",
        "  3. The `pipeline.run()`: This contract overrides any existing schema contracts.\n",
        "  ```python\n",
        "  pipeline.run(source(), schema_contract=\"freeze\")\n",
        "  ```\n",
        "\n",
        "- You can change the contract on a `dlt` source via its `schema_contract` property.\n",
        "```python\n",
        "source = dlt.source(...)\n",
        "source.schema_contract = {\"tables\": \"evolve\", \"columns\": \"freeze\", \"data_type\": \"discard_row\"}\n",
        "```\n",
        "\n",
        "- To update the contract for `dlt` resources, use `apply_hints`.\n",
        "```python\n",
        "resource.apply_hints(schema_contract={\"tables\": \"evolve\", \"columns\": \"freeze\"})\n",
        "```\n",
        "\n",
        "- For the `discard_row` method at the table level, if there are two tables in a parent-child relationship, such as `users` and `users__addresses`, and the contract is violated in the child table, the row in the child table (`users__addresses`) will be discarded, while the corresponding parent row in the `users` table will still be loaded.\n",
        "\n",
        "- If a table is a `new table` that hasn't been created on the destination yet, `dlt` will allow the creation of new columns. During the first pipeline run, the column mode is temporarily changed to `evolve` and then reverted back to the original mode. Following tables are considered new:\n",
        "  1. Child tables inferred the nested data.\n",
        "  2. Dynamic tables created from the data during extraction.\n",
        "  3. Tables containing incomplete columns - columns without a data type bound to them.\n",
        "\n",
        "  > Note that tables with columns defined with Pydantic models are not considered new."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH3F46PaJZe4"
      },
      "source": [
        "✅ ▶ Proceed to the [next lesson](https://colab.research.google.com/drive/1YCjHWMyOO9QGC66t1a5bIxL-ZUeVKViR#forceEdit=true&sandboxMode=true)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_6WprxWXhXi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
