{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbFVutT06Cqq"
   },
   "source": [
    "# Transforming and filtering the data [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-advanced-course/lesson_5_transform_data_before_and_after_loading.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-advanced-course/lesson_5_transform_data_before_and_after_loading.ipynb)\n",
    "\n",
    "In this lesson, we will take a look at various ways of doing data transformations and filtering of the data during and after the ingestion.\n",
    "\n",
    "dlt provides several ways of doing it during the ingestion:\n",
    "1. With custom query (applicable for `sql_database` source).\n",
    "2. With dlt special functions (`add_map` and `add_filter`).\n",
    "3. Via `@dlt.transformers`.\n",
    "4. With `pipeline.dataset()`.\n",
    "\n",
    "Let's review and compare those methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBWy8vU2SXLK"
   },
   "source": [
    "##  What youâ€™ll learn:\n",
    "\n",
    "- How to limit rows at the source with SQL queries.\n",
    "- How to apply custom Python logic per record.\n",
    "- How to write transformations using functional and declarative APIs.\n",
    "- How to access and query your loaded data using `.dataset()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_ich6AVSiVx"
   },
   "source": [
    "## Setup and initial Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZCL61389Jgv"
   },
   "source": [
    "We will be using the `sql_database` source as an example and will connect to the public [MySQL RFam](https://www.google.com/url?q=https%3A%2F%2Fwww.google.com%2Furl%3Fq%3Dhttps%253A%252F%252Fdocs.rfam.org%252Fen%252Flatest%252Fdatabase.html) database. The RFam database contains publicly accessible scientific data on RNA structures.\n",
    "\n",
    "Let's perform an initial load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPgzqDkj7XK_"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U \"dlt[sql_database, duckdb]\"\n",
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQZDiQuEPH6W",
    "outputId": "28a081a9-97cb-4876-d235-debd1234389c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline sql_database_pipeline load step completed in 9.91 seconds\n",
      "1 load package(s) were loaded to destination duckdb and into dataset sql_data\n",
      "The duckdb destination used duckdb:////content/sql_database_pipeline.duckdb location to store data\n",
      "Load package 1753090463.4197385 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\"family\", \"genome\"],\n",
    ")\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    ")\n",
    "load_info = pipeline.run(source)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BV1lkPn8b3xe"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT * FROM genome\") as table:\n",
    "        genome = table.df()\n",
    "genome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Q_pqQZaS1MM"
   },
   "source": [
    "You can check your data count using `sql_client`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nM-D9TIniMRp"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT COUNT(*) AS total_rows FROM genome\") as table:\n",
    "        print(table.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMDR9uXGsy6s"
   },
   "source": [
    "## **1. Filtering the data during the ingestion with `query_adapter_callback`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edAUbOHXuwlL"
   },
   "source": [
    "Imagine a use-case where we're only interested in getting the genome data for bacterias. In this case, ingesting the whole `genome` table would be an unnecessary use of time and compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdA92C7Rss5j"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\n",
    "        \"SELECT COUNT(*) AS total_rows FROM genome WHERE kingdom='bacteria'\"\n",
    "    ) as table:\n",
    "        print(table.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB6ZDmLwxEgQ"
   },
   "source": [
    "When ingesting data using the `sql_database` source, dlt runs a `SELECT` statement in the back, and using the `query_adapter_callback` parameter makes it possible to pass a `WHERE` clause inside the underlying `SELECT` statement.  \n",
    "  \n",
    "In this example, only the table `genome` is filtered on the column `kingdom`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8A675ZXTCn9"
   },
   "outputs": [],
   "source": [
    "from dlt.sources.sql_database.helpers import Table, SelectAny, SelectClause\n",
    "\n",
    "\n",
    "def query_adapter_callback(query: SelectAny, table: Table) -> SelectAny:\n",
    "    if table.name == \"genome\":\n",
    "        # Only select rows where the column kingdom has value \"bacteria\"\n",
    "        return query.where(table.c.kingdom == \"bacteria\")\n",
    "    # Use the original query for other tables\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxG7ln2rTMSD"
   },
   "source": [
    "Attach it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKMNYkebv455"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\"genome\"],\n",
    "    query_adapter_callback=query_adapter_callback,\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_pipeline_filtered\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(source, write_disposition=\"replace\")\n",
    "\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPxw3vs7QaMt"
   },
   "source": [
    "In the snippet above we created an SQL VIEW in your source database and extracted data from it. In that case, dlt will infer all column types and read data in shape you define in a view without any further customization.\n",
    "\n",
    "If creating a view is not feasible, you can fully rewrite the automatically generated query with an extended version of `query_adapter_callback`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtFE8W9pRKEN"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import sqlalchemy as sa\n",
    "\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "\n",
    "def query_adapter_callback(query: SelectAny, table: Table) -> SelectClause:\n",
    "    if table.name == \"genome\":\n",
    "        return sa.text(f\"SELECT * FROM {table.fullname} WHERE kingdom='bacteria'\")\n",
    "    return query\n",
    "\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\"genome\", \"clan\"],\n",
    "    query_adapter_callback=query_adapter_callback,\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_pipeline_filtered\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(source, write_disposition=\"replace\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g183cHB-w0Sr"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\n",
    "        \"SELECT COUNT(*) AS total_rows, MAX(_dlt_load_id) as latest_load_id FROM clan\"\n",
    "    ) as table:\n",
    "        print(\"Table clan:\")\n",
    "        print(table.df())\n",
    "        print(\"\\n\")\n",
    "\n",
    "    with client.execute_query(\n",
    "        \"SELECT COUNT(*) AS total_rows, MAX(_dlt_load_id) as latest_load_id FROM genome\"\n",
    "    ) as table:\n",
    "        print(\"Table genome:\")\n",
    "        print(table.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf7xhp09yKZH"
   },
   "source": [
    "## **2. Transforming the data after extract and before load**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvakUcZu0PWW"
   },
   "source": [
    "Since dlt is a Python library, it gives you a lot of control over the extracted data.  \n",
    "\n",
    "You can attach any number of transformations that are evaluated on an item-per-item basis to your resource. The available transformation types:\n",
    "\n",
    "* `map` - transform the data item (resource.add_map).\n",
    "* `filter` - filter the data item (resource.add_filter).\n",
    "* `yield map` - a map that returns an iterator (so a single row may generate many rows - resource.add_yield_map).\n",
    "* `limit` - limits the number of records processed by a resource. Useful for testing or reducing data volume during development.\n",
    "  \n",
    "For example, if we wanted to anonymize sensitive data before it is loaded into the destination, then we can write a python function for it and apply it to source or resource using the `.add_map()` method.\n",
    "\n",
    "[dlt documentation.](https://dlthub.com/docs/general-usage/resource#filter-transform-and-pivot-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rykm1w8pIL0Z"
   },
   "source": [
    "### Using `add_map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emcVv9-z1V5v"
   },
   "source": [
    "In the table `clan`, we notice that there is a column `author` that we would like to anonymize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q88-DZA2SPq"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT DISTINCT author FROM clan LIMIT 5\") as table:\n",
    "        print(\"Table clan:\")\n",
    "        print(table.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA189SRX2oWM"
   },
   "source": [
    "We write a function in python that anonymizes a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EAOTQro16mq"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from dlt.common.typing import TDataItem\n",
    "\n",
    "\n",
    "def pseudonymize_name(row: TDataItem) -> TDataItem:\n",
    "    \"\"\"\n",
    "    Pseudonymization is a deterministic type of PII-obscuring.\n",
    "    Its role is to allow identifying users by their hash,\n",
    "    without revealing the underlying info.\n",
    "    \"\"\"\n",
    "    # add a constant salt to generate\n",
    "    salt = \"WI@N57%zZrmk#88c\"\n",
    "    salted_string = row[\"author\"] + salt\n",
    "    sh = hashlib.sha256()\n",
    "    sh.update(salted_string.encode())\n",
    "    hashed_string = sh.digest().hex()\n",
    "    row[\"author\"] = hashed_string\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zngIoF001bCM"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import hashlib\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_pipeline_anonymized\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    ")\n",
    "\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\", table_names=[\"clan\"]\n",
    ")\n",
    "\n",
    "source.clan.add_map(\n",
    "    pseudonymize_name\n",
    ")  # Apply the anonymization function to the extracted data\n",
    "\n",
    "info = pipeline.run(source)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHzvddVf3VXi"
   },
   "source": [
    "After the pipeline has run, we can observe that the author column has been anonymized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4lx8LTS1VZw"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT DISTINCT author FROM clan LIMIT 5\") as table:\n",
    "        print(\"Table clan:\")\n",
    "        clan = table.df()\n",
    "\n",
    "clan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn44WYZSTrch"
   },
   "source": [
    "**Note:** If you're using the `pyarrow` or `connectorx` backend, the data is not processed item-by-item. Instead they're processed in batches, therefore your function should be adjusted. For example, for PyArrow chunks the function could be changed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hVsp08csUOEu"
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "\n",
    "def pseudonymize_name_pyarrow(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Pseudonymizes the 'author' column in a PyArrow Table.\n",
    "    \"\"\"\n",
    "    salt = \"WI@N57%zZrmk#88c\"\n",
    "\n",
    "    # Convert PyArrow Table to Pandas DataFrame for hashing\n",
    "    df = table.to_pandas()\n",
    "\n",
    "    # Apply SHA-256 hashing\n",
    "    df[\"author\"] = (\n",
    "        df[\"author\"]\n",
    "        .astype(str)\n",
    "        .apply(lambda x: hashlib.sha256((x + salt).encode()).hexdigest())\n",
    "    )\n",
    "\n",
    "    # Convert back to PyArrow Table\n",
    "    new_table = pa.Table.from_pandas(df)\n",
    "\n",
    "    return new_table\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_pipeline_anonymized1\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    ")\n",
    "\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\"clan\"],\n",
    "    backend=\"pyarrow\",\n",
    ")\n",
    "\n",
    "source.clan.add_map(\n",
    "    pseudonymize_name_pyarrow\n",
    ")  # Apply the anonymization function to the extracted data\n",
    "\n",
    "info = pipeline.run(source)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMeZ3CkBVOlG"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT DISTINCT author FROM clan LIMIT 5\") as table:\n",
    "        print(\"Table clan:\")\n",
    "        print(table.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYQRFJG0ewX0"
   },
   "source": [
    "### `add_map` vs `add_yield_map`\n",
    "\n",
    "The difference between `add_map` and `add_yield_map` matters when a transformation returns multiple records from a single input.\n",
    "\n",
    "#### **`add_map`**\n",
    "- Use `add_map` when you want to transform each item into exactly one item.\n",
    "- Think of it like modifying or enriching a row.\n",
    "- You use a regular function that returns one modified item.\n",
    "- Great for adding fields or changing structure.\n",
    "\n",
    "#### Example\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTzgypHxbtxx"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.common.typing import TDataItems\n",
    "\n",
    "\n",
    "@dlt.resource\n",
    "def resource() -> TDataItems:\n",
    "    yield [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}]\n",
    "\n",
    "\n",
    "def add_greeting(item: TDataItem) -> TDataItem:\n",
    "    item[\"greeting\"] = f\"Hello, {item['name']}!\"\n",
    "    return item\n",
    "\n",
    "\n",
    "resource.add_map(add_greeting)\n",
    "\n",
    "for row in resource():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci7EmdZHb284"
   },
   "source": [
    "#### **`add_yield_map`**\n",
    "- Use `add_yield_map` when you want to turn one item into multiple items, or possibly no items.\n",
    "- Your function is a generator that uses yield.\n",
    "- Great for pivoting nested data, flattening lists, or filtering rows.\n",
    "\n",
    "#### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IVthhcRb-BK"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "@dlt.resource\n",
    "def resource() -> TDataItems:\n",
    "    yield [\n",
    "        {\"name\": \"Alice\", \"hobbies\": [\"reading\", \"chess\"]},\n",
    "        {\"name\": \"Bob\", \"hobbies\": [\"cycling\"]},\n",
    "    ]\n",
    "\n",
    "\n",
    "def expand_hobbies(item: TDataItem) -> TDataItem:\n",
    "    for hobby in item[\"hobbies\"]:\n",
    "        yield {\"name\": item[\"name\"], \"hobby\": hobby}\n",
    "\n",
    "\n",
    "resource.add_yield_map(expand_hobbies)\n",
    "\n",
    "for row in resource():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToPUZRRiVz-b"
   },
   "source": [
    "### Using `add_filter`\n",
    "`add_filter` function can be used similarly. The difference is that `add_filter` expects a function that returns a boolean value for each item. For example, to implement the same filtering we did with a query callback, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iS71P_gVzNI"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import dlt\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\"genome\"],\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_pipeline_filtered\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    ")\n",
    "source.genome.add_filter(lambda item: item[\"kingdom\"] == \"bacteria\")\n",
    "\n",
    "load_info = pipeline.run(source, write_disposition=\"replace\")\n",
    "\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3uu4UyeVUTi"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\n",
    "        \"SELECT COUNT(*) AS total_rows, MAX(_dlt_load_id) as latest_load_id FROM genome\"\n",
    "    ) as table:\n",
    "        print(\"Table genome:\")\n",
    "        genome_count = table.df()\n",
    "genome_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8CSV-TSK4jz"
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "What is a `total_rows` in the example above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3Z4V_npgtGL"
   },
   "source": [
    "### Using `add_limit`\n",
    "\n",
    "If your resource loads thousands of pages of data from a REST API or millions of rows from a database table, you may want to sample just a fragment of it in order to quickly see the dataset with example data and test your transformations, etc.\n",
    "\n",
    "To do this, you limit how many items will be yielded by a resource (or source) by calling the `add_limit` method. This method will close the generator that produces the data after the limit is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWMkMpizhCTn"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import dlt\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\"genome\"],\n",
    "    chunk_size=10,\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_pipeline_filtered\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    ")\n",
    "source.genome.add_limit(1)\n",
    "\n",
    "load_info = pipeline.run(source, write_disposition=\"replace\")\n",
    "\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXUz6I9WhLv1"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT * FROM genome\") as table:\n",
    "        genome_limited = table.df()\n",
    "genome_limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDHLVjLtOFpf"
   },
   "source": [
    "## **3. Transforming data with `@dlt.transformer`**\n",
    "\n",
    "The main purpose of transformers is to create children tables with additional data requests, but they can also be used for data transformations especially if you want to keep the original data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZL0OaxtL0MA"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import dlt\n",
    "import hashlib\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "\n",
    "@dlt.transformer()\n",
    "def batch_stats(items: TDataItems) -> TDataItem:\n",
    "    \"\"\"\n",
    "    Pseudonymization is a deterministic type of PII-obscuring.\n",
    "    Its role is to allow identifying users by their hash,\n",
    "    without revealing the underlying info.\n",
    "    \"\"\"\n",
    "    # add a constant salt to generate\n",
    "    yield {\n",
    "        \"batch_length\": len(items),\n",
    "        \"max_length\": max([item[\"total_length\"] for item in items]),\n",
    "    }\n",
    "\n",
    "\n",
    "genome_resource = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\", chunk_size=10000\n",
    ").genome\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_pipeline_with_transformers1\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "info = pipeline.run([genome_resource, genome_resource | batch_stats])\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojl-I1dKQB-y"
   },
   "outputs": [],
   "source": [
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT * FROM batch_stats\") as table:\n",
    "        res = table.df()\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od3U3_e5fFUf"
   },
   "source": [
    "## **4. Transforming data after the load**\n",
    "\n",
    "Another possibility for data transformation is transforming data after the load. dlt provides several way of doing it:\n",
    "\n",
    "* using `sql_client`,\n",
    "* via `.dataset()` and ibis integration,\n",
    "* via [dbt integration](https://dlthub.com/docs/dlt-ecosystem/transformations/dbt/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdwcBHZNgy9-"
   },
   "source": [
    "### SQL client\n",
    "\n",
    "You already saw examples of using dlt's SQL client. dlt gives you an opportunity to connect to your destination and execute any SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zHMjtlFSpt3"
   },
   "outputs": [],
   "source": [
    "# NOTE: this is the duckdb sql dialect, other destinations may use different expressions\n",
    "with pipeline.sql_client() as client:\n",
    "    client.execute_sql(\n",
    "        \"\"\" CREATE OR REPLACE TABLE genome_length AS\n",
    "            SELECT\n",
    "                SUM(total_length) AS total_total_length,\n",
    "                AVG(total_length) AS average_total_length\n",
    "            FROM\n",
    "                genome\n",
    "    \"\"\"\n",
    "    )\n",
    "    with client.execute_query(\"SELECT * FROM genome_length\") as table:\n",
    "        genome_length = table.df()\n",
    "\n",
    "genome_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JysZE4sXlAh"
   },
   "source": [
    "### Accessing loaded data with `pipeline.dataset()`\n",
    "\n",
    "Use `pipeline.dataset()` to inspect and work with your data in Python after loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FopxsKDAXsFZ"
   },
   "outputs": [],
   "source": [
    "dataset = pipeline.dataset()\n",
    "\n",
    "# List tables\n",
    "dataset.row_counts().df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTYkzCX1eVtn"
   },
   "source": [
    "Note that `row_counts` didn't return the new table `genome_length`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBo4jGURXjbo"
   },
   "outputs": [],
   "source": [
    "# Access as pandas\n",
    "df = dataset[\"genome\"].df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXDDzuzKXzjC"
   },
   "outputs": [],
   "source": [
    "# Access as Arrow\n",
    "arrow_table = dataset[\"genome_length\"].arrow()\n",
    "arrow_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfNpCLMZX-vP"
   },
   "source": [
    "You can also filter, limit, and select columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BebIXat6YCM4"
   },
   "outputs": [],
   "source": [
    "df = dataset[\"genome\"].select(\"kingdom\", \"ncbi_id\").limit(10).df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xboao89gYFcZ"
   },
   "source": [
    "To iterate over large data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jEx0S_VYI6o"
   },
   "outputs": [],
   "source": [
    "for chunk in dataset[\"genome\"].iter_df(chunk_size=500):\n",
    "    print(chunk.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aldfneQUYLwO"
   },
   "source": [
    "For more advanced users, this interface supports **Ibis expressions**, joins, and subqueries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AFmAZoIgHlP"
   },
   "source": [
    "### Ibis integration\n",
    "\n",
    "Ibis is a powerful portable Python dataframe library. Learn more about what it is and how to use it in the [official documentation](https://ibis-project.org/).\n",
    "\n",
    "[dlt provides a way to use Ibis expressions natively](https://dlthub.com/docs/general-usage/dataset-access/ibis-backend) with a lot of destinations. Supported ones are:\n",
    "* Snowflake\n",
    "* DuckDB\n",
    "* MotherDuck\n",
    "* Postgres\n",
    "* Redshift\n",
    "* Clickhouse\n",
    "* MSSQL (including Synapse)\n",
    "* BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hADtIo8Agaup"
   },
   "outputs": [],
   "source": [
    "!pip install ibis-framework[duckdb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmSgJphDgsUf"
   },
   "outputs": [],
   "source": [
    "# get the dataset from the pipeline\n",
    "dataset = pipeline.dataset()\n",
    "dataset_name = pipeline.dataset_name\n",
    "\n",
    "# get the native ibis connection from the dataset\n",
    "ibis_connection = dataset.ibis()\n",
    "\n",
    "# list all tables in the dataset\n",
    "# NOTE: You need to provide the dataset name to ibis, in ibis datasets are named databases\n",
    "print(ibis_connection.list_tables(database=dataset_name))\n",
    "\n",
    "# get the items table\n",
    "table = ibis_connection.table(\"batch_stats\", database=dataset_name)  #\n",
    "\n",
    "# print the first 2 rows\n",
    "print(table.limit(2).execute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AH3F46PaJZe4"
   },
   "source": [
    "âœ… â–¶ Proceed to the [next lesson](https://colab.research.google.com/drive/1XT1xUIQIWj0nPWOmTixThgdXzi4vudce#forceEdit=true&sandboxMode=true)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
