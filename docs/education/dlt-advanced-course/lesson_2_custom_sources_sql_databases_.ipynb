{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvaKFdYx-kbG"
   },
   "source": [
    "# Building custom sources using SQL Databases [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-advanced-course/lesson_2_custom_sources_sql_databases_.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-advanced-course/lesson_2_custom_sources_sql_databases_.ipynb)\n",
    "\n",
    "This lesson covers building flexible and powerful custom sources using the `sql_database` verified source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_2_Custom_sources_SQL_Databases_img1](https://storage.googleapis.com/dlt-blog-images/dlt-advanced-course/Lesson_2_Custom_sources_SQL_Databases_img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysujjb9N-iY1"
   },
   "source": [
    "\n",
    "## What you will learn\n",
    "\n",
    "- How to build a custom pipeline using SQL sources\n",
    "- How to use `query_adapter_callback`, `table_adapter_callback`, and `type_adapter_callback`\n",
    "- How to load only new data with incremental loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PRqLBIQA7rj"
   },
   "source": [
    "Setup & install dlt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WOrNvitG-u9t"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pymysql duckdb dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeRCcFEy-nLX"
   },
   "source": [
    "## Step 1: Load data from SQL Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcIjuBKh-uVH"
   },
   "source": [
    "Weâ€™ll use the [Rfam MySQL public DB](https://docs.rfam.org/en/latest/database.html) and load it into DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TF8-rOBR-zhw"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from dlt.sources.sql_database import sql_database\n",
    "import dlt\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\", table_names=[\"family\"]\n",
    ")\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_example\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(source)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbDqwuG8DCog"
   },
   "source": [
    "Explore the `family` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDwMcfsyC2Ov"
   },
   "outputs": [],
   "source": [
    "pipeline.dataset().family.df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crBva88e-5Jp"
   },
   "source": [
    "## Step 2: Customize SQL queries with `query_adapter_callback`\n",
    "\n",
    "You can fully rewrite or modify the SQL SELECT statement per table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN-90Lry-7KV"
   },
   "source": [
    "### Filter rows using a WHERE clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuyZSy5W--PQ"
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "from dlt.sources.sql_database.helpers import SelectClause, Table\n",
    "\n",
    "\n",
    "def query_adapter_callback(query: SelectClause, table: Table) -> SelectClause:\n",
    "    return text(f\"SELECT * FROM {table.fullname} WHERE rfam_id like '%bacteria%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCqfmt5_UHsc"
   },
   "source": [
    "To be able to use `sql_database` and not have to declare the connection string each time, we save it as an environment variable. This can also (should preferably) be done in `secrets.toml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NX0CLC0hUM5d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SOURCES__SQL_DATABASE__CREDENTIALS\"] = (\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44Z2OUyGR8rc"
   },
   "outputs": [],
   "source": [
    "filtered_resource = sql_database(\n",
    "    query_adapter_callback=query_adapter_callback, table_names=[\"family\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjPZMS6DWVNN"
   },
   "source": [
    "Let's save this filtered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5w047uqrUzx7"
   },
   "outputs": [],
   "source": [
    "info = pipeline.run(filtered_resource, table_name=\"bacterias\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RRp4wk5WZQB"
   },
   "source": [
    "Explore the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eXrVKDPVAKK"
   },
   "outputs": [],
   "source": [
    "pipeline.dataset().bacterias.df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFUwmvimj_6Y"
   },
   "source": [
    "### **Question 1**:\n",
    "\n",
    "How many rows are present in the `bacterias` table?\n",
    "\n",
    ">Answer this question and select the correct option in the homework Quiz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbtoSO-V_GNG"
   },
   "source": [
    "## Step 3: Modify table schema with `table_adapter_callback`\n",
    "\n",
    "Add columns, change types, or transform schema using this hook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CndtKNl_Irp"
   },
   "source": [
    "### Example: Add computed column `max_timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9KLbuSv_G5G"
   },
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "\n",
    "\n",
    "def add_max_timestamp(table: Table) -> Any:\n",
    "    max_ts = sa.func.greatest(table.c.created, table.c.updated).label(\"max_timestamp\")\n",
    "    subq = sa.select(*table.c, max_ts).subquery()\n",
    "    return subq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8skvoqIo_ZC7"
   },
   "source": [
    "Use it with `sql_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "webIN6Yk_azP"
   },
   "outputs": [],
   "source": [
    "from dlt.sources.sql_database import sql_table\n",
    "\n",
    "table = sql_table(\n",
    "    table=\"family\",\n",
    "    table_adapter_callback=add_max_timestamp,\n",
    "    incremental=dlt.sources.incremental(\"max_timestamp\"),\n",
    ")\n",
    "\n",
    "info = pipeline.run(table, table_name=\"family_with_max_timestamp\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mOTzLkcasQH"
   },
   "source": [
    "Let's check out if this column exists!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqZhEXCPaj1p"
   },
   "outputs": [],
   "source": [
    "pipeline.dataset().family_with_max_timestamp.df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW2eZodN_eO2"
   },
   "source": [
    "## Step 4: Adapt column data types with `type_adapter_callback`\n",
    "\n",
    "When the default types donâ€™t match what you want in the destination, you can remap them.\n",
    "\n",
    "Let's look at the schema that has already been loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJDz9dhbbpDU"
   },
   "outputs": [],
   "source": [
    "schema = pipeline.default_schema.to_dict()[\"tables\"][\"family\"][\"columns\"]\n",
    "for column in schema:\n",
    "    print(schema[column][\"name\"], \":\", schema[column][\"data_type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E83xLD1NfiPt"
   },
   "source": [
    "Lets change `hmm_lambda` from decimal to float.\n",
    "\n",
    "ðŸ’¡ Quick fyi: The `float` data type is:\n",
    "- Fast and uses less space\n",
    "- But it's approximate â€” you may get 0.30000000000000004 instead of 0.3\n",
    "- Bad for money, great for probabilities, large numeric ranges, scientific values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KIH0cAi_hCk"
   },
   "source": [
    "### Example: Change data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a571J08o_e-9"
   },
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "from sqlalchemy.types import Float\n",
    "\n",
    "\n",
    "def type_adapter_callback(sql_type: Any) -> Any:\n",
    "    if isinstance(sql_type, sa.Numeric):\n",
    "        return Float\n",
    "    return sql_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zSTyr_4_mWw"
   },
   "source": [
    "Use it with `sql_database`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQZ-Q5W6_oqX"
   },
   "outputs": [],
   "source": [
    "new_source = sql_database(type_adapter_callback=type_adapter_callback, table_names=[\"family\"])\n",
    "\n",
    "info = pipeline.run(new_source, table_name=\"type_changed_family\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3ljz6PTnzUg"
   },
   "source": [
    "ðŸ‘€ Can you see how the column data types have changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dfvywN3qzJK"
   },
   "outputs": [],
   "source": [
    "schema1 = pipeline.default_schema.to_dict()[\"tables\"][\"family\"][\"columns\"]\n",
    "schema2 = pipeline.default_schema.to_dict()[\"tables\"][\"type_changed_family\"][\"columns\"]\n",
    "column = \"trusted_cutoff\"\n",
    "\n",
    "print(\"For table 'family':\", schema1[column][\"name\"], \":\", schema1[column][\"data_type\"])\n",
    "print(\n",
    "    \"For table 'type_changed_family':\", schema2[column][\"name\"], \":\", schema2[column][\"data_type\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBDCuQiwWewU"
   },
   "source": [
    "### **Question 2**:\n",
    "\n",
    "How many columns had their type changed in the `type_changed_family` table?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYFzQH9T_smT"
   },
   "source": [
    "## Step 5: Incremental loads with `sql_database`\n",
    "Track only new rows using a timestamp or ID column.\n",
    "\n",
    "We'll also be looking at where these incremental values are stored.\n",
    "\n",
    "Hint: they are stored in [dlt state](https://dlthub.com/docs/general-usage/state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njIPDIx_pc33"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/var/dlt/pipelines/sql_database_example/state.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[\"sources\"][\"sql_database\"][\"resources\"][\"family\"][\"incremental\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sF4z28C_3Yg"
   },
   "outputs": [],
   "source": [
    "from dlt.sources.sql_database import sql_database\n",
    "import pendulum\n",
    "\n",
    "source = sql_database(table_names=[\"family\"])\n",
    "source.family.apply_hints(\n",
    "    incremental=dlt.sources.incremental(\"updated\", initial_value=pendulum.datetime(2024, 1, 1))\n",
    ")\n",
    "\n",
    "info = pipeline.run(source)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6GklnPEoj5X"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/var/dlt/pipelines/sql_database_example/state.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[\"sources\"][\"sql_database\"][\"resources\"][\"family\"][\"incremental\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9c5P27Qi3t6"
   },
   "source": [
    "## **Rename tables for `sql_database` source**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wU8j2Va9jZkQ"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "source = sql_database(table_names=[\"family\"])\n",
    "\n",
    "# Loop through each resource (table) in the source\n",
    "for _resource_name, resource in source.resources.items():\n",
    "    # Rename the target table by prefixing with \"xxxx__\"\n",
    "    resource.apply_hints(table_name=f\"xxxx__{resource.name}\")\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_db_prefixed_tables\", destination=\"duckdb\", dataset_name=\"renamed_tables\"\n",
    ")\n",
    "\n",
    "\n",
    "print(pipeline.run(source))\n",
    "pipeline.dataset().row_counts().df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkvUgaRhI6iY"
   },
   "source": [
    "âœ… â–¶ Proceed to the [next lesson](https://colab.research.google.com/drive/1P8pOw9C6J9555o2jhZydESVuVb-3z__y#forceEdit=true&sandboxMode=true)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iz0lz3QhJEvv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
