{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ucJBHffzqYB"
   },
   "source": [
    "# Building Custom Sources with the Filesystem in `dlt` [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-advanced-course/lesson_3_custom_sources_filesystem_and_cloud_storage.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-advanced-course/lesson_3_custom_sources_filesystem_and_cloud_storage.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DntqvRiMzztO"
   },
   "source": [
    "## What you will learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5ayDx9Nz1ts"
   },
   "source": [
    "You will learn how to:\n",
    "\n",
    "- Use the `filesystem` resource to build real custom sources\n",
    "- Apply filters to file metadata (name, size, date)\n",
    "- Implement and register custom transformers\n",
    "- Enrich records with file metadata\n",
    "- Use incremental loading both for files and content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb1F6JTjz6oe"
   },
   "source": [
    "## Setup: Download real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siTnHHjg1fSK"
   },
   "source": [
    "Install dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIK5AASS1hwa"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dlt[duckdb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV5dnMYjz_YX"
   },
   "source": [
    "Weâ€™ll use a real `.parquet` file from [TimeStored.com](https://www.timestored.com/data/sample/userdata.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yntTzQtx0Aw8"
   },
   "outputs": [],
   "source": [
    "!mkdir -p local_data && wget -O local_data/userdata.parquet https://www.timestored.com/data/sample/userdata.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RskhbNs0NCs"
   },
   "source": [
    "## Step 1: Load Parquet file from Local Filesystem\n",
    "\n",
    "**What the script below does**: Lists and reads all `.parquet` files in `./local_data` and loads them into a table named `userdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa3AeSSd0Sar"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.filesystem import filesystem, read_parquet\n",
    "\n",
    "# Point to the local file directory\n",
    "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"**/*.parquet\")\n",
    "\n",
    "# Add a transformer\n",
    "parquet_data = fs | read_parquet()\n",
    "\n",
    "# Create and run pipeline\n",
    "pipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\n",
    "load_info = pipeline.run(parquet_data.with_name(\"userdata\"))\n",
    "print(load_info)\n",
    "\n",
    "# Inspect data\n",
    "pipeline.dataset().userdata.df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaBUSqSR3T7R"
   },
   "source": [
    "### **Question 1**:\n",
    "\n",
    "In the `my_pipeline` pipeline, and the `userdata` dataset, what is the ratio of men:women in decimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXV_lHtsiFLL"
   },
   "outputs": [],
   "source": [
    "# check out the numbers below and answer ðŸ‘€\n",
    "df = pipeline.dataset().userdata.df()\n",
    "df.groupby(\"gender\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP5zagq70jXH"
   },
   "source": [
    "## Step 2: Enrich records with file metadata\n",
    "\n",
    "Letâ€™s add the file name to every record to track the data origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uMvF6Gy0lPa"
   },
   "outputs": [],
   "source": [
    "from dlt.common.typing import TDataItems\n",
    "\n",
    "\n",
    "@dlt.transformer()\n",
    "def read_parquet_with_filename(files: TDataItems) -> TDataItems:\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    for file_item in files:\n",
    "        with file_item.open() as f:\n",
    "            table = pq.read_table(f).to_pandas()\n",
    "            table[\"source_file\"] = file_item[\"file_name\"]\n",
    "            yield table.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"*.parquet\")\n",
    "pipeline = dlt.pipeline(\"meta_pipeline\", destination=\"duckdb\")\n",
    "\n",
    "load_info = pipeline.run((fs | read_parquet_with_filename()).with_name(\"userdata\"))\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdHRDmme0tSw"
   },
   "source": [
    "## Step 3: Filter files by metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyzmMBxm0wJB"
   },
   "source": [
    "Only load files matching custom logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quF9IYad0vTd"
   },
   "outputs": [],
   "source": [
    "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"**/*.parquet\")\n",
    "\n",
    "# Only include files that contain \"user\" and are < 1MB\n",
    "fs.add_filter(lambda f: \"user\" in f[\"file_name\"] and f[\"size_in_bytes\"] < 1_000_000)\n",
    "\n",
    "pipeline = dlt.pipeline(\"filtered_pipeline\", destination=\"duckdb\")\n",
    "load_info = pipeline.run((fs | read_parquet()).with_name(\"userdata_filtered\"))\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLSOVhED014r"
   },
   "source": [
    "## Step 4: Load files incrementally\n",
    "Avoid reprocessing the same file twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBL8UpgL02-I"
   },
   "outputs": [],
   "source": [
    "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"**/*.parquet\")\n",
    "fs.apply_hints(incremental=dlt.sources.incremental(\"modification_date\"))\n",
    "\n",
    "data = (fs | read_parquet()).with_name(\"userdata\")\n",
    "pipeline = dlt.pipeline(\"incremental_pipeline\", destination=\"duckdb\")\n",
    "load_info = pipeline.run(data)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSRbRwtk0-YG"
   },
   "source": [
    "## Step 5: Create a custom transformer\n",
    "\n",
    "Letâ€™s read structured data from `.json` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Va8hU_OX0_eU"
   },
   "outputs": [],
   "source": [
    "@dlt.transformer(standalone=True)\n",
    "def read_json(items: TDataItems) -> TDataItems:\n",
    "    from dlt.common import json\n",
    "\n",
    "    for file_obj in items:\n",
    "        with file_obj.open() as f:\n",
    "            yield json.load(f)\n",
    "\n",
    "\n",
    "# Download a JSON file\n",
    "!wget -O local_data/sample.json https://jsonplaceholder.typicode.com/users\n",
    "\n",
    "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"sample.json\")\n",
    "pipeline = dlt.pipeline(\"json_pipeline\", destination=\"duckdb\")\n",
    "\n",
    "load_info = pipeline.run((fs | read_json()).with_name(\"users\"))\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6fldxcG1FWX"
   },
   "source": [
    "ðŸ“ You will see that this file also exists in your local_data directory.\n",
    "\n",
    "> A **standalone** resource is defined on a function that is top-level in a module (not an inner function) that accepts config and secrets values. Additionally, if the standalone flag is specified, the decorated function signature and docstring will be preserved. `dlt.resource` will just wrap the decorated function, and the user must call the wrapper to get the actual resource.\n",
    "\n",
    "Let's inspect the `users` table in your DuckDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcFLDHv37O3Q"
   },
   "outputs": [],
   "source": [
    "pipeline.dataset().users.df().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2KL2AYK1Ops"
   },
   "source": [
    "## Step 6: Copy files before loading\n",
    "\n",
    "Copy files locally as part of the pipeline. This is useful for backups or post-processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRf8QyK01Q9h"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dlt.sources.filesystem import filesystem\n",
    "from dlt.common.storages.fsspec_filesystem import FileItemDict\n",
    "\n",
    "\n",
    "def copy_local(item: FileItemDict) -> FileItemDict:\n",
    "    local_path = os.path.join(\"copied\", item[\"file_name\"])\n",
    "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "    item.fsspec.download(item[\"file_url\"], local_path)\n",
    "    return item\n",
    "\n",
    "\n",
    "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"**/*.parquet\").add_map(copy_local)\n",
    "pipeline = dlt.pipeline(\"copy_pipeline\", destination=\"duckdb\")\n",
    "load_info = pipeline.run(fs.with_name(\"copied_files\"))\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A62GdLRzJitT"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "- Try building a transformer for `.xml` using `xmltodict`\n",
    "- Combine multiple directories or buckets in a single pipeline\n",
    "- Explore [more examples](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/advanced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoWLhw7DLg7i"
   },
   "source": [
    "âœ… â–¶ Proceed to the [next lesson](https://colab.research.google.com/drive/14br3TZTRFwTSwpDyom7fxlZCeRF4efMk#forceEdit=true&sandboxMode=true)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_3_Custom_sources_Filesystem_and_cloud_storage_img1](https://storage.googleapis.com/dlt-blog-images/dlt-advanced-course/Lesson_3_Custom_sources_Filesystem_and_cloud_storage_img1.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBJ9K3XwMhZW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
