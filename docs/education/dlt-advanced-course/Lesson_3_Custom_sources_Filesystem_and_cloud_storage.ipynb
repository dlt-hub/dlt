{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ucJBHffzqYB"
      },
      "source": [
        "# Building Custom Sources with the Filesystem in `dlt` [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/6be4aaac807414ae6100691174c5babcd6a87736/docs/education/dlt-advanced-course/Lesson_3_Custom_sources_Filesystem_and_cloud_storage.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/6be4aaac807414ae6100691174c5babcd6a87736/docs/education/dlt-advanced-course/Lesson_3_Custom_sources_Filesystem_and_cloud_storage.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DntqvRiMzztO"
      },
      "source": [
        "## What you will learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ayDx9Nz1ts"
      },
      "source": [
        "You will learn how to:\n",
        "\n",
        "- Use the `filesystem` resource to build real custom sources\n",
        "- Apply filters to file metadata (name, size, date)\n",
        "- Implement and register custom transformers\n",
        "- Enrich records with file metadata\n",
        "- Use incremental loading both for files and content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb1F6JTjz6oe"
      },
      "source": [
        "## Setup: Download real data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siTnHHjg1fSK"
      },
      "source": [
        "Install dlt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIK5AASS1hwa"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install dlt[duckdb]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV5dnMYjz_YX"
      },
      "source": [
        "We’ll use a real `.parquet` file from [TimeStored.com](https://www.timestored.com/data/sample/userdata.parquet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yntTzQtx0Aw8"
      },
      "outputs": [],
      "source": [
        "!mkdir -p local_data && wget -O local_data/userdata.parquet https://www.timestored.com/data/sample/userdata.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RskhbNs0NCs"
      },
      "source": [
        "## Step 1: Load Parquet file from Local Filesystem\n",
        "\n",
        "**What the script below does**: Lists and reads all `.parquet` files in `./local_data` and loads them into a table named `userdata`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa3AeSSd0Sar"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.filesystem import filesystem, read_parquet\n",
        "\n",
        "# Point to the local file directory\n",
        "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"**/*.parquet\")\n",
        "\n",
        "# Add a transformer\n",
        "parquet_data = fs | read_parquet()\n",
        "\n",
        "# Create and run pipeline\n",
        "pipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\n",
        "load_info = pipeline.run(parquet_data.with_name(\"userdata\"))\n",
        "print(load_info)\n",
        "\n",
        "# Inspect data\n",
        "pipeline.dataset().userdata.df().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaBUSqSR3T7R"
      },
      "source": [
        "### **Question 1**:\n",
        "\n",
        "In the `my_pipeline` pipeline, and the `userdata` dataset, what is the ratio of men:women in decimal?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXV_lHtsiFLL"
      },
      "outputs": [],
      "source": [
        "# check out the numbers below and answer 👀\n",
        "df = pipeline.dataset().userdata.df()\n",
        "df.groupby(\"gender\").describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP5zagq70jXH"
      },
      "source": [
        "## Step 2: Enrich records with file metadata\n",
        "\n",
        "Let’s add the file name to every record to track the data origin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uMvF6Gy0lPa"
      },
      "outputs": [],
      "source": [
        "@dlt.transformer()\n",
        "def read_parquet_with_filename(files):\n",
        "    import pyarrow.parquet as pq\n",
        "    for file_item in files:\n",
        "        with file_item.open() as f:\n",
        "            table = pq.read_table(f).to_pandas()\n",
        "            table[\"source_file\"] = file_item[\"file_name\"]\n",
        "            yield table.to_dict(orient=\"records\")\n",
        "\n",
        "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"*.parquet\")\n",
        "pipeline = dlt.pipeline(\"meta_pipeline\", destination=\"duckdb\")\n",
        "\n",
        "load_info = pipeline.run((fs | read_parquet_with_filename()).with_name(\"userdata\"))\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdHRDmme0tSw"
      },
      "source": [
        "## Step 3: Filter files by metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyzmMBxm0wJB"
      },
      "source": [
        "Only load files matching custom logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quF9IYad0vTd"
      },
      "outputs": [],
      "source": [
        "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"**/*.parquet\")\n",
        "\n",
        "# Only include files that contain \"user\" and are < 1MB\n",
        "fs.add_filter(lambda f: \"user\" in f[\"file_name\"] and f[\"size_in_bytes\"] < 1_000_000)\n",
        "\n",
        "pipeline = dlt.pipeline(\"filtered_pipeline\", destination=\"duckdb\")\n",
        "load_info = pipeline.run((fs | read_parquet()).with_name(\"userdata_filtered\"))\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLSOVhED014r"
      },
      "source": [
        "## Step 4: Load files incrementally\n",
        "Avoid reprocessing the same file twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBL8UpgL02-I"
      },
      "outputs": [],
      "source": [
        "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"**/*.parquet\")\n",
        "fs.apply_hints(incremental=dlt.sources.incremental(\"modification_date\"))\n",
        "\n",
        "data = (fs | read_parquet()).with_name(\"userdata\")\n",
        "pipeline = dlt.pipeline(\"incremental_pipeline\", destination=\"duckdb\")\n",
        "load_info = pipeline.run(data)\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSRbRwtk0-YG"
      },
      "source": [
        "## Step 5: Create a custom transformer\n",
        "\n",
        "Let’s read structured data from `.json` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va8hU_OX0_eU"
      },
      "outputs": [],
      "source": [
        "@dlt.transformer(standalone=True)\n",
        "def read_json(items):\n",
        "    from dlt.common import json\n",
        "    for file_obj in items:\n",
        "        with file_obj.open() as f:\n",
        "            yield json.load(f)\n",
        "\n",
        "# Download a JSON file\n",
        "!wget -O local_data/sample.json https://jsonplaceholder.typicode.com/users\n",
        "\n",
        "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"sample.json\")\n",
        "pipeline = dlt.pipeline(\"json_pipeline\", destination=\"duckdb\")\n",
        "\n",
        "load_info = pipeline.run((fs | read_json()).with_name(\"users\"))\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6fldxcG1FWX"
      },
      "source": [
        "📁 You will see that this file also exists in your local_data directory.\n",
        "\n",
        "> A **standalone** resource is defined on a function that is top-level in a module (not an inner function) that accepts config and secrets values. Additionally, if the standalone flag is specified, the decorated function signature and docstring will be preserved. `dlt.resource` will just wrap the decorated function, and the user must call the wrapper to get the actual resource.\n",
        "\n",
        "Let's inspect the `users` table in your DuckDB dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcFLDHv37O3Q"
      },
      "outputs": [],
      "source": [
        "pipeline.dataset().users.df().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2KL2AYK1Ops"
      },
      "source": [
        "## Step 6: Copy files before loading\n",
        "\n",
        "Copy files locally as part of the pipeline. This is useful for backups or post-processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRf8QyK01Q9h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dlt.sources.filesystem import filesystem\n",
        "from dlt.common.storages.fsspec_filesystem import FileItemDict\n",
        "\n",
        "def copy_local(item: FileItemDict) -> FileItemDict:\n",
        "    local_path = os.path.join(\"copied\", item[\"file_name\"])\n",
        "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "    item.fsspec.download(item[\"file_url\"], local_path)\n",
        "    return item\n",
        "\n",
        "fs = filesystem(bucket_url=\"./local_data\", file_glob=\"**/*.parquet\").add_map(copy_local)\n",
        "pipeline = dlt.pipeline(\"copy_pipeline\", destination=\"duckdb\")\n",
        "load_info = pipeline.run(fs.with_name(\"copied_files\"))\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A62GdLRzJitT"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "- Try building a transformer for `.xml` using `xmltodict`\n",
        "- Combine multiple directories or buckets in a single pipeline\n",
        "- Explore [more examples](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/advanced)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoWLhw7DLg7i"
      },
      "source": [
        "✅ ▶ Proceed to the [next lesson](https://colab.research.google.com/drive/14br3TZTRFwTSwpDyom7fxlZCeRF4efMk#forceEdit=true&sandboxMode=true)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Lesson_3_Custom_sources_Filesystem_and_cloud_storage_img1](https://storage.googleapis.com/dlt-blog-images/dlt-advanced-course/Lesson_3_Custom_sources_Filesystem_and_cloud_storage_img1.webp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBJ9K3XwMhZW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
