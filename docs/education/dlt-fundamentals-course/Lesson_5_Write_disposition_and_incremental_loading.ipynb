{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h93BcC8SX2fj"
      },
      "source": [
        "# **Recap of [Lesson 4](https://colab.research.google.com/drive/1mfqZulsuFDc7h27d6joe2_Dduvl1uM-2#forceEdit=true&sandboxMode=true) 👩‍💻🚀**\n",
        "\n",
        "1. Listed all available verified sources;\n",
        "2. Initialized `github_api` verified source;\n",
        "3. Explored built-in `rest_api` source.\n",
        "4. Explored built-in `sql_database` source.\n",
        "5. Explored built-in `filesystem` source.\n",
        "6. Learned how to switch between destinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTAeTdoKJHZV"
      },
      "source": [
        "---\n",
        "\n",
        "# **Write Disposition and Incremental Loading** ⚙️🧠 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/Lesson_5_Write_disposition_and_incremental_loading.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/Lesson_5_Write_disposition_and_incremental_loading.ipynb)\n",
        "\n",
        "\n",
        "**Here, you will learn:**\n",
        "- `dlt` write dispositions:\n",
        "  - Append\n",
        "  - Replace\n",
        "  - Merge\n",
        "- What incremental loading is\n",
        "- How to update and deduplicate your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jdWPjjmqANA"
      },
      "source": [
        "---\n",
        "## **`dlt` write dispositions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ThZzzAwqLnn"
      },
      "source": [
        "Write disposition in the context of the dlt library defines how the data should be written to the destination. There are three types of write dispositions:\n",
        "\n",
        "* **Append**: This is the **default** disposition. It will append the data to the existing data in the destination.\n",
        "\n",
        "* **Replace**: This disposition replaces the data in the destination with the data from the resource. It **deletes** all the data and **recreates** the schema before loading the data.\n",
        "\n",
        "* **Merge**: This write disposition merges the data from the resource with the data at the destination. For the merge disposition, you need to specify a `primary_key` for the resource.\n",
        "\n",
        "The write disposition you choose depends on the dataset and how you can extract it. For more details, you can refer to the [Incremental loading page](https://dlthub.com/docs/general-usage/incremental-loading).\n",
        "\n",
        "\n",
        "\n",
        "A `write_disposition` in `dlt` can specified in the resource decorator:\n",
        "\n",
        "```python\n",
        "@dlt.resource(write_disposition=\"append\")\n",
        "def my_resource():\n",
        "  ...\n",
        "  yield data\n",
        "```\n",
        "\n",
        "Or directly in the pipeline run:\n",
        "\n",
        "```python\n",
        "load_info = pipeline.run(my_resource, write_disposition=\"replace\")\n",
        "```\n",
        "\n",
        "> In case you specify both, the write disposition specified at the pipeline run level will override the write disposition specified at the resource level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpEU7xzw9lZL"
      },
      "source": [
        "### **0. Install dlt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su4oUJelKaZY"
      },
      "source": [
        "Install `dlt` with DuckDB as a destination as per usual:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygrb6tHBJ_Zm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"dlt[duckdb]\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8gucDYi-AKy"
      },
      "source": [
        "---\n",
        "### **1. Append**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IpPPDpVrU75"
      },
      "source": [
        "As we already have said `append` is a default loading behavior. Now we will explore how this write disposition works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJweOgGK-_sg"
      },
      "source": [
        "Let's remember our Quick Start data sample with pokemons:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsMeCZwJiEea"
      },
      "outputs": [],
      "source": [
        "# Sample data containing pokemon details\n",
        "data = [\n",
        "    {\"id\": \"1\", \"name\": \"bulbasaur\", \"size\": {\"weight\": 6.9, \"height\": 0.7}},\n",
        "    {\"id\": \"4\", \"name\": \"charmander\", \"size\": {\"weight\": 8.5, \"height\": 0.6}},\n",
        "    {\"id\": \"25\", \"name\": \"pikachu\", \"size\": {\"weight\": 6, \"height\": 0.4}},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CltUh8t6rGUP"
      },
      "source": [
        "We create dlt pipeline as usual and load this data into DuckDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9nkupig_eAj"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "@dlt.resource(\n",
        "    name='pokemon',\n",
        "    write_disposition='append', # <--- add new argument into decorator\n",
        ")\n",
        "def pokemon():\n",
        "    yield data\n",
        "\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"poke_pipeline\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"pokemon_data\",\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)\n",
        "\n",
        "# explore loaded data\n",
        "pipeline.dataset().pokemon.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtz2oUpCs7Ay"
      },
      "source": [
        "Run this example **twice**, and you'll notice that each time a copy of the data is added to your tables. We call this load mode **append**. It is very useful.\n",
        "\n",
        "Example use case: when you have a new folder created daily with json file logs, and you want to ingest them incrementally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqP0UbTStNBm"
      },
      "outputs": [],
      "source": [
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)\n",
        "\n",
        "# explore loaded data\n",
        "pipeline.dataset().pokemon.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SgDby08DkG6"
      },
      "source": [
        "---\n",
        "### **2. Replace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njz_qUcpDtTW"
      },
      "source": [
        "Perhaps this duplicated data is not what you want to get in your work projects. For example, if your data was updated, how we can refresh it in the database? One method is to tell dlt to **replace** the data in existing tables by using **write_disposition**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvlzYBsdDwYi"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "@dlt.resource(\n",
        "    name='pokemon',\n",
        "    write_disposition='replace', # <--- change 'append' to 'replace'\n",
        ")\n",
        "def pokemon():\n",
        "    yield data\n",
        "\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"poke_pipeline\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"pokemon_data\",\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)\n",
        "\n",
        "# explore loaded data\n",
        "pipeline.dataset().pokemon.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYlSswk8t139"
      },
      "source": [
        "Run it again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7Vc9Davt4OS"
      },
      "outputs": [],
      "source": [
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)\n",
        "\n",
        "# explore loaded data\n",
        "pipeline.dataset().pokemon.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPjezxijt_mz"
      },
      "source": [
        "TAADA! No duplicates, your data was [fully refreshed](https://dlthub.com/docs/general-usage/full-loading)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICvz5T94EK7u"
      },
      "source": [
        "---\n",
        "### **3. [Merge](https://dlthub.com/docs/general-usage/incremental-loading#merge-incremental-loading)**\n",
        "\n",
        "Consider a scenario where the data in the source has been updated, but you want to avoid reloading the entire dataset.\n",
        "\n",
        "\n",
        "\n",
        "Merge write disposition is used to merge new data into the destination, using a `merge_key` and/or **deduplicating**/**upserting** new data using a `primary_key`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Lesson_5_Write_disposition_and_incremental_loading_img1](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_5_Write_disposition_and_incremental_loading_img1.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPrDUs-3w-BU"
      },
      "source": [
        "\n",
        "The **merge** write disposition can be useful in several situations:\n",
        "\n",
        "1.  If you have a dataset where records are frequently updated and you want to reflect these changes in your database, the `merge` write disposition can be used. It will **update the existing records** with the new data instead of creating duplicate entries.\n",
        "\n",
        "2. If your data source occasionally sends **duplicate records**, the merge write disposition can help handle this. It uses a `primary_key` to identify unique records, so if a duplicate record (with the same `primary_key`) is encountered, it will be merged with the existing record instead of creating a new one.\n",
        "\n",
        "3. If you are dealing with **Slowly Changing Dimensions** (SCD) where the attribute of a record changes over time and you want to maintain a history of these changes, you can use the `merge` write disposition with the scd2 strategy.\n",
        "\n",
        "\n",
        "When using the merge disposition, you need to specify a `primary_key` or `merge_key` for the resource.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m486m1RzFbR"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "@dlt.resource(\n",
        "    name='pokemon',\n",
        "    write_disposition='merge', # <--- change 'replace' to 'merge'\n",
        "    primary_key=\"id\", # <--- add primary_key\n",
        ")\n",
        "def pokemon():\n",
        "    yield data\n",
        "\n",
        "\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"poke_pipeline_merge\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"pokemon_data\",\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)\n",
        "\n",
        "# explore loaded data\n",
        "pipeline.dataset().pokemon.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP0VEWLryQX6"
      },
      "source": [
        "The merge write disposition can be used with three different strategies:\n",
        "\n",
        "* delete-insert (default strategy)\n",
        "* scd2\n",
        "* upsert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Y1oCAvJ79I"
      },
      "source": [
        "---\n",
        "##  **Incremental Loading**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cFamtHqeicW"
      },
      "source": [
        "Incremental loading is the act of loading only new or changed data and not old records that we already loaded.\n",
        "\n",
        "Imagine you’re a Pokémon trainer trying to catch ‘em all. You don’t want to keep visiting the same old PokéStops, catching the same old Bulbasaurs—you only want to find new and exciting Pokémon that have appeared since your last trip. That’s what incremental loading is all about: collecting only the new data that’s been added or changed, without wasting your Poké Balls (or database resources) on what you already have.\n",
        "\n",
        "In this example, we have a dataset of Pokémon, each with a **unique ID**, their **name**, **size** (height and weight), and **when** they were \"caught\" (`created_at` field).\n",
        "\n",
        "### **Step 1: Adding the `created_at` Field**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7auH8P0Nm_yp"
      },
      "outputs": [],
      "source": [
        "# We added `created_at` field to the data\n",
        "data = [\n",
        "    {\n",
        "        \"id\": \"1\",\n",
        "        \"name\": \"bulbasaur\",\n",
        "        \"size\": {\"weight\": 6.9, \"height\": 0.7},\n",
        "        \"created_at\": \"2024-12-01\"    # <------- new field\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"4\",\n",
        "        \"name\": \"charmander\",\n",
        "        \"size\": {\"weight\": 8.5, \"height\": 0.6},\n",
        "        \"created_at\": \"2024-09-01\"    # <------- new field\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"25\",\n",
        "        \"name\": \"pikachu\",\n",
        "        \"size\": {\"weight\": 6, \"height\": 0.4},\n",
        "        \"created_at\": \"2023-06-01\"    # <------- new field\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO63mHgE_Oya"
      },
      "source": [
        "**The goal**: Load only Pokémon caught after January 1, 2024, skipping the ones you already have.\n",
        "\n",
        "### **Step 2: Defining the incremental logic**\n",
        "\n",
        "Using `dlt`, we set up an [incremental filter](https://www.google.com/url?q=https://dlthub.com/docs/general-usage/incremental-loading%23incremental-loading-with-a-cursor-field&sa=D&source=editors&ust=1734717286675253&usg=AOvVaw3rAF3y3p86sGt49ImCTgon) to only fetch Pokémon caught after a certain date:\n",
        "```python\n",
        "cursor_date = dlt.sources.incremental(\"created_at\", initial_value=\"2024-01-01\")\n",
        "```\n",
        "This tells `dlt`:\n",
        "- **Start date**: January 1, 2024 (`initial_value`).\n",
        "- **Field to track**: `created_at` (our timestamp).\n",
        "\n",
        "As you run the pipeline repeatedly, `dlt` will keep track of the latest `created_at` value processed. It will skip records older than this date in future runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXSzU50JmsvH"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "@dlt.resource(\n",
        "    name=\"pokemon\",\n",
        "    write_disposition=\"append\",\n",
        ")\n",
        "def pokemon(cursor_date=dlt.sources.incremental(\"created_at\", initial_value=\"2024-01-01\")):\n",
        "    yield data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pq66CH7FJkh"
      },
      "source": [
        "We use the `@dlt.resource` decorator to declare table **name** to which data will be loaded and **write disposition**, which is **append** by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07YDkA8uANIs"
      },
      "source": [
        "### **Step 3: Running the pipeline**\n",
        "Finally, we run our pipeline and load the fresh Pokémon data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K86bDrfnmgU"
      },
      "outputs": [],
      "source": [
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"poke_pipeline_incremental\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"pokemon_data\",\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)\n",
        "\n",
        "# explore loaded data\n",
        "pipeline.dataset().pokemon.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAskPiHXAVc0"
      },
      "source": [
        "This:\n",
        "1. Loads **only Charmander and Bulbasaur** (caught after 2024-01-01).\n",
        "2. Skips Pikachu because it’s old news."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBZU_pD0pJ8F"
      },
      "source": [
        "Only data for 2024 year was loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U17Ae4kruM_F"
      },
      "source": [
        "![Lesson_5_Write_disposition_and_incremental_loading_img2](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_5_Write_disposition_and_incremental_loading_img2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYHrh512EaX3"
      },
      "source": [
        "Run the same pipeline again. The pipeline will detect that there are **no new records** based on the `created_at` field and the incremental cursor. As a result, **no new data will be loaded** into the destination:\n",
        ">0 load package(s) were loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-EiH0bPEh-7"
      },
      "outputs": [],
      "source": [
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eyLtTktAtOZ"
      },
      "source": [
        "### **Why incremental loading matters**\n",
        "\n",
        "* **Efficiency**. Skip redundant data, saving time and resources.\n",
        "* **Scalability**. Handle growing datasets without bottlenecks.\n",
        "* **Automation**. Let the tool track changes for you—no manual effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obKfcQuSEUzW"
      },
      "source": [
        "## **Update and deduplicate your data**\n",
        "The script above finds new pokemons and adds them to the database. It will ignore any updates to user information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvSzUJ0KE-he"
      },
      "outputs": [],
      "source": [
        "# We added `created_at` field to the data\n",
        "data = [\n",
        "    {\n",
        "        \"id\": \"1\",\n",
        "        \"name\": \"bulbasaur\",\n",
        "        \"size\": {\"weight\": 6.9, \"height\": 0.7},\n",
        "        \"created_at\": \"2024-12-01\",\n",
        "        \"updated_at\": \"2024-12-01\"    # <------- new field\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"4\",\n",
        "        \"name\": \"charmander\",\n",
        "        \"size\": {\"weight\": 8.5, \"height\": 0.6},\n",
        "        \"created_at\": \"2024-09-01\",\n",
        "        \"updated_at\": \"2024-09-01\"    # <------- new field\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"25\",\n",
        "        \"name\": \"pikachu\",\n",
        "        \"size\": {\"weight\": 9, \"height\": 0.4}, # <----- pikachu gained weight from 6 to 9\n",
        "        \"created_at\": \"2023-06-01\",\n",
        "        \"updated_at\": \"2024-12-16\"    # <------- new field, information about pikachu has updated\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRMUVnQcsxAn"
      },
      "source": [
        "Get always fresh content of all the pokemons: combine an **incremental load** with **merge** write disposition, like in the script below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiRWu_diml40"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "\n",
        "@dlt.resource(\n",
        "    name=\"pokemon\",\n",
        "    write_disposition=\"merge\",  # <--- change write disposition from 'append' to 'merge'\n",
        "    primary_key=\"id\",  # <--- set a primary key\n",
        ")\n",
        "def pokemon(cursor_date=dlt.sources.incremental(\"updated_at\", initial_value=\"2024-01-01\")):  # <--- change the cursor name from 'created_at' to 'updated_at'\n",
        "    yield data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VoPe5AgtCQv"
      },
      "source": [
        "The incremental cursor keeps an eye on the `updated_at` field. Every time the pipeline runs, it only processes records with `updated_at` values greater than the last run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4EusomVnV9-"
      },
      "outputs": [],
      "source": [
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"poke_pipeline_dedup\",\n",
        "    destination=\"duckdb\",\n",
        "    dataset_name=\"pokemon_data\",\n",
        ")\n",
        "\n",
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)\n",
        "\n",
        "# explore loaded data\n",
        "pipeline.dataset().pokemon.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omG1cgzcrqOs"
      },
      "source": [
        "All Pokémon are processed because this is the pipeline’s first run.\n",
        "\n",
        "Now, let’s say Pikachu goes to gym and sheds some weight (down to 7.5), and the `updated_at` field is set to `2024-12-23`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljiJE9pGnplS"
      },
      "outputs": [],
      "source": [
        "# We added `created_at` field to the data\n",
        "data = [\n",
        "    {\n",
        "        \"id\": \"1\",\n",
        "        \"name\": \"bulbasaur\",\n",
        "        \"size\": {\"weight\": 6.9, \"height\": 0.7},\n",
        "        \"created_at\": \"2024-12-01\",\n",
        "        \"updated_at\": \"2024-12-01\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"4\",\n",
        "        \"name\": \"charmander\",\n",
        "        \"size\": {\"weight\": 8.5, \"height\": 0.6},\n",
        "        \"created_at\": \"2024-09-01\",\n",
        "        \"updated_at\": \"2024-09-01\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"25\",\n",
        "        \"name\": \"pikachu\",\n",
        "        \"size\": {\"weight\": 7.5, \"height\": 0.4}, # <--- pikachu lost weight\n",
        "        \"created_at\": \"2023-06-01\",\n",
        "        \"updated_at\": \"2024-12-23\"  # <--- data about his weight was updated a week later\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfPGzOh2okdm"
      },
      "source": [
        "Run the same pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP5i8tZKoksd"
      },
      "outputs": [],
      "source": [
        "load_info = pipeline.run(pokemon)\n",
        "print(load_info)\n",
        "\n",
        "# explore loaded data\n",
        "pipeline.dataset().pokemon.df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2hZHn_EowBd"
      },
      "source": [
        "**What happens?**\n",
        "\n",
        "* The pipeline detects that `updated_at` for Bulbasaur and Charmander hasn’t changed—they’re skipped.\n",
        "* Pikachu’s record is updated to reflect the latest weight.\n",
        "\n",
        "You can see that the **`_dlt_load_id`** for Bulbasaur and Charmander remained the same, but for Pikachu it was changed since only the updated Pikachu data was loaded into the destination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pufZ_GWPxqEQ"
      },
      "source": [
        "The **`dlt.sources.incremental`** instance above has the next attributes:\n",
        "\n",
        "* **`cursor_date.initial_value`** which is always equal to \"2024-01-01\" passed in the constructor;\n",
        "* **`cursor_date.start_value`** a maximum `updated_at` value from the previous run or the `initial_value` on the first run;\n",
        "* **`cursor_date.last_value`** a \"real-time\" `updated_at` value updated with each yielded item or page. Before the first yield, it equals `start_value`;\n",
        "* **`cursor_date.end_value`** (here not used) marking the end of the backfill range.\n",
        "\n",
        "## **Example**\n",
        "You can use them in the resource code to make **more efficient requests**. Take look at the GitHub API example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4C_IFK7G4m9"
      },
      "outputs": [],
      "source": [
        "exit() # we use exit() to reset all ENVs we set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxhTJT452X-C"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "from dlt.sources.helpers import requests\n",
        "from dlt.sources.helpers.rest_client import RESTClient\n",
        "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
        "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"SOURCES__ACCESS_TOKEN\"] = userdata.get('SECRET_KEY')\n",
        "\n",
        "\n",
        "@dlt.source\n",
        "def github_source(access_token=dlt.secrets.value):\n",
        "    client = RESTClient(\n",
        "            base_url=\"https://api.github.com\",\n",
        "            auth=BearerTokenAuth(token=access_token),\n",
        "            paginator=HeaderLinkPaginator(),\n",
        "    )\n",
        "\n",
        "    @dlt.resource(\n",
        "        name=\"issues\",\n",
        "        write_disposition=\"merge\",\n",
        "        primary_key=\"id\"\n",
        "    )\n",
        "    def github_issues(cursor_date=dlt.sources.incremental(\"updated_at\", initial_value=\"2024-12-01\")):\n",
        "        params = {\n",
        "            \"since\": cursor_date.last_value,  # <--- use last_value to request only new data from API\n",
        "            \"status\": \"open\"\n",
        "        }\n",
        "        for page in client.paginate(\"repos/dlt-hub/dlt/issues\", params=params):\n",
        "            yield page\n",
        "\n",
        "\n",
        "    return github_issues\n",
        "\n",
        "\n",
        "# define new dlt pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"github_incr\",\n",
        "    destination=\"duckdb\"\n",
        ")\n",
        "\n",
        "\n",
        "# run the pipeline with the new resource\n",
        "load_info = pipeline.run(github_source())\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d1J5DPX3Dn3"
      },
      "source": [
        "Pay attention how we use **since** GitHub API parameter and `cursor_date.last_value` to tell GitHub which issues we are interested in. `cursor_date.last_value` holds the last `cursor_date` value from the previous run.\n",
        "\n",
        "Run the pipeline again and make sure that **no data was loaded**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkAr3_AR3B5k"
      },
      "outputs": [],
      "source": [
        "# run the pipeline with the new resource\n",
        "load_info = pipeline.run(github_source())\n",
        "print(load_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XprE0oiYtIiV"
      },
      "source": [
        "## **Apply Hints**\n",
        "\n",
        "Alternatively, you can use `apply_hints` on a resource to define an incremental field:\n",
        "\n",
        "```python\n",
        "resource = resource()\n",
        "resource.apply_hints(incremental=dlt.sources.incremental(\"updated_at\"))\n",
        "```\n",
        "\n",
        "When you apply an incremental hint using `apply_hints`, the source still performs a full extract. The incremental hint is used by `dlt` to filter the data after it has been extracted, before it is loaded into the destination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ULo72pWu0sX"
      },
      "source": [
        "## **Exercise 1: Make the GitHub API pipeline incremental**\n",
        "\n",
        "In the previous lessons, you built a pipeline to pull data from the GitHub API. Now, let’s level it up by making it incremental, so it fetches only new or updated data.\n",
        "\n",
        "\n",
        "Transform your GitHub API pipeline to use incremental loading. This means:\n",
        "\n",
        "* Implement new `dlt.resource` for `pulls/comments` (List comments for Pull Requests) endpoint.\n",
        "* Fetch only pulls comments updated after the last pipeline run.\n",
        "* Use the `updated_at` field from the GitHub API as the incremental cursor.\n",
        "* [Endpoint documentation](https://docs.github.com/en/rest/pulls/comments?apiVersion=2022-11-28#list-review-comments-in-a-repository)\n",
        "* Endpoint URL: `https://api.github.com/repos/OWNER/REPO/pulls/comments`\n",
        "* Use `since` parameter - only show results that were last updated after the given time - and `last_value`.\n",
        "* `initial_value` is `2024-12-01`.\n",
        "\n",
        "\n",
        "### Question\n",
        "\n",
        "How many columns does the `comments` table have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYbccmLie1zm"
      },
      "source": [
        "✅ ▶ Proceed to the [next lesson](https://colab.research.google.com/drive/1geSMNRkSwAelQJKd3e8vdoHCKiHMdmIo#forceEdit=true&sandboxMode=true)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVyiG5wRVo1B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
