{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTmIgQKpV355"
   },
   "source": [
    "# **Recap of [Lesson 3](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_3_pagination_and_authentication_and_dlt_configuration.ipynb) üë©‚ÄçüíªüöÄ**\n",
    "\n",
    "1. Used pagination with REST APIs.  \n",
    "2. Applied authentication for REST APIs.  \n",
    "3. Tried the dlt `RESTClient`.  \n",
    "4. Used environment variables to manage secrets and configuration.  \n",
    "5. Learned how to add values to `secrets.toml` and `config.toml`.  \n",
    "6. Used the special `secrets.toml` environment variable setup for Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gsY9pD3cw0O"
   },
   "source": [
    "---\n",
    "# **`dlt`‚Äôs pre-built Sources and Destinations** [![Open in molab](https://marimo.io/molab-shield.svg)](https://molab.marimo.io/github/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_4_using_pre_build_sources_and_destinations.py) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_4_using_pre_build_sources_and_destinations.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_4_using_pre_build_sources_and_destinations.ipynb)\n",
    "\n",
    "\n",
    "**Here, you will learn:**\n",
    "- How to initialize verified sources.\n",
    "- The built-in `rest_api` source.\n",
    "- The built-in `sql_database` source.\n",
    "- The built-in `filesystem` source.\n",
    "- How to switch between destinations.\n",
    "\n",
    "---\n",
    "\n",
    "Our verified sources are the simplest way to start building your stack. Choose from any of our fully customizable 30+ pre-built sources, such as SQL databases, Google Sheets, Salesforce, and more.\n",
    "\n",
    "With our numerous destinations, you can load data into a local database, data warehouse, or data lake. Choose from Snowflake, Databricks, and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_4_Using_pre_build_sources_and_destinations_img1](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_4_Using_pre_build_sources_and_destinations_img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo-vUEJuYSxg"
   },
   "source": [
    "# **Existing verified sources**\n",
    "To use an [existing verified source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/), just run the `dlt init` command.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dIUHlb1GTJl"
   },
   "source": [
    "\n",
    "There's a base project for each `dlt` verified source + destination combination, which you can adjust according to your needs.\n",
    "\n",
    "These base project can be initialized with a simple command:\n",
    "\n",
    "```\n",
    "dlt init <verified-source> <destination>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBc-KEm-YSRz"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dlt[duckdb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2HEi-5VYgas"
   },
   "source": [
    "List all verified sources:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb1tDZmAYTzW"
   },
   "outputs": [],
   "source": [
    "!dlt init --list-sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KeAw139YfvZ"
   },
   "source": [
    "This command shows all available verified sources and their short descriptions. For each source, it checks if your local `dlt` version requires an update and prints the relevant warning.\n",
    "\n",
    "Consider an example pipeline for the GitHub API:\n",
    "\n",
    "```\n",
    "Available dlt single file templates:\n",
    "---\n",
    "arrow: The Arrow Pipeline Template will show how to load and transform arrow tables.\n",
    "dataframe: The DataFrame Pipeline Template will show how to load and transform pandas dataframes.\n",
    "debug: The Debug Pipeline Template will load a column with each datatype to your destination.\n",
    "default: The Intro Pipeline Template contains the example from the docs intro page\n",
    "fruitshop: The Default Pipeline Template provides a simple starting point for your dlt pipeline\n",
    "\n",
    "---> github_api: The Github API templates provides a starting\n",
    "\n",
    "point to read data from REST APIs with REST Client helper\n",
    "requests: The Requests Pipeline Template provides a simple starting point for a dlt pipeline with the requests library\n",
    "```\n",
    "\n",
    "### Step 1. Initialize the source\n",
    "\n",
    "This command will initialize the pipeline example with the GitHub API as the source and DuckBD as the destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn0lrTt1YkVB"
   },
   "outputs": [],
   "source": [
    "!dlt --non-interactive init github_api duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6ix1K0nYIXt"
   },
   "source": [
    "Now, check  your files on the left side bar. It should contain all the necessary files to run your GitHub API -> DuckDB pipeline:\n",
    "\n",
    "- The `.dlt` folder containing `secrets.toml` and `config.toml`\n",
    "- The pipeline script `github_api_pipeline.py`\n",
    "- `requirements.txt`\n",
    "- `.gitignore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOxZcn8OZSWU"
   },
   "outputs": [],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR97cnHAJNe5"
   },
   "source": [
    "What you would normally do with the project:\n",
    "- Add your credentials and define configurations\n",
    "- Adjust the pipeline script as needed\n",
    "- Run the pipeline script\n",
    "\n",
    "> If needed, you can adjust the verified source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wql2dCjua0gY"
   },
   "outputs": [],
   "source": [
    "!cat github_api_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr3RWZSHcnSs"
   },
   "source": [
    "From the code, we can see that this pipeline loads **only the `\"issues\"` endpoint**.  \n",
    "You can adjust this code as needed: add new endpoints, include additional logic, apply transformations, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYYkrymgaORJ"
   },
   "source": [
    "### Step 2. Add credentials\n",
    "\n",
    "In Colab (or Molab), it is more convenient to use environment variables or `dlt.secrets`.\n",
    "\n",
    "In the pipeline above, the `access_token` parameter is set to `dlt.secrets.value`, which means you need to configure this variable:\n",
    "\n",
    "\n",
    "```python\n",
    "@dlt.resource(write_disposition=\"replace\")\n",
    "def github_api_resource(access_token: Optional[str] = dlt.secrets.value):\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-tu4f0aae1t"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "dlt.secrets[\"SOURCES__ACCESS_TOKEN\"] = userdata.get(\"SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8-evov4abv9"
   },
   "source": [
    "### Step 3. Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6Ud6jfCYWuY"
   },
   "source": [
    "Let's run the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQVPnS5YYnPC"
   },
   "outputs": [],
   "source": [
    "!python github_api_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imvWv_2Cbumt"
   },
   "source": [
    "From the pipeline output, we can get information such as the pipeline name, dataset name, destination path, and more.\n",
    "\n",
    "> Pipeline **github_api_pipeline** load step completed in 1.23 seconds  \n",
    "> 1 load package was loaded to the DuckDB destination and into the dataset **github_api_data**.  \n",
    "> The DuckDB destination used `duckdb:////content/**github_api_pipeline.duckdb**` as the storage location.  \n",
    "> Load package `1733848559.8195539` is **LOADED** and contains no failed jobs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeJEVZqQNPgE"
   },
   "source": [
    "## Step 4: Explore your data\n",
    "\n",
    "Let's explore what tables were created in the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D9uYTdLYqoG"
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(\"github_api_pipeline.duckdb\")\n",
    "conn.sql(\"SET search_path = 'github_api_data'\")\n",
    "conn.sql(\"DESCRIBE\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDrjDaZScSG_"
   },
   "outputs": [],
   "source": [
    "data_table = conn.sql(\"SELECT * FROM github_api_resource\").df()\n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SmS1Nskg4KP"
   },
   "source": [
    "# **Built-in sources: RestAPI, SQL database & Filesystem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV82MW55j-Qf"
   },
   "source": [
    "## **[RestAPI source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api/basic)**\n",
    "\n",
    "`rest_api` is a generic source that lets you create a `dlt` source from any REST API using a declarative configuration. Since most REST APIs follow similar patterns, this source provides a convenient way to define your integration declaratively.\n",
    "\n",
    "Using a [declarative configuration](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api/basic#source-configuration), you can specify:\n",
    "\n",
    "- the API endpoints to pull data from,\n",
    "- their relationships,\n",
    "- how to handle pagination,\n",
    "- authentication.\n",
    "\n",
    "`dlt` handles the rest for you: **unnesting the data, inferring the schema**, and **writing it to the destination**.\n",
    "\n",
    "In the previous lesson, you already used the REST API Client. `dlt`‚Äôs **[RESTClient](https://dlthub.com/docs/general-usage/http/rest-client)** is the **low-level abstraction** that powers the RestAPI source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqoKS0mNdFOd"
   },
   "source": [
    "### Initialize the `rest_api` template\n",
    "\n",
    "You can initialize the `rest_api` **template** using the `init` command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOXqRQgPdKIT"
   },
   "outputs": [],
   "source": [
    "!yes | dlt init rest_api duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ89LnH91GQh"
   },
   "source": [
    "In the `rest_api_pipeline.py` script, you will find sources for both the GitHub API and the PokeAPI, defined using the `rest_api` source and `RESTAPIConfig`.\n",
    "\n",
    "Since the `rest_api` source is a **built-in source**, you don't need to initialize it. You can simply **import** it from `dlt.sources` and start using it.\n",
    "\n",
    "### Example\n",
    "\n",
    "Here is a simplified example of how to configure the REST API source to load `issues` and issue `comments` from the GitHub API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U7d9Gvo0-LR"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.rest_api import RESTAPIConfig, rest_api_source\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "\n",
    "config: RESTAPIConfig = {\n",
    "    \"client\": {\n",
    "        \"base_url\": \"https://api.github.com\",\n",
    "        \"auth\": {\n",
    "            \"token\": dlt.secrets[\"sources.access_token\"],\n",
    "        },\n",
    "        \"paginator\": \"header_link\",\n",
    "    },\n",
    "    \"resources\": [\n",
    "        {\n",
    "            \"name\": \"issues\",\n",
    "            \"endpoint\": {\n",
    "                \"path\": \"repos/dlt-hub/dlt/issues\",\n",
    "                \"params\": {\n",
    "                    \"state\": \"open\",\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"issue_comments\",\n",
    "            \"endpoint\": {\n",
    "                \"path\": \"repos/dlt-hub/dlt/issues/{issue_number}/comments\",\n",
    "                \"params\": {\n",
    "                    \"issue_number\": {\n",
    "                        \"type\": (\"resolve\"),\n",
    "                        \"resource\": \"issues\",\n",
    "                        \"field\": \"number\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "github_source = rest_api_source(config)\n",
    "\n",
    "\n",
    "rest_api_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rest_api_github\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"rest_api_data\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "load_info = rest_api_pipeline.run(github_source)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJOstYElM6wB"
   },
   "outputs": [],
   "source": [
    "rest_api_pipeline.dataset().issues.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQuK4l23c8Of"
   },
   "source": [
    "### **Exercise 1: Run `rest_api` source**\n",
    "\n",
    "Explore the cells above and answer the question below using `sql_client` or `pipeline.dataset()`.\n",
    "\n",
    "#### **Question**\n",
    "How many columns does the `issues` table have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTKIM2ntOIrh"
   },
   "source": [
    "### **Exercise 2: Create a dlt source with `rest_api`**\n",
    "\n",
    "Add the `contributors` endpoint for the `dlt` repository to the `rest_api` configuration:\n",
    "\n",
    "- Resource name: **\"contributors\"**\n",
    "- Endpoint path: **\"repos/dlt-hub/dlt/contributors\"**\n",
    "- No parameters\n",
    "\n",
    "#### **Question**\n",
    "How many columns does the `contributors` table have?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpjPqN1_Z87N"
   },
   "source": [
    "---\n",
    "## **[SQL Databases source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/sql_database/)**\n",
    "\n",
    "SQL databases are management systems (DBMS) that store data in a structured format, commonly used for efficient and reliable data retrieval.\n",
    "\n",
    "The `sql_database` verified source loads data to your specified destination using one of the following backends:\n",
    "* SQLAlchemy,\n",
    "* PyArrow,\n",
    "* pandas,\n",
    "* ConnectorX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHcBOhgVdmZH"
   },
   "source": [
    "### Initialize the `sql_database` template\n",
    "\n",
    "Initialize the `dlt` template for `sql_database` using the `init` command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf1mmErIc8k-"
   },
   "outputs": [],
   "source": [
    "!yes | dlt init sql_database duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjeHVYE7Ms7g"
   },
   "source": [
    "The `sql_database` source is also a **built-in source**, you don't have to initialize it, just **import** it from `dlt.sources`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAG60opUd2r_"
   },
   "source": [
    "### Example\n",
    "\n",
    "The example below shows how you can use dlt to load data from a SQL database (PostgreSQL, MySQL, SQLite, Oracle, IBM DB2, etc.) into a destination.\n",
    "\n",
    "To make it easy to reproduce, we will load data from the [public MySQL Rfam database](https://docs.rfam.org/en/latest/database.html) into a local DuckDB instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTPcG3KDenfU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipA8wfRbdjzH"
   },
   "outputs": [],
   "source": [
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "sql_source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\n",
    "        \"family\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "sql_db_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_example\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "load_info = sql_db_pipeline.run(sql_source)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjyJyF4Ofyuu"
   },
   "source": [
    "### **Exercise 3: Run `sql_database` source**\n",
    "\n",
    "Explore the cells above and answer the question below using `sql_client` or `pipeline.dataset()`.\n",
    "\n",
    "#### **Question**\n",
    "How many columns does the `family` table have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seCYprSsaAnL"
   },
   "source": [
    "---\n",
    "## **[Filesystem source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp9xII0tgCMG"
   },
   "source": [
    "The filesystem source allows seamless loading of files from the following locations:\n",
    "\n",
    "* AWS S3\n",
    "* Google Cloud Storage\n",
    "* Google Drive\n",
    "* Azure Blob Storage\n",
    "* remote filesystem (via SFTP)\n",
    "* local filesystem\n",
    "\n",
    "The filesystem source natively supports CSV, Parquet, and JSONL files and allows customization for loading any type of structured file.\n",
    "\n",
    "\n",
    "**How filesystem source works**\n",
    "\n",
    "The Filesystem source doesn't just give you an easy way to load data from both remote and local files ‚Äî it also comes with a powerful set of tools that let you customize the loading process to fit your specific needs.\n",
    "\n",
    "Filesystem source loads data in two steps:\n",
    "\n",
    "1. It accesses the files in your remote or local file storage **without** actually **reading** the content yet. At this point, you can filter files by metadata or name. You can also set up incremental loading to load only new files.\n",
    "2. The **transformer** **reads** the files' content and yields the records. At this step, you can filter out the actual data, enrich records with metadata from files, or perform incremental loading based on the file content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfLjS_raUH9G"
   },
   "source": [
    "### Initialize the `filesystem` template\n",
    "\n",
    "Initialize the dlt template for `filesystem` using the `init` command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q25BBtqQUH9H"
   },
   "outputs": [],
   "source": [
    "!yes | dlt init filesystem duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4ptQy4NUH9H"
   },
   "source": [
    "The `filesystem` source is also a **built-in source**, you don't have to initialize it, just **import** it from `dlt.sources`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVK4dDRpNs6H"
   },
   "source": [
    "### Example\n",
    "\n",
    "To illustrate how this **built-in source** works, we first download some file to the local (Colab) filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uRhLsQcCVYk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "folder_name = \"local_data\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "full_path = os.path.abspath(folder_name)\n",
    "\n",
    "url = \"https://www.timestored.com/data/sample/userdata.parquet\"\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status()\n",
    "\n",
    "with open(f\"{full_path}/userdata.parquet\", \"wb\") as f:\n",
    "    f.write(resp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ja8hVK_HBfQ7"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.filesystem import filesystem, read_parquet\n",
    "\n",
    "filesystem_resource = filesystem(bucket_url=full_path, file_glob=\"**/*.parquet\")\n",
    "filesystem_pipe = filesystem_resource | read_parquet()\n",
    "\n",
    "# We load the data into the table_name table\n",
    "fs_pipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\n",
    "load_info = fs_pipeline.run(filesystem_pipe.with_name(\"userdata\"))\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jzeZeINEzQb"
   },
   "source": [
    "### **Exercise 4: Run `filesystem` source**\n",
    "\n",
    "Explore the cells above and answer the question below using `sql_client` or `pipeline.dataset()`.\n",
    "\n",
    "#### **Question**\n",
    "How many columns does the `userdata` table have?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4SGNHSkF7_Y"
   },
   "source": [
    "You can read how to configure **Cloud Storage** in the official  \n",
    "[dlt documentation](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/basic#configuration).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M03Zc9l7Y6Ue"
   },
   "source": [
    "# [**Built-in Destinations**](https://dlthub.com/docs/dlt-ecosystem/destinations/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_4_Using_pre_build_sources_and_destinations_img2](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_4_Using_pre_build_sources_and_destinations_img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7Y1oCAvJ79I"
   },
   "source": [
    "---\n",
    "##  **Exploring `dlt` destinations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWAnIbicE4XC"
   },
   "source": [
    "To be honest, this is simply a matter of going through the  \n",
    "[documentation](https://dlthub.com/docs/dlt-ecosystem/destinations/) üëÄ, but to sum it up:\n",
    "\n",
    "- Most likely, the destination where you want to load data is already a `dlt` integration that undergoes several hundred automated tests every day.\n",
    "- If not, you can define a custom destination and still benefit from most `dlt`-specific features.  \n",
    "  *FYI: custom destinations will be covered in the next Advanced course ‚Äî so we expect you to come back for part two‚Ä¶*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybn9apxwXnq6"
   },
   "source": [
    "## **Choosing a destination**\n",
    "\n",
    "Switching between destinations in `dlt` is incredibly straightforward. Simply modify the `destination` parameter in your pipeline configuration. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7-OJvvCaJnb"
   },
   "outputs": [],
   "source": [
    "data_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"data_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"data\",\n",
    ")\n",
    "print(data_pipeline.destination.destination_type)\n",
    "\n",
    "data_pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"data_pipeline\",\n",
    "    destination=\"bigquery\",\n",
    "    dataset_name=\"data\",\n",
    ")\n",
    "print(data_pipeline.destination.destination_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i08Q7tl8bkG"
   },
   "source": [
    "This flexibility allows you to easily transition from local development to production-grade environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiP_mMPnEIVN"
   },
   "source": [
    "## **Filesystem destination**\n",
    "\n",
    "The `filesystem` destination enables you to load data into **files stored locally** or in **cloud storage** solutions, making it an excellent choice for lightweight testing, prototyping, or file-based workflows.\n",
    "\n",
    "Below is an **example** demonstrating how to use the `filesystem` destination to load data in **Parquet** format:\n",
    "\n",
    "* Step 1: Set up a local bucket or cloud directory for storing files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kR5vqZe6EwTU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"BUCKET_URL\"] = \"./content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u68DGFNo9GgZ"
   },
   "source": [
    "* Step 2: Define the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQzemNAmEHi3"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\n",
    "        \"family\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"fs_pipeline\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"fs_data\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(source, loader_file_format=\"parquet\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGIcfEx-FpkK"
   },
   "source": [
    "Look at the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox1EFHgXFvVD"
   },
   "outputs": [],
   "source": [
    "! ls ./content/fs_data/family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MahQp-IUF1hy"
   },
   "source": [
    "Look at the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AW4g9zW2F7Y8"
   },
   "outputs": [],
   "source": [
    "# explore loaded data\n",
    "pipeline.dataset().family.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3kp-3wLQc7d"
   },
   "source": [
    "### **Table formats: [Delta tables & Iceberg](https://dlthub.com/docs/dlt-ecosystem/destinations/delta-iceberg)**\n",
    "\n",
    "dlt supports writing **Delta** and **Iceberg** tables when using the `filesystem` destination.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "dlt uses the `deltalake` and `pyiceberg` libraries to write Delta and Iceberg tables, respectively. One or multiple Parquet files are prepared during the extract and normalize steps. In the load step, these Parquet files are exposed as an Arrow data structure and fed into `deltalake` or `pyiceberg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20ovRJgy-YEU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"dlt[pyiceberg]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9HEPyKX-hi9"
   },
   "outputs": [],
   "source": [
    "load_info = pipeline.run(\n",
    "    source,\n",
    "    loader_file_format=\"parquet\",\n",
    "    table_format=\"iceberg\",\n",
    ")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmEu0FxZCd3J"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "The open-source version of dlt supports basic functionality for **Iceberg**, but the dltHub team is currently working on an **extended** and **more powerful** Iceberg integration.\n",
    "\n",
    "[Join the waiting list to learn more about dltHub and Iceberg.](https://info.dlthub.com/waiting-list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2KIyagJZEcs"
   },
   "source": [
    "# **Spoiler: Custom Sources & Destinations**\n",
    "\n",
    "`dlt` aims to simplify the process of creating both custom sources  \n",
    "([REST API Client](https://dlthub.com/docs/general-usage/http/rest-client),  \n",
    "[`rest_api` source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api))  \n",
    "and [custom destinations](https://dlthub.com/docs/dlt-ecosystem/destinations/destination).\n",
    "\n",
    "We will explore this topic in more detail in the next Advanced course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYbccmLie1zm"
   },
   "source": [
    "‚úÖ ‚ñ∂ Proceed to the [next lesson](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_5_write_disposition_and_incremental_loading.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
