{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTmIgQKpV355"
   },
   "source": [
    "# **Recap of [Lesson 3](https://colab.research.google.com/drive/1-jVNzMJTRYHhbRlXgGFlhMwdML1L9zMx#forceEdit=true&sandboxMode=true) üë©‚ÄçüíªüöÄ**\n",
    "\n",
    "1. Used pagination for RestAPIs.\n",
    "2. Used authentication for RestAPIs.\n",
    "3. Tried dlt RESTClient.\n",
    "4. Used environment variables to handle both secrets & configs.\n",
    "5. Learned how to add values to `secrets.toml` or `config.toml`.\n",
    "6. Used `secrets.toml` ENV variable special for Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gsY9pD3cw0O"
   },
   "source": [
    "---\n",
    "# **`dlt`‚Äôs pre-built Sources and Destinations** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_4_using_pre_build_sources_and_destinations.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_4_using_pre_build_sources_and_destinations.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "**Here, you will learn:**\n",
    "- How to initialize verified sources;\n",
    "- Built-in `rest_api` source.\n",
    "- Built-in `sql_database` source.\n",
    "- Built-in `filesystem` source.\n",
    "- How to switch between destinations.\n",
    "\n",
    "---\n",
    "\n",
    "Our verified sources are the simplest way to get started with building your stack. Choose from any of our fully customizable 30+ pre-built sources, such as any SQL database, Google Sheets, Salesforce and others.\n",
    "\n",
    "With our numerous destinations you can load data to a local database, warehouse or a data lake. Choose from Snowflake, Databricks and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_4_Using_pre_build_sources_and_destinations_img1](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_4_Using_pre_build_sources_and_destinations_img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo-vUEJuYSxg"
   },
   "source": [
    "# **Existing verified sources**\n",
    "To use an [existing verified source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/), just run the `dlt init` command.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dIUHlb1GTJl"
   },
   "source": [
    "\n",
    "There's a base project for each `dlt` verified source + destination combination, which you can adjust according to your needs.\n",
    "\n",
    "These base project can be initialized with a simple command:\n",
    "\n",
    "```\n",
    "dlt init <verified-source> <destination>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNs9mHKaEaTE"
   },
   "source": [
    "### Step 0: Install dlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XBc-KEm-YSRz"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dlt[duckdb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2HEi-5VYgas"
   },
   "source": [
    "List all verified sources:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mb1tDZmAYTzW"
   },
   "outputs": [],
   "source": [
    "!dlt init --list-sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KeAw139YfvZ"
   },
   "source": [
    "This command shows all available verified sources and their short descriptions. For each source, it checks if your local `dlt` version requires an update and prints the relevant warning.\n",
    "\n",
    "Consider an example of a pipeline for the GitHub API:\n",
    "\n",
    "```\n",
    "Available dlt single file templates:\n",
    "---\n",
    "arrow: The Arrow Pipeline Template will show how to load and transform arrow tables.\n",
    "dataframe: The DataFrame Pipeline Template will show how to load and transform pandas dataframes.\n",
    "debug: The Debug Pipeline Template will load a column with each datatype to your destination.\n",
    "default: The Intro Pipeline Template contains the example from the docs intro page\n",
    "fruitshop: The Default Pipeline Template provides a simple starting point for your dlt pipeline\n",
    "\n",
    "---> github_api: The Github API templates provides a starting\n",
    "\n",
    "point to read data from REST APIs with REST Client helper\n",
    "requests: The Requests Pipeline Template provides a simple starting point for a dlt pipeline with the requests library\n",
    "```\n",
    "\n",
    "### Step 1. Initialize the source\n",
    "\n",
    "This command will initialize the pipeline example with GitHub API as the source and DuckBD as the destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn0lrTt1YkVB"
   },
   "outputs": [],
   "source": [
    "!dlt --non-interactive init github_api duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6ix1K0nYIXt"
   },
   "source": [
    "Now, check  your files on the left side bar. It should contain all the necessary files to run your GitHub API -> DuckDB pipeline:\n",
    "* `.dlt` folder for `secrets.toml` and `config.toml`;\n",
    "* pipeline script `github_api_pipeline.py`;\n",
    "* requirements.txt;\n",
    "* `.gitignore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOxZcn8OZSWU"
   },
   "outputs": [],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR97cnHAJNe5"
   },
   "source": [
    "What you would normally do with the project:\n",
    "- Add your credentials and define configurations\n",
    "- Adjust the pipeline script as needed\n",
    "- Run the pipeline script\n",
    "\n",
    "> In certain cases, you can adjust the verified source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wql2dCjua0gY"
   },
   "outputs": [],
   "source": [
    "!cat github_api_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr3RWZSHcnSs"
   },
   "source": [
    "From the code we can see that this pipeline loads **only \"issues\" endpoint**, you can adjust this code as you wish: add new endpoints, add additional logic, add transformations, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYYkrymgaORJ"
   },
   "source": [
    "### Step 2. Add credentials\n",
    "\n",
    "In Colab is more convenient to use ENVs. In the previous lesson you learned how to configure dlt resource via environment variable.\n",
    "\n",
    "In the pipeline above we can see that `access_token` variable is `dlt.secrets.value`, it means we should configure this variable.\n",
    "\n",
    "```python\n",
    "@dlt.resource(write_disposition=\"replace\")\n",
    "def github_api_resource(access_token: Optional[str] = dlt.secrets.value):\n",
    "  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-tu4f0aae1t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"SOURCES__ACCESS_TOKEN\"] = userdata.get(\"SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8-evov4abv9"
   },
   "source": [
    "### Step 3. Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6Ud6jfCYWuY"
   },
   "source": [
    "Let's run the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQVPnS5YYnPC"
   },
   "outputs": [],
   "source": [
    "!python github_api_pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imvWv_2Cbumt"
   },
   "source": [
    "From the pipeline output we can take pipeline information like pipeline_name, dataset_name, destination path, etc.\n",
    "\n",
    "\n",
    "> Pipeline **github_api_pipeline** load step completed in 1.23 seconds\n",
    "1 load package(s) were loaded to destination duckdb and into dataset **github_api_data**\n",
    "The duckdb destination used duckdb:////content/**github_api_pipeline.duckdb** location to store data\n",
    "Load package 1733848559.8195539 is LOADED and contains no failed jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeJEVZqQNPgE"
   },
   "source": [
    "## Step 4: Explore your data\n",
    "\n",
    "Let's explore what tables were created in duckdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D9uYTdLYqoG"
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(\"github_api_pipeline.duckdb\")\n",
    "conn.sql(\"SET search_path = 'github_api_data'\")\n",
    "conn.sql(\"DESCRIBE\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDrjDaZScSG_"
   },
   "outputs": [],
   "source": [
    "data_table = conn.sql(\"SELECT * FROM github_api_resource\").df()\n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SmS1Nskg4KP"
   },
   "source": [
    "# **Built-in sources: RestAPI, SQL database & Filesystem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV82MW55j-Qf"
   },
   "source": [
    "## **[RestAPI source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api/basic)**\n",
    "\n",
    "`rest_api` is a generic source that you can use to create a `dlt` source from a REST API using a declarative configuration. The majority of REST APIs behave in a similar way; this `dlt` source attempts to provide a declarative way to define a `dlt` source for those APIs.\n",
    "\n",
    "Using a [declarative configuration](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api/basic#source-configuration), you can define:\n",
    "\n",
    "- the API endpoints to pull data from,\n",
    "- their relationships,\n",
    "- how to handle pagination,\n",
    "- authentication.\n",
    "\n",
    "dlt will take care of the rest: **unnesting the data, inferring the schema**, etc., and **writing to the destination**\n",
    "\n",
    "In previous lesson you've already met Rest API Client. `dlt`‚Äôs **[RESTClient](https://dlthub.com/docs/general-usage/http/rest-client)** is the **low level abstraction** that powers the REST API Source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqoKS0mNdFOd"
   },
   "source": [
    "### Initialize `rest_api` template\n",
    "You can initialize `rest_api` **template** using `init` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOXqRQgPdKIT"
   },
   "outputs": [],
   "source": [
    "!yes | dlt init rest_api duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJ89LnH91GQh"
   },
   "source": [
    "In the `rest_api_pipeline.py` script you will find sources for GitHub API and for PokeAPI, which were defined using `rest_api` source and `RESTAPIConfig`.\n",
    "\n",
    "Since the `rest_api` source is a **built-in source**, you don't have to initialize it. You can **import** it from `dlt.sources` and use it immediately.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Here's a simplified example of how to configure the REST API source to load `issues` and issue `comments` from GitHub API:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5U7d9Gvo0-LR"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.rest_api import RESTAPIConfig, rest_api_source\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "\n",
    "config: RESTAPIConfig = {\n",
    "    \"client\": {\n",
    "        \"base_url\": \"https://api.github.com\",\n",
    "        \"auth\": {\n",
    "            \"token\": dlt.secrets[\n",
    "                \"sources.access_token\"\n",
    "            ],  # <--- we already configured access_token above\n",
    "        },\n",
    "        \"paginator\": \"header_link\",  # <---- set up paginator type\n",
    "    },\n",
    "    \"resources\": [  # <--- list resources\n",
    "        {\n",
    "            \"name\": \"issues\",\n",
    "            \"endpoint\": {\n",
    "                \"path\": \"repos/dlt-hub/dlt/issues\",\n",
    "                \"params\": {\n",
    "                    \"state\": \"open\",\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"issue_comments\",  # <-- here we declare dlt.transformer\n",
    "            \"endpoint\": {\n",
    "                \"path\": \"repos/dlt-hub/dlt/issues/{issue_number}/comments\",\n",
    "                \"params\": {\n",
    "                    \"issue_number\": {\n",
    "                        \"type\": (\n",
    "                            \"resolve\"\n",
    "                        ),  # <--- use type 'resolve' to resolve {issue_number} for transformer\n",
    "                        \"resource\": \"issues\",\n",
    "                        \"field\": \"number\",\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"contributors\",\n",
    "            \"endpoint\": {\n",
    "                \"path\": \"repos/dlt-hub/dlt/contributors\",\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "github_source = rest_api_source(config)\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"rest_api_github\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"rest_api_data\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(github_source)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJOstYElM6wB"
   },
   "outputs": [],
   "source": [
    "pipeline.dataset().issues.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQuK4l23c8Of"
   },
   "source": [
    "### **Exercise 1: Run rest_api source**\n",
    "\n",
    "Explore the cells above and answer the question below using `sql_client` or `pipeline.dataset()`.\n",
    "\n",
    "#### Question\n",
    "How many columns has the `issues` table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTKIM2ntOIrh"
   },
   "source": [
    "###  **Exercise 2: Create dlt source with rest_api**\n",
    "\n",
    "Add `contributors` endpoint for dlt repository to the `rest_api` configuration:\n",
    "- resource name is \"contributors\"\n",
    "- endpoint path : \"repos/dlt-hub/dlt/contributors\"\n",
    "- no parameters\n",
    "\n",
    "#### Question\n",
    "How many columns has the `contributors` table?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpjPqN1_Z87N"
   },
   "source": [
    "---\n",
    "## **[SQL Databases source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/sql_database/)**\n",
    "\n",
    "SQL databases are management systems (DBMS) that store data in a structured format, commonly used for efficient and reliable data retrieval.\n",
    "\n",
    "The `sql_database` verified source loads data to your specified destination using one of the following backends:\n",
    "* SQLAlchemy,\n",
    "* PyArrow,\n",
    "* pandas,\n",
    "* ConnectorX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHcBOhgVdmZH"
   },
   "source": [
    "### Initialize `sql_database` template\n",
    "\n",
    "Initialize dlt template for `sql_database` using `init` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf1mmErIc8k-"
   },
   "outputs": [],
   "source": [
    "!yes | dlt init sql_database duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjeHVYE7Ms7g"
   },
   "source": [
    "The `sql_database` source is also a **built-in source**, you don't have to initialize it, just **import** it from `dlt.sources`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAG60opUd2r_"
   },
   "source": [
    "### Example\n",
    "\n",
    "The example below will show you how you can use dlt to load data from a SQL Database (PostgreSQL, MySQL, SQLight, Oracle, IBM DB2, etc.) into destination.\n",
    "\n",
    "To make it easy to reproduce, we will be loading data from the [public MySQL RFam database](https://docs.rfam.org/en/latest/database.html) into a local DuckDB instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTPcG3KDenfU"
   },
   "outputs": [],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipA8wfRbdjzH"
   },
   "outputs": [],
   "source": [
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\n",
    "        \"family\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"sql_database_example\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sql_data\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(source)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjyJyF4Ofyuu"
   },
   "source": [
    "### **Exercise 3: Run sql_database source**\n",
    "\n",
    "Explore the cells above and answer the question below using `sql_client` or `pipeline.dataset()`.\n",
    "\n",
    "#### Question\n",
    "How many columns does the `family` table have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seCYprSsaAnL"
   },
   "source": [
    "---\n",
    "## **[Filesystem source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp9xII0tgCMG"
   },
   "source": [
    "The filesystem source allows seamless loading of files from the following locations:\n",
    "\n",
    "* AWS S3\n",
    "* Google Cloud Storage\n",
    "* Google Drive\n",
    "* Azure Blob Storage\n",
    "* remote filesystem (via SFTP)\n",
    "* local filesystem\n",
    "\n",
    "The filesystem source natively supports CSV, Parquet, and JSONL files and allows customization for loading any type of structured file.\n",
    "\n",
    "\n",
    "**How filesystem source works**\n",
    "\n",
    "The Filesystem source doesn't just give you an easy way to load data from both remote and local files ‚Äî it also comes with a powerful set of tools that let you customize the loading process to fit your specific needs.\n",
    "\n",
    "Filesystem source loads data in two steps:\n",
    "\n",
    "1. It accesses the files in your remote or local file storage **without** actually **reading** the content yet. At this point, you can filter files by metadata or name. You can also set up incremental loading to load only new files.\n",
    "2. The **transformer** **reads** the files' content and yields the records. At this step, you can filter out the actual data, enrich records with metadata from files, or perform incremental loading based on the file content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfLjS_raUH9G"
   },
   "source": [
    "### Initialize `filesystem` template\n",
    "\n",
    "Initialize dlt template for `filesystem` using `init` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q25BBtqQUH9H"
   },
   "outputs": [],
   "source": [
    "!yes | dlt init filesystem duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4ptQy4NUH9H"
   },
   "source": [
    "The `filesystem` source is also a **built-in source**, you don't have to initialize it, just **import** it from `dlt.sources`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVK4dDRpNs6H"
   },
   "source": [
    "### Example\n",
    "\n",
    "To illustrate how this **built-in source** works, we first download some file to the local (Colab) filesystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uRhLsQcCVYk"
   },
   "outputs": [],
   "source": [
    "!mkdir -p local_data && wget -O local_data/userdata.parquet https://www.timestored.com/data/sample/userdata.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ja8hVK_HBfQ7"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.filesystem import filesystem, read_parquet\n",
    "\n",
    "filesystem_resource = filesystem(bucket_url=\"/content/local_data\", file_glob=\"**/*.parquet\")\n",
    "filesystem_pipe = filesystem_resource | read_parquet()\n",
    "\n",
    "# We load the data into the table_name table\n",
    "pipeline = dlt.pipeline(pipeline_name=\"my_pipeline\", destination=\"duckdb\")\n",
    "load_info = pipeline.run(filesystem_pipe.with_name(\"userdata\"))\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jzeZeINEzQb"
   },
   "source": [
    "### **Exercise 4: Run filesystem source**\n",
    "\n",
    "Explore the cells above and answer the question below using `sql_client` or `pipeline.dataset()`.\n",
    "\n",
    "#### Question\n",
    "How many columns does the `userdata` table have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4SGNHSkF7_Y"
   },
   "source": [
    "How to configure **Cloud Storage** you can read in the official [dlt documentation](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/basic#configuration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M03Zc9l7Y6Ue"
   },
   "source": [
    "# **Built-in Destinations**\n",
    "\n",
    "https://dlthub.com/docs/dlt-ecosystem/destinations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_4_Using_pre_build_sources_and_destinations_img2](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_4_Using_pre_build_sources_and_destinations_img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7Y1oCAvJ79I"
   },
   "source": [
    "---\n",
    "##  **Exploring `dlt` destinations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWAnIbicE4XC"
   },
   "source": [
    "TBH this is a matter of simply going through the [documentation](https://dlthub.com/docs/dlt-ecosystem/destinations/) üëÄ, but to sum it up:\n",
    "- Most likely the destination where you want to load data is already a `dlt` integration that undergoes several hundred automated tests every day.\n",
    "- If not, you can simply define a custom destination and still be able to benefit from most `dlt`-specific features. FYI, custom destinations will be covered in the next Advanced course, so we expect you to come back for the second part..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybn9apxwXnq6"
   },
   "source": [
    "## **Choosing a destination**\n",
    "\n",
    "Switching between destinations in dlt is incredibly straightforward‚Äîsimply modify the `destination` parameter in your pipeline configuration. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7-OJvvCaJnb"
   },
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"data_pipeline\",\n",
    "    destination=\"duckdb\",  # <--- to test pipeline locally\n",
    "    dataset_name=\"data\",\n",
    ")\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"data_pipeline\",\n",
    "    destination=\"bigquery\",  # <--- to run pipeline in production\n",
    "    dataset_name=\"data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i08Q7tl8bkG"
   },
   "source": [
    "This flexibility allows you to easily transition from local development to production-grade environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiP_mMPnEIVN"
   },
   "source": [
    "## **Filesystem destination**\n",
    "\n",
    "The `filesystem` destination enables you to load data into **files stored locally** or in **cloud storage** solutions, making it an excellent choice for lightweight testing, prototyping, or file-based workflows.\n",
    "\n",
    "Below is an **example** demonstrating how to use the `filesystem` destination to load data in **Parquet** format:\n",
    "\n",
    "* Step 1: Set up a local bucket or cloud directory for storing files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kR5vqZe6EwTU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"BUCKET_URL\"] = \"/content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u68DGFNo9GgZ"
   },
   "source": [
    "* Step 2: Define the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQzemNAmEHi3"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.sql_database import sql_database\n",
    "\n",
    "source = sql_database(\n",
    "    \"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\",\n",
    "    table_names=[\n",
    "        \"family\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"fs_pipeline\",\n",
    "    destination=\"filesystem\",  # <--- change destination to 'filesystem'\n",
    "    dataset_name=\"fs_data\",\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(\n",
    "    source, loader_file_format=\"parquet\"\n",
    ")  # <--- choose a file format: parquet, csv or jsonl\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGIcfEx-FpkK"
   },
   "source": [
    "Look at the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox1EFHgXFvVD"
   },
   "outputs": [],
   "source": [
    "! ls fs_data/family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MahQp-IUF1hy"
   },
   "source": [
    "Look at the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AW4g9zW2F7Y8"
   },
   "outputs": [],
   "source": [
    "# explore loaded data\n",
    "pipeline.dataset().family.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3kp-3wLQc7d"
   },
   "source": [
    "### **Table formats: [Delta tables & Iceberg](https://dlthub.com/docs/dlt-ecosystem/destinations/delta-iceberg)**\n",
    "\n",
    "dlt supports writing **Delta** and **Iceberg** tables when using the `filesystem` destination.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "dlt uses the `deltalake` and `pyiceberg` libraries to write Delta and Iceberg tables, respectively. One or multiple Parquet files are prepared during the extract and normalize steps. In the load step, these Parquet files are exposed as an Arrow data structure and fed into `deltalake` or `pyiceberg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20ovRJgy-YEU"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"dlt[pyiceberg]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9HEPyKX-hi9"
   },
   "outputs": [],
   "source": [
    "load_info = pipeline.run(\n",
    "    source,\n",
    "    loader_file_format=\"parquet\",\n",
    "    table_format=\"iceberg\",  # <--- choose a table format: delta or iceberg\n",
    ")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmEu0FxZCd3J"
   },
   "source": [
    "**Note:**\n",
    "\n",
    "Open source version of dlt supports basic functionality for **iceberg**, but the dltHub team is currently working on an **extended** and **more powerful** integration with iceberg.\n",
    "\n",
    "[Join the waiting list to learn more about dlt+ and Iceberg.](https://info.dlthub.com/waiting-list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2KIyagJZEcs"
   },
   "source": [
    "# **Spoiler: Custom Sources & Destinations**\n",
    "\n",
    "`dlt` tried to simplify as much as possible both the process of creating sources ([RestAPI Client](https://dlthub.com/docs/general-usage/http/rest-client), [rest_api source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/rest_api)) and [custom destinations](https://dlthub.com/docs/dlt-ecosystem/destinations/destination).\n",
    "\n",
    "We will look at this topic in more detail in the next Advanced course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYbccmLie1zm"
   },
   "source": [
    "‚úÖ ‚ñ∂ Proceed to the [next lesson](https://colab.research.google.com/drive/1Zf24gIVMNNj9j-gtXFl8p0orI9ttySDn#forceEdit=true&sandboxMode=true)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrVnW2UdVjV4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
