{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvMyiV0uMY-7"
   },
   "source": [
    "# **dlt sources and resources**: Create first dlt pipeline. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_2_dlt_sources_and_resources_create_first_dlt_pipeline.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_2_dlt_sources_and_resources_create_first_dlt_pipeline.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pjw4Z9HXZpoD"
   },
   "source": [
    "![Lesson_2_dlt_sources_and_resources_Create_first_dlt_pipeline_img1.png](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_2_dlt_sources_and_resources_Create_first_dlt_pipeline_img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZCRBANQftVQ"
   },
   "source": [
    "## Recap of [Lesson 1](https://colab.research.google.com/drive/1QwlDWxX5hvwbHMkCgiF0UCzGFRMRoSPY#forceEdit=true&sandboxMode=true) üë©‚ÄçüíªüöÄ\n",
    "1. Created a pipeline, loaded toy data into DuckDB, and viewed load info.\n",
    "2. Used `dlt.pipeline` and `pipeline.run` methods.\n",
    "3. Used DuckDB, `sql_client` and dlt `dataset` to view tables and query data.\n",
    "\n",
    "Now we move to the next lesson to learn more details about dlt! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nVmGk0_rVxI"
   },
   "source": [
    "**Here, you will learn how to:**\n",
    "- Run a simple pipeline with different types of data, such as dataframes, databases and RestAPI.\n",
    "- Use `dlt.resource`, `dlt.source` and `dlt.transformer`.\n",
    "- Build your first dlt pipeline for RestAPI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaLSnDr9hSxE"
   },
   "source": [
    "## **Install dlt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RLlJqBxNScV"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWNvOMGdSfMR"
   },
   "source": [
    "---\n",
    "## **`dlt` resources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpvBDAupT4-W"
   },
   "source": [
    "---\n",
    "### List of dicts\n",
    "\n",
    "\n",
    "In the previous lesson, we simply used a list of dictionaries that essentially represents the `pokemon` table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oLGx5YyhK2J"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "# Sample data containing pokemon details\n",
    "data = [\n",
    "    {\"id\": \"1\", \"name\": \"bulbasaur\", \"size\": {\"weight\": 6.9, \"height\": 0.7}},\n",
    "    {\"id\": \"4\", \"name\": \"charmander\", \"size\": {\"weight\": 8.5, \"height\": 0.6}},\n",
    "    {\"id\": \"25\", \"name\": \"pikachu\", \"size\": {\"weight\": 6, \"height\": 0.4}},\n",
    "]\n",
    "\n",
    "\n",
    "# Set pipeline name, destination, and dataset name\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"quick_start\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "# Run the pipeline with data and table name\n",
    "load_info = pipeline.run(data, table_name=\"pokemon\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7tdXqmLhi4O"
   },
   "source": [
    "A better way is to wrap it in the `@dlt.resource` decorator which denotes a logical grouping of data within a data source, typically holding data of similar structure and origin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k77LIUgHT3p9"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.common.typing import TDataItems, TDataItem\n",
    "\n",
    "\n",
    "# Create a dlt resource from the data\n",
    "@dlt.resource(table_name=\"pokemon_new\")  # <--- we set new table name\n",
    "def my_dict_list() -> TDataItems:\n",
    "    yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kyqKhhXYbZ_Q"
   },
   "source": [
    "Commonly used arguments:\n",
    "\n",
    "* **`name`**: The resource name and the name of the table generated by this resource. Defaults to the decorated function name.\n",
    "* **`table_name`**: the name of the table, if different from the resource name.\n",
    "* **`write_disposition`**: controls how to write data to a table. Defaults to the value \"append\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgbljmV9VOZP"
   },
   "source": [
    "> **Why is it a better way?** This allows you to use `dlt` functionalities to the fullest that follow Data Engineering best practices, including incremental loading and data contracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N9FgQsyV72S"
   },
   "source": [
    "Try running the pipeline with the `my_dict_list` resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbeFBp8xVTTi"
   },
   "outputs": [],
   "source": [
    "# Run the pipeline and print load info\n",
    "load_info = pipeline.run(my_dict_list)\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqhQ4jcEWJFF"
   },
   "source": [
    "Check what was loaded to the `pokemon_new` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BD5MibQbXI4u"
   },
   "outputs": [],
   "source": [
    "pipeline.dataset().pokemon_new.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u5UHF-TYTfz"
   },
   "source": [
    "Instead of a dict list, the data could also be a/an:\n",
    "- dataframe\n",
    "- database query response\n",
    "- API request response\n",
    "- Anything you can transform into JSON/dict format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNQcVr4CYywY"
   },
   "source": [
    "---\n",
    "### Dataframes\n",
    "For creating a pipeline using dataframes, you would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWnnFQDUYifh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define a resource to load data from a CSV\n",
    "@dlt.resource(table_name=\"df_data\")\n",
    "def my_df() -> TDataItems:\n",
    "    sample_df = pd.read_csv(\"https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\")\n",
    "    yield sample_df\n",
    "\n",
    "\n",
    "# Run the pipeline with the defined resource\n",
    "load_info = pipeline.run(my_df)\n",
    "print(load_info)\n",
    "\n",
    "# Query the loaded data from 'df_data'\n",
    "pipeline.dataset().df_data.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUdzmTGcZNY6"
   },
   "source": [
    "---\n",
    "### Database\n",
    "\n",
    "For creating a pipeline from an SQL database query you would:\n",
    "\n",
    "1. Install the PyMySQL package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAaONiqadjky"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktAAuuJqW792"
   },
   "source": [
    "2. Create and run a pipeline to fetch data from an SQL resource and query the loaded data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fuUFnGyfZWVT"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# Define a resource to fetch genome data from the database\n",
    "@dlt.resource(table_name=\"genome_data\")\n",
    "def get_genome_data() -> TDataItems:\n",
    "    engine = create_engine(\"mysql+pymysql://rfamro@mysql-rfam-public.ebi.ac.uk:4497/Rfam\")\n",
    "    with engine.connect() as conn:\n",
    "        query = \"SELECT * FROM genome LIMIT 1000\"\n",
    "        rows = conn.execution_options(yield_per=100).exec_driver_sql(query)\n",
    "        yield from map(lambda row: dict(row._mapping), rows)\n",
    "\n",
    "\n",
    "# Run the pipeline with the genome data resource\n",
    "load_info = pipeline.run(get_genome_data)\n",
    "print(load_info)\n",
    "\n",
    "# Query the loaded data from 'genome_data'\n",
    "pipeline.dataset().genome_data.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9EVGCcdaHIv"
   },
   "source": [
    "---\n",
    "### REST API\n",
    "\n",
    "For REST API endpoints, create a pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PxgFyU1zaOJ8"
   },
   "outputs": [],
   "source": [
    "from dlt.sources.helpers import requests\n",
    "\n",
    "\n",
    "# Define a resource to fetch pokemons from PokeAPI\n",
    "@dlt.resource(table_name=\"pokemon_api\")\n",
    "def get_pokemon() -> TDataItems:\n",
    "    url = \"https://pokeapi.co/api/v2/pokemon\"\n",
    "    response = requests.get(url)\n",
    "    yield response.json()[\"results\"]\n",
    "\n",
    "\n",
    "# Run the pipeline using the defined resource\n",
    "load_info = pipeline.run(get_pokemon)\n",
    "print(load_info)\n",
    "\n",
    "# Query the loaded data from 'pokemon_api' table\n",
    "pipeline.dataset().pokemon_api.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkZfoY12aqJv"
   },
   "source": [
    "Try loading everything above, in a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "6HURAgn7at9k"
   },
   "outputs": [],
   "source": [
    "load_info = pipeline.run([my_df, get_genome_data, get_pokemon])\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbJu9ozie7Kq"
   },
   "source": [
    "Check which new tables were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-y4CzNBcw5X"
   },
   "outputs": [],
   "source": [
    "# List all table names from the database\n",
    "with pipeline.sql_client() as client:\n",
    "    with client.execute_query(\"SELECT table_name FROM information_schema.tables\") as table:\n",
    "        print(table.df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTMJEbBae98_"
   },
   "source": [
    "\n",
    "---\n",
    "## **`dlt` sources**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtjtp-uRfCC5"
   },
   "source": [
    "Now that there are multiple `dlt` resources, each corresponding to a separate table, we can group them into a `dlt` source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDJjNcZVjPmd"
   },
   "source": [
    "![Lesson_2_dlt_sources_and_resources_Create_first_dlt_pipeline_img2](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_2_dlt_sources_and_resources_Create_first_dlt_pipeline_img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKQVYhjRjWqL"
   },
   "source": [
    "A source is a logical grouping of resources, e.g., endpoints of a single API. The most common approach is to define it in a separate Python module.\n",
    "\n",
    "* A source is a function decorated with `@dlt.source` that returns one or more resources.\n",
    "* A source can optionally define a schema with tables, columns, performance hints, and more.\n",
    "* The source Python module typically contains optional customizations and data transformations.\n",
    "* The source Python module typically contains the authentication and pagination code for a particular API.\n",
    "\n",
    "Read more about [sources](https://dlthub.com/docs/general-usage/source) and [resources](https://dlthub.com/docs/general-usage/resource) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wx33Ti-2frlV"
   },
   "source": [
    "You declare a source by decorating a function that returns or yields one or more resources with `@dlt.source`.\n",
    "\n",
    "Here's how it's done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PnQ0laa2fyGJ"
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from dlt.extract import DltResource\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def all_data() -> Iterable[DltResource]:\n",
    "    return my_df, get_genome_data, get_pokemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-ObA05Lgprc"
   },
   "source": [
    "Only using the source above, load everything into a separate database using a new pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "VbI8i6p_fBTs"
   },
   "outputs": [],
   "source": [
    "# Create a pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"resource_source_new\", destination=\"duckdb\", dataset_name=\"all_data\"\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "load_info = pipeline.run(all_data())\n",
    "\n",
    "# Print load info\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1qCOQh6iCwq"
   },
   "source": [
    "> **Why does this matter?**:\n",
    "- It is more efficient than running your resources separately.\n",
    "- It organizes both your schema and your code. üôÇ\n",
    "- It enables the option for parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFOi-9jAACsL"
   },
   "source": [
    "---\n",
    "## **`dlt` transformers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfzfHk_7AGSF"
   },
   "source": [
    "We now know that `dlt` resources can be grouped into a `dlt` source, represented as:\n",
    "\n",
    "\n",
    "```\n",
    "                  Source\n",
    "               /          \\\n",
    "          Resource 1  ...  Resource N\n",
    "\n",
    "```\n",
    "\n",
    "However, imagine a scenario where you need an additional step in between:\n",
    "\n",
    "```\n",
    "                  Source\n",
    "                 /     \\\n",
    "             step        \\\n",
    "             /             \\\n",
    "        Resource 1  ...  Resource N\n",
    "\n",
    "```\n",
    "\n",
    "This step could arise, for example, in a situation where:\n",
    "\n",
    "- Resource 1 returns a list of pokemons IDs, and you need to use each of those IDs to retrieve detailed information about the pokemons from a separate API endpoint.\n",
    "\n",
    "In such cases, you would use `dlt` transformers ‚Äî special `dlt` resources that can be fed data from another resource:\n",
    "\n",
    "```\n",
    "                  Source\n",
    "                 /     \\\n",
    "          Transformer    \\\n",
    "             /             \\\n",
    "        Resource 1  ...  Resource N\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6bH0VvpDDjx"
   },
   "source": [
    "Let‚Äôs assume Resource 1 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnPICY1SAEIJ"
   },
   "outputs": [],
   "source": [
    "@dlt.resource(table_name=\"pokemon\")\n",
    "def my_dict_list() -> TDataItems:\n",
    "    yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZOtxIyeFfhE"
   },
   "source": [
    "We need to get detailed information about pokemons from [PokeAPI](https://pokeapi.co/) `\"https://pokeapi.co/api/v2/pokemon/{id}\"` based on their IDs. We would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azswkLYVFtR8"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = [\n",
    "    {\"id\": \"1\", \"name\": \"bulbasaur\", \"size\": {\"weight\": 6.9, \"height\": 0.7}},\n",
    "    {\"id\": \"4\", \"name\": \"charmander\", \"size\": {\"weight\": 8.5, \"height\": 0.6}},\n",
    "    {\"id\": \"25\", \"name\": \"pikachu\", \"size\": {\"weight\": 6, \"height\": 0.4}},\n",
    "]\n",
    "\n",
    "\n",
    "# Define a resource to read and write data to pokemon table\n",
    "@dlt.resource(table_name=\"pokemon\")\n",
    "def my_dict_list() -> TDataItems:\n",
    "    yield data\n",
    "\n",
    "\n",
    "# Define a transformer to enrich pokemon data with additional details\n",
    "@dlt.transformer(data_from=my_dict_list, table_name=\"detailed_info\")\n",
    "def poke_details(\n",
    "    items: TDataItems,\n",
    ") -> TDataItems:  # <--- `items` is a variable and contains data from `my_dict_list` resource\n",
    "    for item in items:\n",
    "        print(f\"Item: {item}\\n\")  # <-- print what data we get from `my_dict_list` source\n",
    "\n",
    "        item_id = item[\"id\"]\n",
    "        url = f\"https://pokeapi.co/api/v2/pokemon/{item_id}\"\n",
    "        response = requests.get(url)\n",
    "        details = response.json()\n",
    "\n",
    "        print(f\"Details: {details}\\n\")  # <--- print what data we get from API\n",
    "\n",
    "        yield details\n",
    "\n",
    "\n",
    "# Set pipeline name, destination, and dataset name\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"quick_start\", destination=\"duckdb\", dataset_name=\"pokedata\", dev_mode=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjYIytigHnV_"
   },
   "source": [
    "Run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wCbmM2JeHp3o"
   },
   "outputs": [],
   "source": [
    "load_info = pipeline.run(poke_details())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpruvqxBIcq6"
   },
   "source": [
    "Alternatively, we could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWlIz6xAIfZM"
   },
   "outputs": [],
   "source": [
    "@dlt.resource(table_name=\"pokemon\")\n",
    "def my_dict_list() -> TDataItems:\n",
    "    yield from data  # <--- This would yield one item at a time\n",
    "\n",
    "\n",
    "@dlt.transformer(data_from=my_dict_list, table_name=\"detailed_info\")\n",
    "def details(data_item: TDataItem) -> TDataItems:  # <--- Transformer receives one item at a time\n",
    "    item_id = data_item[\"id\"]\n",
    "    url = f\"https://pokeapi.co/api/v2/pokemon/{item_id}\"\n",
    "    response = requests.get(url)\n",
    "    details = response.json()\n",
    "\n",
    "    yield details\n",
    "\n",
    "\n",
    "load_info = pipeline.run(details())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tw4lPMRukqOk"
   },
   "source": [
    "You can also use pipe instead of `data_from`, this is useful when you want to apply `dlt.transformer` to multiple `dlt.resources`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1t6fNmbkpZR"
   },
   "outputs": [],
   "source": [
    "load_info = pipeline.run(my_dict_list | details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6BLITRpJctR"
   },
   "source": [
    "Check the loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JK23SpTJfDz"
   },
   "outputs": [],
   "source": [
    "# Query the 'detailed_info' table and convert the result to a DataFrame\n",
    "pipeline.dataset().detailed_info.df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXsQCH5J8tOn"
   },
   "source": [
    "---\n",
    "## **Reduce the nesting level of generated tables**\n",
    "You can limit how deep dlt goes when generating nested tables and flattening dicts into columns. By default, the library will descend and generate nested tables for all nested lists, without limit.\n",
    "\n",
    "You can set nesting level for all resources on the source level:\n",
    "\n",
    "```python\n",
    "@dlt.source(max_table_nesting=1)\n",
    "def all_data():\n",
    "  return my_df, get_genome_data, get_pokemon\n",
    "```\n",
    "\n",
    "or for each resource separately:\n",
    "\n",
    "```python\n",
    "@dlt.resource(table_name='pokemon_new', max_table_nesting=1)\n",
    "def my_dict_list():\n",
    "    yield data\n",
    "```\n",
    "\n",
    "In the example above, we want only 1 level of nested tables to be generated (so there are no nested tables of a nested table). Typical settings:\n",
    "\n",
    "* `max_table_nesting=0` will not generate nested tables and will not flatten dicts into columns at all. All nested data will be represented as JSON.\n",
    "* `max_table_nesting=1` will generate nested tables of root tables and nothing more. All nested data in nested tables will be represented as JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9p1NvuIpq3jw"
   },
   "source": [
    "---\n",
    "## **Exercise 1: Create a pipeline for GitHub API - repos endpoint**\n",
    "\n",
    "In this exercise, you'll build a dlt pipeline to fetch data from the GitHub REST API. The goal is to learn how to use `dlt.pipeline`, `dlt.resource`, and `dlt.source` to extract and load data into a destination.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. **Explore the GitHub API**\n",
    "\n",
    "  Visit the [GitHub REST API Docs](https://docs.github.com/en/rest) to understand the endpoint to [list public repositories](https://docs.github.com/en/rest/repos/repos?apiVersion=2022-11-28) for an organization:\n",
    "\n",
    "  GET https://api.github.com/orgs/{org}/repos\n",
    "\n",
    "2. **Build the Pipeline**\n",
    "\n",
    "  Write a script to:\n",
    "\n",
    "  * Fetch repositories for a **dlt-hub** organization.\n",
    "  * Use `dlt.resource` to define the data extraction logic.\n",
    "  * Combine all resources to a single `@dlt.source`.\n",
    "  * Load the data into a DuckDB database.\n",
    "\n",
    "3. **Look at the data**\n",
    "\n",
    "  Use `duckdb` connection, `sql_client` or `pipeline.dataset()`.\n",
    "\n",
    "> **Note**: For this exercise you don't need to use Auth and Pagination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcBEFsCUuylN"
   },
   "source": [
    "Play with API using requests library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Ws7JhfPJvRTa"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://api.github.com/orgs/dlt-hub/repos\")\n",
    "response.json()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PUyt5LAXEMY"
   },
   "source": [
    "In the code snippet below you will find an **example** for the **`events`** endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHEc1vdpuWS2"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "\n",
    "# Example resource\n",
    "@dlt.resource\n",
    "def github_events() -> TDataItems:\n",
    "    url = \"https://api.github.com/orgs/dlt-hub/events\"\n",
    "    response = requests.get(url)\n",
    "    yield response.json()\n",
    "\n",
    "\n",
    "# here is your code\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def github_data() -> Iterable[DltResource]:\n",
    "    return (github_events,)  # github_repos\n",
    "\n",
    "\n",
    "# Set pipeline name, destination, and dataset name\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_pipeline\", destination=\"duckdb\", dataset_name=\"github_data\"\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(github_data())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16T2GpYZTSeH"
   },
   "source": [
    "### Question\n",
    "How many columns has the `github_repos` table? Use `duckdb` connection, `sql_client` or `pipeline.dataset()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYfeMBI82Tg0"
   },
   "source": [
    "## **Exercise 2: Create a pipeline for GitHub API - stargazers endpoint**\n",
    "\n",
    "Create a `dlt.transformer` for the \"stargazers\" endpoint\n",
    "https://api.github.com/repos/OWNER/REPO/stargazers for `dlt-hub` organization.\n",
    "\n",
    "Use `github_repos` resource as a main resource for the transformer:\n",
    "1. Get all `dlt-hub` repositories.\n",
    "2. Feed these repository names to dlt transformer and get all stargazers for all `dlt-hub` repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FaXP1YUKYjF5"
   },
   "outputs": [],
   "source": [
    "# here is your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ5SLh5VTec4"
   },
   "source": [
    "### Question\n",
    "How many columns has the `github_stargazer` table? Use `duckdb` connection, `sql_client` or `pipeline.dataset()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYbccmLie1zm"
   },
   "source": [
    "‚úÖ ‚ñ∂ Proceed to the [next lesson](https://colab.research.google.com/drive/1-jVNzMJTRYHhbRlXgGFlhMwdML1L9zMx#forceEdit=true&sandboxMode=true)!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
