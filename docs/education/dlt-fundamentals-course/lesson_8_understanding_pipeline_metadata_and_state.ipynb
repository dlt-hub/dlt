{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h93BcC8SX2fj"
   },
   "source": [
    "# **Recap of [Lesson 7](https://colab.research.google.com/drive/1LokUcM5YSazdq5jfbkop-Z5rmP-39y4r#forceEdit=true&sandboxMode=true) ðŸ‘©â€ðŸ’»ðŸš€**\n",
    "\n",
    "1. Learned what is a schema.\n",
    "2. Explored schema settings and components.\n",
    "3. Learned how to retrieve dlt pipeline schema.\n",
    "4. Learned how to adjust schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTAeTdoKJHZV"
   },
   "source": [
    "---\n",
    "\n",
    "# **Understanding Pipeline Metadata and State** ðŸ‘»ðŸ“„ [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_8_understanding_pipeline_metadata_and_state.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/lesson_8_understanding_pipeline_metadata_and_state.ipynb)\n",
    "\n",
    "\n",
    "**Here, you will learn or brush up on:**\n",
    "- What's pipeline metadata\n",
    "- Exploring pipeline metadata from load info\n",
    "- Exploring pipeline metadate from trace\n",
    "- Exploring pipeline metadata from state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7Y1oCAvJ79I"
   },
   "source": [
    "---\n",
    "##  **Pipeline Metadata**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFZNlDb1Y7ZH"
   },
   "source": [
    "Metadata is basically data about data.\n",
    "\n",
    "Pipeline Metadata is data about your data pipeline. This can be useful if you want to know things like:\n",
    "\n",
    "- When your pipeline first ran\n",
    "- When your pipeline last ran\n",
    "- Information about your source or destination\n",
    "- Processing time\n",
    "- Or information that you yourself may want to add to the metadata\n",
    "- And much more!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_8_Understanding_Pipeline_Metadata_and_State_img1](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_8_Understanding_Pipeline_Metadata_and_State_img1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY2ySVotY-JU"
   },
   "source": [
    " `dlt` allows you to be able to view all this metadata through various options!\n",
    "\n",
    "This notebook will walk you through those options. Namely:\n",
    "\n",
    "- Load info\n",
    "- Trace\n",
    "- State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTR2acUYZbku"
   },
   "source": [
    "Let's load some GitHub data to DuckDB to inspect the pipeline metadata in different ways. First we need to install dlt with DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOgUD5tdY2Df"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"dlt[duckdb]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhU2JVjTZn_j"
   },
   "source": [
    "Define a dlt resource that fetches Pull Requests and wrap it in a dlt source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1ovfVT4ZoP8"
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import dlt\n",
    "from dlt.extract import DltResource\n",
    "from dlt.common.typing import TDataItems\n",
    "from dlt.sources.helpers import requests\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"SOURCES__SECRET_KEY\"] = userdata.get(\"SECRET_KEY\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def github_source(secret_key: str = dlt.secrets.value) -> Iterable[DltResource]:\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://api.github.com\",\n",
    "        auth=BearerTokenAuth(token=secret_key),\n",
    "        paginator=HeaderLinkPaginator(),\n",
    "    )\n",
    "\n",
    "    @dlt.resource\n",
    "    def github_pulls(\n",
    "        cursor_date: dlt.sources.incremental[str] = dlt.sources.incremental(\n",
    "            \"updated_at\", initial_value=\"2024-12-01\"\n",
    "        )\n",
    "    ) -> TDataItems:\n",
    "        params = {\"since\": cursor_date.last_value, \"status\": \"open\"}\n",
    "        for page in client.paginate(\"repos/dlt-hub/dlt/pulls\", params=params):\n",
    "            yield page\n",
    "\n",
    "    return github_pulls\n",
    "\n",
    "\n",
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    ")\n",
    "\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf3wvdIXZ5Du"
   },
   "source": [
    "---\n",
    "##  **Load info**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8dlRUlOah0B"
   },
   "source": [
    "`Load Info:` This is a collection of useful information about the recently loaded data. It includes details like the pipeline and dataset name, destination information, and a list of loaded packages with their statuses, file sizes, types, and error messages (if any).\n",
    "\n",
    "`Load Package:` A load package is a collection of jobs with data for specific tables, generated during each execution of the pipeline. Each package is uniquely identified by a `load_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqsnfD4Iayu6"
   },
   "source": [
    "---\n",
    "###  **(0) CLI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA2dPY3_a2Ue"
   },
   "source": [
    "From the [`Inspecting & Adjusting Schema`](https://colab.research.google.com/drive/1LokUcM5YSazdq5jfbkop-Z5rmP-39y4r) Colab we've already learned that we can see which schema changes a load package has introduced with the command:\n",
    "\n",
    "```\n",
    "dlt pipeline -v <pipeline_name> load-package\n",
    "```\n",
    "\n",
    "The verbose flag only accounts for the schema changes, so if we run it without the flag, we will still see the most recent load package info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSdfV3d8Z-XX"
   },
   "outputs": [],
   "source": [
    "!dlt pipeline github_pipeline load-package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9ztJjzWcB3q"
   },
   "source": [
    "The `load_id` of a particular package is added to the top data tables (parent tables) and to the special `_dlt_loads` table with a status of 0 when the load process is fully completed. The `_dlt_loads` table tracks complete loads and allows chaining transformations on top of them.\n",
    "\n",
    "We can also see load package info with a specific load id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J4EOMiJ0cCId"
   },
   "outputs": [],
   "source": [
    "!dlt pipeline github_pipeline load-package 1741348101.3398592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bsLHRADcxUr"
   },
   "source": [
    "---\n",
    "###  **(0) Python**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg1lg6FVdKLl"
   },
   "source": [
    "From the [`Inspecting & Adjusting Schema`](https://colab.research.google.com/drive/1LokUcM5YSazdq5jfbkop-Z5rmP-39y4r?usp=sharing) Colab we've also learned that a schema can be accessed with:\n",
    "\n",
    "```python\n",
    "print(load_info.load_packages[0].schema)\n",
    "```\n",
    "Similarly if we drop the schema part, we will just get the load package info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQeh9zMKc0Cq"
   },
   "outputs": [],
   "source": [
    "print(load_info.load_packages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEZZHIrZdwc_"
   },
   "source": [
    "which has the following public methods and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aues6jCVdxtV"
   },
   "outputs": [],
   "source": [
    "# This code snippet just prints out the public methoda and attributes of the schema object in load info\n",
    "all_attributes_methods = dir(load_info.load_packages[0])\n",
    "public_attributes_methods = [attr for attr in all_attributes_methods if not attr.startswith(\"_\")]\n",
    "\n",
    "print(f\"{'Attribute/Method':<50} {'Type':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for attr in public_attributes_methods:\n",
    "    attr_value = getattr(load_info.load_packages[0], attr)\n",
    "    if callable(attr_value):\n",
    "        print(f\"{attr:<50} {'method':<10}\")\n",
    "    else:\n",
    "        print(f\"{attr:<50} {'attribute':<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-crwFCcd6ju"
   },
   "source": [
    "---\n",
    "##  **Trace**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdKksdLxePOS"
   },
   "source": [
    " `Trace`: A trace is a detailed record of the execution of a pipeline. It provides rich information on the pipeline processing steps: **extract**, **normalize**, and **load**. It also shows the last `load_info`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7u1TnL3eev8"
   },
   "source": [
    "---\n",
    "###  **(0) CLI**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3_rFHz6elTy"
   },
   "source": [
    "You can access pipeline trace using the command:\n",
    "\n",
    "\n",
    "```\n",
    "dlt pipeline <pipeline_name> trace\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2B3-30Yezbi"
   },
   "source": [
    "Try on the github issues pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDxOCqgieiBN"
   },
   "outputs": [],
   "source": [
    "!dlt pipeline github_pipeline trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8jaV5fnfAxJ"
   },
   "source": [
    "---\n",
    "###  **(0) Python**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkaT2kwsfFDQ"
   },
   "source": [
    "We can also print out the trace in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h_NVxCPhfC0O"
   },
   "outputs": [],
   "source": [
    "# print human friendly trace information\n",
    "print(pipeline.last_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LSqATD9fNH_"
   },
   "source": [
    "Separately receive the extract stage info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L4CGuf6fSWw"
   },
   "outputs": [],
   "source": [
    "# print human friendly trace information\n",
    "print(pipeline.last_trace.last_extract_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56YQEoeIfZSV"
   },
   "source": [
    "As well as the normalization stage info with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7lZGP5-fdZv"
   },
   "outputs": [],
   "source": [
    "# print human friendly normalization information\n",
    "print(pipeline.last_trace.last_normalize_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMsVhKYHff20"
   },
   "source": [
    "In particular how many rows of data were normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvX2Zn1AfjtK"
   },
   "outputs": [],
   "source": [
    "# access row counts dictionary of normalize info\n",
    "print(pipeline.last_trace.last_normalize_info.row_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz3V3w1YflmO"
   },
   "source": [
    "And finally the load stage info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP1m2Cchfm9Z"
   },
   "outputs": [],
   "source": [
    "# print human friendly load information\n",
    "print(pipeline.last_trace.last_load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tmZYJjFfrBo"
   },
   "source": [
    "---\n",
    "##  **State**\n",
    "\n",
    "[`The pipeline state`](https://dlthub.com/docs/general-usage/state) is a Python dictionary that lives alongside your data. You can store values in it during a pipeline run, and then retrieve them in the next pipeline run. It's used for tasks like preserving the \"last value\" or similar loading checkpoints, and it gets committed atomically with the data. The state is stored locally in the pipeline working directory and is also stored at the destination for future runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDT-ETyog_MC"
   },
   "source": [
    "**When to use pipeline state**\n",
    "- dlt uses the state internally to implement last value incremental loading. This use case should cover around 90% of your needs to use the pipeline state.\n",
    "- Store a list of already requested entities if the list is not much bigger than 100k elements.\n",
    "- Store large dictionaries of last values if you are not able to implement it with the standard incremental construct.\n",
    "- Store the custom fields dictionaries, dynamic configurations and other source-scoped state.\n",
    "\n",
    "**When not to use pipeline state**\n",
    "\n",
    "Do not use dlt state when it may grow to millions of elements. Do you plan to store modification timestamps of all of your millions of user records? This is probably a bad idea! In that case you could:\n",
    "\n",
    "- Store the state in dynamo-db, redis etc. taking into the account that if the extract stage fails you'll end with invalid state.\n",
    "- Use your loaded data as the state. dlt exposes the current pipeline via dlt.current.pipeline() from which you can obtain sqlclient and load the data of interest. In that case try at least to process your user records in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su-uPFp8hKqV"
   },
   "source": [
    "---\n",
    "###  **(0) CLI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrVdxJBVhOrR"
   },
   "outputs": [],
   "source": [
    "!dlt pipeline -v github_pipeline info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Mu9452ghMSU"
   },
   "source": [
    "---\n",
    "###  **(1) Python**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXt3-wtxftym"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def read_state(filepath: str) -> str:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        pretty_json = json.dumps(data, indent=4)\n",
    "        return pretty_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0NUV6IFhiuy"
   },
   "outputs": [],
   "source": [
    "# stored in your default pipelines folder\n",
    "print(read_state(\"/var/dlt/pipelines/github_pipeline/state.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppcsqSq6iWHw"
   },
   "source": [
    "---\n",
    "###  **Modify State**\n",
    "\n",
    "The pipeline state is a Python dictionary that lives alongside your data; you can store values in it and, on the next pipeline run, request them back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSlTey2Xib92"
   },
   "source": [
    "---\n",
    "####  **(0) Resource state**\n",
    "\n",
    "You can **read** and **write** the state in your resources using:\n",
    "\n",
    "```python\n",
    "dlt.current.resource_state().get()\n",
    "```\n",
    "and\n",
    "\n",
    "```python\n",
    "dlt.current.resource_state().setdefault(key, value)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPIjNZ2Rihgt"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers import requests\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"SOURCES__SECRET_KEY\"] = userdata.get(\"SECRET_KEY\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def github_source(secret_key: str = dlt.secrets.value) -> Iterable[DltResource]:\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://api.github.com\",\n",
    "        auth=BearerTokenAuth(token=secret_key),\n",
    "        paginator=HeaderLinkPaginator(),\n",
    "    )\n",
    "\n",
    "    @dlt.resource\n",
    "    def github_pulls(\n",
    "        cursor_date: dlt.sources.incremental[str] = dlt.sources.incremental(\n",
    "            \"updated_at\", initial_value=\"2024-12-01\"\n",
    "        )\n",
    "    ) -> TDataItems:\n",
    "        # Let's set some custom state information\n",
    "        dlt.current.resource_state().setdefault(\n",
    "            \"new_key\", [\"first_value\", \"second_value\"]\n",
    "        )  # <--- new item in the state\n",
    "\n",
    "        params = {\"since\": cursor_date.last_value, \"status\": \"open\"}\n",
    "        for page in client.paginate(\"repos/dlt-hub/dlt/pulls\", params=params):\n",
    "            yield page\n",
    "\n",
    "    return github_pulls\n",
    "\n",
    "\n",
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    ")\n",
    "\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unrjGJ2ziijv"
   },
   "outputs": [],
   "source": [
    "print(read_state(\"/var/dlt/pipelines/github_pipeline/state.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEBszW96bX1F"
   },
   "source": [
    "In the state you will see the new items:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_8_Understanding_Pipeline_Metadata_and_State_img2](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_8_Understanding_Pipeline_Metadata_and_State_img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYaVqhlmbFGx"
   },
   "source": [
    "You can modify any item in the state dict:\n",
    "\n",
    "```python\n",
    "new_keys = dlt.current.resource_state().setdefault(\"new_key\", [\"first_value\", \"second_value\"])\n",
    "\n",
    "if \"something_happend\":\n",
    "    new_keys.append(\"third_value\")\n",
    "\n",
    "incremental_dict = dlt.current.resource_state().get(\"incremental\")\n",
    "incremental_dict.update({\"second_new_key\": \"fourth_value\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbKXPZnebSbN"
   },
   "source": [
    "Full example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuIhA7oSkUgu"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers import requests\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"SOURCES__SECRET_KEY\"] = userdata.get(\"SECRET_KEY\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def github_source(secret_key: str = dlt.secrets.value) -> Iterable[DltResource]:\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://api.github.com\",\n",
    "        auth=BearerTokenAuth(token=secret_key),\n",
    "        paginator=HeaderLinkPaginator(),\n",
    "    )\n",
    "\n",
    "    @dlt.resource\n",
    "    def github_pulls(\n",
    "        cursor_date: dlt.sources.incremental[str] = dlt.sources.incremental(\n",
    "            \"updated_at\", initial_value=\"2024-12-01\"\n",
    "        )\n",
    "    ) -> TDataItems:\n",
    "        # Play with state even more\n",
    "        new_keys = dlt.current.resource_state().setdefault(\n",
    "            \"new_key\", [\"first_value\", \"second_value\"]\n",
    "        )\n",
    "\n",
    "        if \"something_happened\":\n",
    "            new_keys.append(\"third_value\")\n",
    "\n",
    "        incremental_dict = dlt.current.resource_state().get(\"incremental\")\n",
    "        incremental_dict.update({\"second_new_key\": \"fourth_value\"})\n",
    "\n",
    "        params = {\"since\": cursor_date.last_value, \"status\": \"open\"}\n",
    "        for page in client.paginate(\"repos/dlt-hub/dlt/pulls\", params=params):\n",
    "            yield page\n",
    "\n",
    "    return github_pulls\n",
    "\n",
    "\n",
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    ")\n",
    "\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAFdwyK-kdag"
   },
   "outputs": [],
   "source": [
    "print(read_state(\"/var/dlt/pipelines/github_pipeline/state.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TKr9dJfifKH"
   },
   "source": [
    "---\n",
    "####  **(1) Source state**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im-o7K5IkoW5"
   },
   "source": [
    "You can also access the source-scoped state with `dlt.current.source_state()` which can be shared across resources of a particular source and is also available **read-only** in the source-decorated functions. The most common use case for the source-scoped state is to store mapping of custom fields to their displayable names.\n",
    "\n",
    "Let's read some custom keys from the state:\n",
    "```python\n",
    "# Let's read some custom state information\n",
    "source_new_keys = dlt.current.source_state().get(\"resources\", {}).get(\"github_pulls\", {}).get(\"new_key\")\n",
    "```\n",
    "Full example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-XYbem9ksbD"
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.helpers import requests\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"SOURCES__SECRET_KEY\"] = userdata.get(\"SECRET_KEY\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def github_source(secret_key: str = dlt.secrets.value) -> Iterable[DltResource]:\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://api.github.com\",\n",
    "        auth=BearerTokenAuth(token=secret_key),\n",
    "        paginator=HeaderLinkPaginator(),\n",
    "    )\n",
    "\n",
    "    @dlt.resource\n",
    "    def github_pulls(\n",
    "        cursor_date: dlt.sources.incremental[str] = dlt.sources.incremental(\n",
    "            \"updated_at\", initial_value=\"2024-12-01\"\n",
    "        )\n",
    "    ) -> TDataItems:\n",
    "        params = {\"since\": cursor_date.last_value, \"status\": \"open\"}\n",
    "        for page in client.paginate(\"repos/dlt-hub/dlt/pulls\", params=params):\n",
    "            yield page\n",
    "\n",
    "        # Let's read some custom state information\n",
    "        source_new_keys = (\n",
    "            dlt.current.source_state().get(\"resources\", {}).get(\"github_pulls\", {}).get(\"new_key\")\n",
    "        )\n",
    "        print(\"My custom values: \", source_new_keys)\n",
    "\n",
    "    return github_pulls\n",
    "\n",
    "\n",
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    ")\n",
    "\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAyLrlFclBnh"
   },
   "source": [
    "---\n",
    "###  **Sync State**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIhvQCY_lEaB"
   },
   "source": [
    "What if you run your pipeline on, for example, Airflow where every task gets a clean filesystem and pipeline working directory is always deleted?\n",
    "\n",
    "**dlt loads** your **state** into the destination **together** with all other **data** and when faced with a clean start, it will try to restore state from the destination.\n",
    "\n",
    "The remote state is identified by pipeline name, the destination location (as given by the credentials) and destination dataset. To re-use **the same state**, use **the same pipeline name** and destination.\n",
    "\n",
    "The state is stored in the `_dlt_pipeline_state` table at the destination and contains information about the pipeline, pipeline run (that the state belongs to) and state blob.\n",
    "\n",
    "dlt has `dlt pipeline <pipeline name> sync` command where you can request the state back from that table.\n",
    "\n",
    "ðŸ’¡ If you can keep the pipeline working directory across the runs, you can disable the state sync by setting `restore_from_destination=false` i.e. in your `config.toml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qkFpGvDdlCae"
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from google.colab import data_table\n",
    "from IPython.display import display\n",
    "\n",
    "data_table.enable_dataframe_formatter()\n",
    "\n",
    "# a database 'chess_pipeline.duckdb' was created in working directory so just connect to it\n",
    "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
    "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
    "stats_table = conn.sql(\"SELECT * FROM _dlt_pipeline_state\").df()\n",
    "display(stats_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIy5yLOAlJ9M"
   },
   "source": [
    "Column \"state\" is compressed json dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OAB-Ew4Xdmc"
   },
   "source": [
    "|index|version|engine\\_version|pipeline\\_name|state|created\\_at|version\\_hash|\\_dlt\\_load\\_id|\\_dlt\\_id|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|0|1|4|github\\_pipeline|eNplkN....6+/m/QA7mbNc|2025-03-10 14:02:34\\.340458+00:00|pnp+9AIA5jAGx5LKon6zWmPnfYVb10ROa5aIKjv9O0I=|1741615353\\.5473728|FOzn5XuSZ/y/BQ|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZV_N7YYrlKLg"
   },
   "outputs": [],
   "source": [
    "!dlt --non-interactive pipeline github_pipeline sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFu0ySYElRke"
   },
   "source": [
    "---\n",
    "###  **Reset State**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMJTUiMTl4u_"
   },
   "source": [
    "**To fully reset the state:**\n",
    "\n",
    "Drop the destination dataset to fully reset the pipeline.\n",
    "Set the `dev_mode` flag when creating pipeline.\n",
    "Use the `dlt pipeline drop --drop-all` command to drop state and tables for a given schema name.\n",
    "\n",
    "**To partially reset the state:**\n",
    "\n",
    "Use the `dlt pipeline drop <resource_name>` command to drop state and tables for a given resource.\n",
    "Use the `dlt pipeline drop --state-paths` command to reset the state at given path without touching the tables and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUuRzapCl8pC"
   },
   "source": [
    "**Example for partial reset:**\n",
    "\n",
    ">  in an ipynb environment, when the duckdb connection we opened is not yet closed -> close the connection before attempting to edit the pipeline through the CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Zs65pFClWCs"
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CfoBcQ2tmCzN"
   },
   "outputs": [],
   "source": [
    "!yes | dlt pipeline github_pipeline drop github_pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCLO5etjmFiJ"
   },
   "outputs": [],
   "source": [
    "!dlt pipeline -v github_pipeline info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYbccmLie1zm"
   },
   "source": [
    "ðŸŽŠðŸŽŠðŸŽŠ That is actually it! We hope you enjoyed this course and learned more about dlt! ðŸŽŠðŸŽŠðŸŽŠ\n",
    "\n",
    "Please share your feedback with us: [Feedback Google Form](https://forms.gle/1NYrGcRj5gLQ4WDt8) ðŸŒ¼"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
