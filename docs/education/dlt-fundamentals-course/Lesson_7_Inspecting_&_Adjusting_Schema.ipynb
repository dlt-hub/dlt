{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h93BcC8SX2fj"
   },
   "source": [
    "# **Recap of [Lesson 6](https://colab.research.google.com/drive/1geSMNRkSwAelQJKd3e8vdoHCKiHMdmIo#forceEdit=true&sandboxMode=true) 👩‍💻🚀**\n",
    "\n",
    "1. Learned how dlt works under the hood;\n",
    "2. Explored 3 main steps:\n",
    "  * Extract;\n",
    "  * Normalize;\n",
    "  * Load.\n",
    "3. Learned which file formats dlt supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTAeTdoKJHZV"
   },
   "source": [
    "---\n",
    "\n",
    "# **Inspecting & Adjusting Schema** 🧠🧠 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/Lesson_7_Inspecting_&_Adjusting_Schema.ipynb) [![GitHub badge](https://img.shields.io/badge/github-view_source-2b3137?logo=github)](https://github.com/dlt-hub/dlt/blob/master/docs/education/dlt-fundamentals-course/Lesson_7_Inspecting_&_Adjusting_Schema.ipynb)\n",
    "\n",
    "\n",
    "**Here, you will learn or refresh your knowledge on:**\n",
    "- Methods to inspect a schema\n",
    "- The components of a schema\n",
    "- How to modify a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7Y1oCAvJ79I"
   },
   "source": [
    "---\n",
    "##  **Methods to inspect a schema**\n",
    "\n",
    "- **What's a schema?** The schema describes the structure of normalized data (e.g. tables, columns, data types, etc.). `dlt` generates schemas from the data during the normalization process.\n",
    "\n",
    "- **How can you inspect a schema in `dlt`?** There are multiple ways:\n",
    "  - CLI\n",
    "  - Python\n",
    "  - Export schema directly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vRudCVb9zII"
   },
   "source": [
    "Let's load some GitHub data to DuckDB to inspect the schema in different ways. First we need to install dlt with DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxdRin94-CYD"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKvf4NWW-U9V"
   },
   "source": [
    "Define a dlt resource that fetches pull requests and wrap it in a dlt source, create a pipeline and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppf4XBe_-a0m"
   },
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import dlt\n",
    "from dlt.common.typing import TDataItems\n",
    "from dlt.extract import DltResource\n",
    "from dlt.sources.helpers import requests\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.auth import BearerTokenAuth\n",
    "from dlt.sources.helpers.rest_client.paginators import HeaderLinkPaginator\n",
    "\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"SOURCES__SECRET_KEY\"] = userdata.get(\"SECRET_KEY\")\n",
    "\n",
    "\n",
    "@dlt.source\n",
    "def github_source(secret_key: str = dlt.secrets.value) -> Iterable[DltResource]:\n",
    "    client = RESTClient(\n",
    "        base_url=\"https://api.github.com\",\n",
    "        auth=BearerTokenAuth(token=secret_key),\n",
    "        paginator=HeaderLinkPaginator(),\n",
    "    )\n",
    "\n",
    "    @dlt.resource\n",
    "    def github_pulls(\n",
    "        cursor_date: dlt.sources.incremental[str] = dlt.sources.incremental(\n",
    "            \"updated_at\", initial_value=\"2024-12-01\"\n",
    "        )\n",
    "    ) -> TDataItems:\n",
    "        params = {\"since\": cursor_date.last_value, \"status\": \"open\"}\n",
    "        for page in client.paginate(\"repos/dlt-hub/dlt/pulls\", params=params):\n",
    "            yield page\n",
    "\n",
    "    return github_pulls\n",
    "\n",
    "\n",
    "# define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_pipeline1\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    ")\n",
    "\n",
    "\n",
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO8urJdQ9iaN"
   },
   "source": [
    "---\n",
    "### **(0) CLI**\n",
    "\n",
    "Let's first try the CLI command `dlt pipeline -v <pipeline_name> load-package`, which is used to inspect a load package in verbose mode.\n",
    "\n",
    "> In the context of the `dlt` library, a load package is a collection of jobs with data for particular tables. The -v flag stands for verbose, which means the command will provide more detailed output.\n",
    "\n",
    "Specifically, this command will show the schema changes introduced in the load package for the given pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUcSd9a2_ij5"
   },
   "outputs": [],
   "source": [
    "!dlt pipeline -v github_pipeline1 load-package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjv_Icl4ASet"
   },
   "source": [
    "---\n",
    "### **(1) Python**\n",
    "\n",
    "Alternatively, we can inspect the schema object from load info with:\n",
    "\n",
    "```python\n",
    "print(load_info.load_packages[0].schema)\n",
    "```\n",
    "\n",
    "which has the following public methods and attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLKFyyeABqVi"
   },
   "outputs": [],
   "source": [
    "# This code snippet just prints out the public methods and attributes of the schema object in load info\n",
    "all_attributes_methods = dir(load_info.load_packages[0].schema)\n",
    "public_attributes_methods = [attr for attr in all_attributes_methods if not attr.startswith(\"_\")]\n",
    "\n",
    "print(f\"{'Attribute/Method':<50} {'Type':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for attr in public_attributes_methods:\n",
    "    attr_value = getattr(load_info.load_packages[0].schema, attr)\n",
    "    if callable(attr_value):\n",
    "        print(f\"{attr:<50} {'method':<10}\")\n",
    "    else:\n",
    "        print(f\"{attr:<50} {'attribute':<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nCDy7WfCZcc"
   },
   "source": [
    "Let's use the `to_pretty_json` method and print the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I45Tm6MGAafF"
   },
   "outputs": [],
   "source": [
    "print(load_info.load_packages[0].schema.to_pretty_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDPXaNDiCqTd"
   },
   "source": [
    "---\n",
    "### **(2) Exporting schema**\n",
    "\n",
    "> Exporting the data schema directly into a file might be even more straightforward than the two previous approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mzk_J2BGC9US"
   },
   "source": [
    "The instruction to export a schema should be provided at the beginning when creating a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmJ0D1ExDCRL"
   },
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_pipeline2\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    "    export_schema_path=\"schemas/export\",  # <--- dir path for a schema export\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8Y5oeVaDV6s"
   },
   "source": [
    "Run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stm0UC0GDXcV"
   },
   "outputs": [],
   "source": [
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_7_Inspecting_%26_Adjusting_Schema_img1](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_7_Inspecting_%26_Adjusting_Schema_img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRJLORoDZLkq"
   },
   "source": [
    "Check if the schema was exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcrvR0O52REV"
   },
   "outputs": [],
   "source": [
    "!ls schemas/export && cat schemas/export/github_source.schema.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6jBWWFi8po-"
   },
   "source": [
    "---\n",
    "##  **The components of a schema**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFdjkL5LL7tx"
   },
   "source": [
    "> Since we learned the ways we can inspect the schema, it's important to actually understand what it contains to be able to meaningfully adjust it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCWhPUDwMWSV"
   },
   "source": [
    "A schema (in YAML format) looks somethng like this:\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "version_hash: wdIt+pExjT8Mj1ygQEMhq3E3SXtNBuIbHg0fDz9xD9I=\n",
    "engine_version: 11\n",
    "name: github_source\n",
    "tables:\n",
    "  _dlt_version:\n",
    "    ...\n",
    "  _dlt_loads:\n",
    "    ...\n",
    "  github_pulls:\n",
    "    ...\n",
    "settings:\n",
    "  detections:\n",
    "  - iso_timestamp\n",
    "  default_hints:\n",
    "    not_null:\n",
    "    - _dlt_id\n",
    "    - _dlt_root_id\n",
    "    - _dlt_parent_id\n",
    "    - _dlt_list_idx\n",
    "    - _dlt_load_id\n",
    "    parent_key:\n",
    "    - _dlt_parent_id\n",
    "    root_key:\n",
    "    - _dlt_root_id\n",
    "    unique:\n",
    "    - _dlt_id\n",
    "    row_key:\n",
    "    - _dlt_id\n",
    "normalizers:\n",
    "  names: snake_case\n",
    "  json:\n",
    "    module: dlt.common.normalizers.json.relational\n",
    "previous_hashes:\n",
    "- 0WLnuf3Jh1J1XsbVrV2eB824Z6heOlf5o912i1v3tho=\n",
    "- 0d1z0RFV2O0OvfEWkebtSjxrCjjiyv1lOeNiF0V8Lws=\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo4RoHN3MowJ"
   },
   "source": [
    "---\n",
    "### **(0) Schema version hash**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teDfhzCrMu5Y"
   },
   "source": [
    "The schema hash, denoted by `version_hash`, is generated from the actual schema content, excluding the hash values and version of the schema.\n",
    "\n",
    "Each time the schema is changed, a new hash is produced.\n",
    "\n",
    "> Note that during the initial run (the first pipeline run), the version will be 2, and there will be two previous hashes because the schema is updated during both the extract and normalize stages. You can rely on the version number to determine how many times the schema has been changed, but keep in mind that it stops being reliable when parallelization is introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qg6VhVLXM2e0"
   },
   "source": [
    "Each version hash is then stored in the `_dlt_version` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_7_Inspecting_%26_Adjusting_Schema_img2](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_7_Inspecting_%26_Adjusting_Schema_img2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOb9FHwtM60V"
   },
   "source": [
    "On subsequent runs, `dlt` checks if the generated schema hash is stored in this table. If it is not, `dlt` concludes that the schema has changed and migrates the destination accordingly.\n",
    "\n",
    "- If multiple pipelines are sending data to the same dataset and there is a clash in table names, a single table with the union of the columns will be created.\n",
    "- If columns clash and have different types or other incompatible characteristics, the load may fail if the data cannot be coerced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bke6cqBqPX8-"
   },
   "source": [
    "---\n",
    "### **(1) Naming convention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiWljhExPeKU"
   },
   "source": [
    "Each schema contains a naming convention that is denoted in the following way when the schema is exported:\n",
    "\n",
    "```yaml\n",
    "...\n",
    "normalizers:\n",
    "  names: snake_case # naming convention\n",
    "...\n",
    "```\n",
    "The naming convention is particularly useful if the identifiers of the data to be loaded (e.g., keys in JSON files) need to match the namespace of the destination (such as Redshift, which accepts case-insensitive alphanumeric identifiers with a maximum of 127 characters). This convention is used by `dlt` to translate between these identifiers and namespaces.\n",
    "\n",
    "The standard behavior of `dlt` is to use the same naming convention for all destinations, ensuring that users always see the same tables and columns in their databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJofnrYsPjgy"
   },
   "source": [
    "The default naming convention is `snake_case`:\n",
    "\n",
    "- Removes all ASCII characters except alphanumerics and underscores.\n",
    "- Adds an underscore (`_`) if the name starts with a number.\n",
    "- Multiple underscores (`_`) are reduced to a single underscore.\n",
    "- The parent-child relationship is expressed as a double underscore (`__`) in names.\n",
    "- The identifier is shortened if it exceeds the length allowed at the destination.\n",
    "\n",
    "> If you provide any schema elements that contain identifiers via decorators or arguments (e.g., `table_name` or `columns`), all the names used will be converted according to the naming convention when added to the schema. For example, if you execute `dlt.run(..., table_name=\"CamelCaseTableName\")`, the data will be loaded into `camel_case_table_name`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTDYzvOhPoif"
   },
   "source": [
    "To retain the original naming convention, you can define the following in your `config.toml`:\n",
    "\n",
    "```python\n",
    "[schema]\n",
    "naming=\"direct\"\n",
    "```\n",
    "\n",
    "or use an environment variable as:\n",
    "\n",
    "```\n",
    "SCHEMA__NAMING=direct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p2-UYrHN2uv"
   },
   "source": [
    "---\n",
    "### **(2) Schema settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF5_D5HfOOkq"
   },
   "source": [
    "\n",
    "The `settings` section of the schema file allows you to define various global rules that impact how tables and columns are inferred from data.\n",
    "\n",
    "```yaml\n",
    "settings:\n",
    "  detections:\n",
    "    ...\n",
    "  default_hints:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1caGqCiFOTlX"
   },
   "source": [
    "**1. Detections**\n",
    "\n",
    "You can define a set of functions that will be used to infer the data type of the column from a value. These functions are executed sequentially from top to bottom on the list.\n",
    "\n",
    "```yaml\n",
    "settings:\n",
    "  detections:\n",
    "    - timestamp # detects int and float values that can be interpreted as timestamps within a 5-year range and converts them\n",
    "    - iso_timestamp # detects ISO 8601 strings and converts them to timestamp\n",
    "    - iso_date #detects strings representing an ISO-like date (excluding timestamps) and, if so, converts to date\n",
    "    - large_integer # detects integers too large for 64-bit and classifies as \"wei\" or converts to text if extremely large\n",
    "    - hexbytes_to_text # detects HexBytes objects and converts them to text\n",
    "    - wei_to_double # detects Wei values and converts them to double for aggregate non-financial reporting\n",
    "```\n",
    "\n",
    ">  `iso_timestamp` detector is enabled by default.\n",
    "\n",
    "Detectors can be removed or added directly in code:\n",
    "\n",
    "```python\n",
    "  source = source()\n",
    "  source.schema.remove_type_detection(\"iso_timestamp\")\n",
    "  source.schema.add_type_detection(\"timestamp\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCof4rIDOo9J"
   },
   "source": [
    "**2. Column hint rules**\n",
    "\n",
    "The `default_hints` section in the schema file is used to define global rules that apply to newly inferred columns.\n",
    "\n",
    "> These rules are applied **after normalization**, meaning after the naming convention is applied!\n",
    "\n",
    "\n",
    "By default, schema adopts column hint rules from the json(relational) normalizer to support correct hinting of columns added by the normalizer:\n",
    "\n",
    "```yaml\n",
    "settings:\n",
    "  default_hints:\n",
    "    foreign_key:\n",
    "      - _dlt_parent_id\n",
    "    not_null:\n",
    "      - _dlt_id\n",
    "      - _dlt_root_id\n",
    "      - _dlt_parent_id\n",
    "      - _dlt_list_idx\n",
    "      - _dlt_load_id\n",
    "    unique:\n",
    "      - _dlt_id\n",
    "    root_key:\n",
    "      - _dlt_root_id\n",
    "```\n",
    "\n",
    "\n",
    "You can define column names with regular expressions as well.\n",
    "\n",
    "```yaml\n",
    "settings:\n",
    "  default_hints:\n",
    "  partition:\n",
    "        - re:_timestamp$ #  add partition hint to all columns ending with _timestamp\n",
    "```\n",
    "\n",
    "Column hints can be added directly in code:\n",
    "\n",
    "```python\n",
    "  source = data_source()\n",
    "  # this will update existing hints with the hints passed\n",
    "  source.schema.merge_hints({\"partition\": [\"re:_timestamp$\"]})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws6hDbA8PAHV"
   },
   "source": [
    "**3. Preferred data types**\n",
    "\n",
    "In the `preferred_types` section, you can define rules that will set the data type for newly created columns. On the left side, you specify a rule for a column name, and on the right side, you define the corresponding data type. You can use column names directly or with regular expressions to match them.\n",
    "\n",
    "```yaml\n",
    "settings:\n",
    "  preferred_types:\n",
    "    re:timestamp: timestamp\n",
    "    inserted_at: timestamp\n",
    "    created_at: timestamp\n",
    "    updated_at: timestamp\n",
    "```\n",
    "Above, we prefer `timestamp` data type for all columns containing timestamp substring and define a exact matches for certain columns.\n",
    "\n",
    "Preferred data types can be added directly in code as well:\n",
    "\n",
    "```python\n",
    "source = data_source()\n",
    "source.schema.update_preferred_types(\n",
    "  {\n",
    "    \"re:timestamp\": \"timestamp\",\n",
    "    \"inserted_at\": \"timestamp\",\n",
    "    \"created_at\": \"timestamp\",\n",
    "    \"updated_at\": \"timestamp\",\n",
    "  }\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_gSbvEZPyWV"
   },
   "source": [
    "---\n",
    "##  **How to modify a schema**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8w8yvOk4QU2d"
   },
   "source": [
    "Speaking of data types... you can directly apply data types and hints to your resources, bypassing the need for importing and adjusting schemas. This approach is ideal for rapid prototyping and handling data sources with dynamic schema requirements.\n",
    "\n",
    "The two main approaches are:\n",
    "\n",
    "- Using the `columns` argument in the `dlt.resource` decorator.\n",
    "- Using the `apply_hints` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8BIxFswQddu"
   },
   "source": [
    "---\n",
    "### **`(0) @dlt.resource(columns=...)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_5RHsM4QnPi"
   },
   "source": [
    "This code snippet sets up a nullable boolean column named `my_column` directly in the decorator.\n",
    "\n",
    "```python\n",
    "@dlt.resource(name='my_table', columns={\"my_column\": {\"data_type\": \"bool\", \"nullable\": True}})\n",
    "def my_resource():\n",
    "    for i in range(10):\n",
    "        yield {'my_column': i % 2 == 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHOU5IxuQtlZ"
   },
   "source": [
    "---\n",
    "### **(1) `apply_hints`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugTbyJRqQwt_"
   },
   "source": [
    "When dealing with dynamically generated resources or needing to programmatically set hints, `apply_hints` is your go-to tool.\n",
    "\n",
    "The `apply_hints` method in dlt is used to programmatically **set** or **adjust** various aspects of your data resources or pipeline. It can be used in several ways:\n",
    "\n",
    "* You can use `apply_hints` to **directly define data types** and their properties, such as nullability, within the `@dlt.resource` decorator. This eliminates the dependency on external schema files.\n",
    "\n",
    "* When **dealing with dynamically generated resources** or needing to programmatically set hints, `apply_hints` is your tool. It's especially useful for applying hints across various collections or tables at once.\n",
    "\n",
    "* `apply_hints` can be used to **load your data incrementally**. For example, you can load only files that have been updated since the last time dlt processed them, or load only the new or updated records by looking at a specific column.\n",
    "\n",
    "* You can **set or update the table name, columns, and other schema elements** when your resource is executed, and you already yield data. Such changes will be merged with the existing schema in the same way the `apply_hints` method works.\n",
    "\n",
    "\n",
    "It’s especially useful for applying hints across multiple collections or tables at once.\n",
    "\n",
    "For example, to apply a complex data type across all collections from a MongoDB source:\n",
    "\n",
    "```python\n",
    "all_collections = [\"collection1\", \"collection2\", \"collection3\"]  # replace with your actual collection names\n",
    "source_data = mongodb().with_resources(*all_collections)\n",
    "\n",
    "for col in all_collections:\n",
    "    source_data.resources[col].apply_hints(columns={\"column_name\": {\"data_type\": \"complex\"}})\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"mongodb_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"mongodb_data\"\n",
    ")\n",
    "load_info = pipeline.run(source_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex6tDo6OQ0G4"
   },
   "source": [
    "---\n",
    "### **(2) Adjusting schema settings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQjYjFhcQ8xd"
   },
   "source": [
    "> Maybe you've noticed, but there several ways to adjust your schema settings directly in code were already covered. This is just a recap. You can go back directly to the Schema Settings section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbtMMCjPRNLg"
   },
   "source": [
    "Detectors can be removed or added directly in code:\n",
    "\n",
    "```python\n",
    "  source = source()\n",
    "  source.schema.remove_type_detection(\"iso_timestamp\")\n",
    "  source.schema.add_type_detection(\"timestamp\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V02Uw2j0RboZ"
   },
   "source": [
    "Column hints can be added directly in code:\n",
    "\n",
    "```python\n",
    "  source = data_source()\n",
    "  # this will update existing hints with the hints passed\n",
    "  source.schema.merge_hints({\"partition\": [\"re:_timestamp$\"]})\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFtak7pkRgWO"
   },
   "source": [
    "Preferred data types can be added directly in code as well:\n",
    "\n",
    "```python\n",
    "source = data_source()\n",
    "source.schema.update_preferred_types(\n",
    "  {\n",
    "    \"re:timestamp\": \"timestamp\",\n",
    "    \"inserted_at\": \"timestamp\",\n",
    "    \"created_at\": \"timestamp\",\n",
    "    \"updated_at\": \"timestamp\",\n",
    "  }\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRxGxNsWRyuo"
   },
   "source": [
    "---\n",
    "### **(3) Importing a schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOhGYWlTR2J1"
   },
   "source": [
    "> We mentioned that you can export a schema. In a similar fashion you can import a schema. The usual approach to use this functionaility is to export the schema first, make the adjustments and put the adjusted schema into the corresponding import folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5Fto-5tSRZB"
   },
   "source": [
    "The instruction to import a schema should be provided at the beginning when creating a pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkeaKlPuR1yS"
   },
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"github_pipeline3\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"github_data\",\n",
    "    export_schema_path=\"schemas/export\",\n",
    "    import_schema_path=\"schemas/import\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVoVmwJqIbmK"
   },
   "source": [
    "Let's make an initial pipeline run to export schema into the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLz6aDFrIhwX"
   },
   "outputs": [],
   "source": [
    "# run the pipeline with the new resource\n",
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hls7K7vxTfcm"
   },
   "source": [
    "Look at the \"Files\" in the left sidebar, see the `schema` folder, and `export` and `import` folders inside.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lesson_7_Inspecting_%26_Adjusting_Schema_img3](https://storage.googleapis.com/dlt-blog-images/dlt-fundamentals-course/Lesson_7_Inspecting_%26_Adjusting_Schema_img3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szn8vHPVI75E"
   },
   "source": [
    "Now, both folders contain identic schema files.\n",
    "\n",
    "### **Exercise 1: Adjust import schema**\n",
    "\n",
    "**Adjust the import schema** by adding a description of the **`github_pulls`** table.\n",
    "\n",
    "\n",
    "```\n",
    "github_pulls:\n",
    "  columns:\n",
    "    updated_at:\n",
    "      incremental: true\n",
    "  write_disposition: append\n",
    "  resource: github_pulls\n",
    "  description: Table contains all pull requests information from dlt repository\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kelVxsPAWMdl"
   },
   "source": [
    "Run the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OHyOuiNV_oi"
   },
   "outputs": [],
   "source": [
    "load_info = pipeline.run(github_source())\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYF9RquDWXAD"
   },
   "source": [
    "Check the exported schema file. It should now contain a description for the `github_pulls` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjL_Pk-yLFHC"
   },
   "outputs": [],
   "source": [
    "!cat schemas/export/github_source.schema.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jil79p73LU2s"
   },
   "source": [
    "### Question\n",
    "\n",
    "What **data type** does the column `version` in the `_dlt_version` table have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYbccmLie1zm"
   },
   "source": [
    "✅ ▶ Proceed to the [next lesson](https://colab.research.google.com/drive/1jp5UtydA3x9cAq-fbW2tRmAOl4LMZqM1#forceEdit=true&sandboxMode=true)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gxU44wP9GvG6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
