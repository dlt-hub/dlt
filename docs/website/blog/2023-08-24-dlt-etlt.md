---
slug: dlt-etlt
title: "The return of ETL in the Python age"
image: /img/went-full-etltl.png
authors:
  name: Adrian Brudaru
  title: Open source data engineer
  url: https://github.com/adrianbr
  image_url: https://avatars.githubusercontent.com/u/5762770?v=4
tags: [ETL, ELT, EtlT, ]
---
:::info
PSSSST! You do ELT, right? not ETL? asking for a friend...
:::

## ETL vs ELT? A vendor driven story.



One of the earliest tooling for "ETL" data was Pentaho Kettle.
Kettle stands for "Kettle Extraction Transformation Transport Load Environment" and signifies that it transforms the data before loading it.
It was usually used to load data which was later transformed in sql via "sql scripts", while still in the tool, or via database triggers or views outside of the tool.

Indeed, the tool creators imagined some folks would write java to transform before loading, but the vast majority of data users just wanted to use sql.

Sounds familiar? This is not so different to today's "ELT", is it?

##  Why did we call it ELT?

### The people

Well, first of all SQL is much more accessible and very powerful for transforming tables,
columns and rows - where programming handles single values.
So before purpose built tooling existed, data people were already doing the transform in SQL - it just made sense.

### The "EL" vendors
In the decade following Pentaho, Saas solutions started offering pipelines that load data into your database, removing the option for you to tinker with it before loading.
For this reason, they would call it "ELT".

### The db vendors
The concept also resonated with MPP DBs (massive parallel processing), such as Snowflake, Redshift, Bigquery, which were more than happy to encourage doing all the compute on their side.

### The "T in ELT" vendors

Another puzzle piece was dbt, a tool purpose built for sql transform. So if there's a question of
ETL or ETL, dbt can only answer ELT. In dbt's word view, data starts dirty in your warehouse, where you "rename, cast, join, enrich" - a true ELT.
To make the drudgery of data cleaning in sql easier, dbt offers some python support to enable generating some of the typing and renaming sql.
They also offer a litte bit of python support for scalar operations in some db vendor systems.

##  What do we really do?

Most of us do a little bit of both - we extract with python, and the next steps are loading,
cleaning and curation. In some cases, cleaning and curation are optional. For example,
when we load a report from another platform we will probably not need to clean or curate anything.

### Where do we clean data?

Data cleaning usually refers to normalising the data into correct types, usable names, etc.
Doing this in SQL results in writing a lot of manual code that needs to be maintained.
On the other hand, sturcturing data in python isn't easy either,
it's just less technically difficult, but when metadata is missing, it becomes guesswork.

So, technically the easier place to clean data is in python, but likely the majority will do it in SQL as they are more practiced in sql.

### Where do we transform data?
When it comes to working with tables, sql is still the better place to be.
Joins and aggregations are the core operations that will happen here and they would be much harder to handle scalably in python.


# dlt puts the small t back in EtlT

So, python is still superior at a few operations
- Typing, renaming, normalising, unpacking
- complex scalar operations

While we will leave the aggregations and joins to the big T, SQL.

### Normalisation, typing, unpacking

dlt does this well out of the box. Automatic typing, renaming,
flattening, and ddl deployment are all handled by the schema inference and evolution engine.
This engine is configurable in both how it works and what it does,
you can read more here: [Normaliser, schema settings](https://dlthub.com/docs/general-usage/schema#data-normalizer)

### Scalar operations

Sometimes we need to edit a column's value in some very specific way for which SQL doesn't quite cut it.
Sometimes, we have data we need to pseudonymise before loading for regulatory reasons.

Because dlt is a libary, it means you can easily change how the data stream is produced or ingested.
Besides your own customisations, dlt also supports injecting your transform code inside the event stream,
[see an example here](https://dlthub.com/docs/general-usage/customising-pipelines/renaming_columns#renaming-columns-by-replacing-the-special-characters)

### The big T

Finally, once you have clean data loaded, you will probably prefer to use sql and one of the standard tools.
dlt offers a dbt runner to get you started easily with your transformation package.

##  In conclusion

ETL vs ELT was never really a debate.
With some exceptions almost everyone transforms the data in SQL -
but what they call this process depends on who's telling the story.

While it's easier to do most of the transformation in SQL, the tedious is completely automatable in python,
and the dirty data doesn't need manual normalisation. With dlt, you can do ETL or ELT, or even better, both, as EtLT


Or, if you're feeling funny, you can add [duckdb](https://dlthub.com/docs/dlt-ecosystem/destinations/duckdb) in the middle and go full EtLTLT
where you have an additional T step in the middle for the kinds of operations that could be done locally.




## Start using dlt today
What are you waiting for?
* Dive into our [getting started docs](https://dlthub.com/docs/getting-started)
* [Join the ⭐Slack Community⭐ for discussion and help!](https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g)