---
title: "Release highlights: 1.15"
description:  Release highlights provide a concise overview of the most important new features, improvements, and fixes in a software update, helping users quickly understand what's changed and how it impacts their workflow.
keywords: [dlt, data-pipelines, etl, release-notes, data-engineering]
---

# Release highlights: 1.15


## ⚠️ Breaking changes: `.gz` extensions now added to compressed files

**PR: #2835**

Starting with v1.15, all compressed files will include the `.gz` extension by default. This applies to:

- Filesystem destinations
- Internal working directories
- Staging locations (feeding downstream destinations like Redshift, BigQuery)

**Notes**

- Does **not apply** to `.parquet` files
- Existing filesystem destination will keep the old behavior (no `.gz`) for backward compatibility
- Compressed files uploaded to staging destinations will now have the `.gz` extension, also if `dlt` is configured to keep data in stage.

[More details → File Compression in Filesystem Destination](../dlt-ecosystem/destinations/filesystem#file-compression)

---

## Export schemas in DBML format

**PR: #2929**

You can now export your pipeline schema in **DBML format**, ready for visualization in DBML frontends.

```py
# Generate a string that can be rendered in a DBML frontend
dbml_str = pipeline.default_schema.to_dbml()
```

This includes:

- Data and dlt tables
- Table/column metadata
- User-defined/root-child/parent-child references
- Grouping by resources
- etc.

![diagram](https://storage.googleapis.com/dlt-blog-images/release-highlights/470492597-cc6882ec-d08a-46a0-b268-cda8a6c6bf70.png)
---

## REST API Client Enhancements

### 1. `has_more` flag in paginators

**PR: #2817**

Some APIs provide a `has_more` boolean instead of `total` or `offset` limits.

You can now configure paginators to use that flag.

```py
from dlt.sources.helpers.rest_client.paginators import OffsetPaginator, PageNumberPaginator

# Paginator starts at offset=0, increments by `limit`
# Stops automatically when response["has_more"] == False
paginator = OffsetPaginator(
    offset=0,
    limit=2,
    has_more_path="has_more"   # JSONPath to the boolean flag
)

# Paginator starts at page=1, increments by page
# Stops automatically when response["has_more"] == False
paginator = PageNumberPaginator(
    page=1,
    has_more_path="has_more"  # JSONPath to the boolean in response
)
```

### 2. JSON body pagination support

**PR: #2917**

Pagination parameters can now be placed in the **request body** (e.g., for `POST` requests).

```py
from dlt.sources.helpers.rest_client.paginators import OffsetPaginator, PageNumberPaginator

# We tell dlt to place pagination values INSIDE the request JSON body
paginator = OffsetPaginator(
    offset=0,
    limit=3,
    offset_body_path="offset",     # put 'offset' in request.json["offset"]
    limit_body_path="limit",       # put 'limit'  in request.json["limit"]
)

paginator = PageNumberPaginator(
    page=1,
    page_body_path="page",           # put 'page'      in request.json["page"]
)
```

---

## Databricks + Unity Catalog enhancement

**PR: #2674**

New support for **comments, tags, and constraints** in Databricks Unity Catalog tables.

- Add **table comments** and **tags**
- Add **column comments** and **tags**
- Apply **primary/foreign key constraints**

```toml
[destination.databricks]
# Add PRIMARY KEY and FOREIGN KEY constraints to tables
create_indexes = true
```

Use the adapter for column- and table-level hints:

```py
from dlt.destinations.adapters import databricks_adapter

databricks_adapter(
    my_resource,
    table_comment="Event data",
    table_tags=["pii", {"cost_center": "12345"}],
    column_hints={
        "user_id": {
            "column_comment": "User ID",
            "column_tags": ["pii"]
        }
    },
)
```

---

## SQLAlchemy destination: MSSQL + Trino support

**PR: #2951**

- **MSSQL** fully supported (including JSON fields)
- **Trino** partially supported (with limitations: no merge/SCD2, JSON casted to string)
- Fixes for ClickHouse temp tables and BigQuery numeric handling

[Example of customizing a type mapper for Trino →](../dlt-ecosystem/destinations/sqlalchemy#adapting-destination-for-a-dialect)

---

## Delta Lake: control over streamed execution

**PR: #2961**

By default, dlt runs **Delta Lake upserts in *streamed mode***.

- Streamed execution = the source table is read **row by row**, which **reduces memory pressure** and works well for large datasets.
- But in streamed mode, **table statistics and pruning predicates are not used** — meaning Delta can’t skip irrelevant partitions efficiently.

**New option: `deltalake_streamed_exec`**

Now you can explicitly choose whether to run Delta merge operations in streamed mode or not.

- **`true` (default):** safe for very large datasets, lower memory usage.
- **`false`:** enables Delta to use table statistics and pruning (better performance when partitions and stats exist).

```toml
[destination.filesystem]
deltalake_streamed_exec = false
```

---

## **Python 3.14 support**

**PR: #2789**

Python `3.14` is currently in `beta4`. We now provide experimental support for Python `3.14` to make it easier for projects depending on dlt to prepare for the upgrade.

---

## Switched ConnectorX backend from `arrow2` to `arrow`

**PR: #2933**

ConnectorX dropped the `arrow2` backend in v0.4.2, which was dlt’s default.

dlt now defaults to the `arrow` backend, with minor adjustments for behavioral differences, and relaxed ConnectorX version constraints.

---

## **AI Command IDE integration**: now works with all major IDEs (Cursor, Copilot, Windsurf, etc.)

**PR: #2937**

[The AI command](../reference/command-line-interface#dlt-ai) now detects and supports all major IDEs, automatically copying the correct rule files for each environment. This ensures seamless integration whether you’re using Cursor, Copilot, Windsurf, Claude, or other supported editors.

---

## **Callback collector**: add runtime hooks for progress and schema updates

**PR: #2922**

The new **`CallbackCollector`** (built on `LogCollector`) lets you attach callbacks to pipeline lifecycle events and monitor counters in real time.

**Example**

```py
import dlt
from dlt.common.runtime.collector import LogCollector

class MyCollector(LogCollector):
    def on_start_trace(self, *args, **kwargs):
        print("Pipeline started")

    def on_end_trace(self, *args, **kwargs):
        print("Pipeline finished")

    def on_log(self):
        print("Counters:", dict(self.counters))

collector = MyCollector()

pipeline = dlt.pipeline("test_pipeline", destination="duckdb", progress=collector)
pipeline.extract([{"id": 1}, {"id": 2}], table_name="users")
```

**Output:**

```text
Pipeline started
Counters: {'Resources': 0}
Counters: {'Resources': 0, 'users': 1}
Counters: {'Resources': 1, 'users': 2}
Pipeline finished
```

This makes it easy to **integrate with orchestrators, dashboards, or monitoring tools** by reacting to lifecycle events and progress updates.

---

## **Shout-out to new contributors**

Big thanks to our newest contributors:

- [@bayees](https://github.com/bayees) — #2674
- [@7amza79](https://github.com/7amza79) — #2783
- [@michaelconan](https://github.com/michaelconan) — #2817
- [@Giackgamba](https://github.com/Giackgamba) — #2917

---

**Full release notes**

[View the complete list of changes →](https://github.com/dlt-hub/dlt/releases)