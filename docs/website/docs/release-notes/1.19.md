---
title: "Release highlights: 1.19"
description: Release highlights provide a concise overview of the most important new features, improvements, and fixes in a software update, helping users quickly understand what's changed and how it impacts their workflow.
keywords: [dlt, data-pipelines, etl, snowflake, cluster, release-notes, data-engineering]
---
# Release highlights: 1.19

## New: Arrow streaming with ConnectorX

You can now return results from `sql_database` queries as an **Arrow stream** when using the ConnectorX backend.

This allows large query results to be processed incrementally, avoiding loading the full result set into memory.

Example:

```py
from dlt.sources.sql_database import sql_database

db = sql_database(
    backend="connectorx",
    backend_kwargs={
        "return_type": "arrow_stream",  # new in 1.19
    },
)
```

By default, ConnectorX returns PyArrow tables. Arrow streaming must be explicitly enabled.

[Read more →](../dlt-ecosystem/verified-sources/sql_database/configuration#connectorx)

---

## Visual pipeline run history in the dashboard

The dashboard now includes a **visual history of pipeline runs**, making it easier to inspect run status, duration, and changes over time.

This provides a clearer overview of pipeline health and helps diagnose failures faster.

![run_history](https://storage.googleapis.com/dlt-blog-images/release-highlights/run_history.png)

[Read more ->](../general-usage/dashboard)

---

## Faster Parquet ingestion into MSSQL, MySQL, and SQLite via ADBC

dlt can now ingest **Parquet files into SQL databases** (MSSQL, MySQL, and SQLite) using **ADBC**.

When an ADBC driver is available, Parquet loading is enabled automatically and becomes the preferred method. This delivers a **10×–100× speedup** compared to `INSERT`-based loading and is more reliable than CSV fallbacks.

If needed, you can explicitly revert to `INSERT` loading:

```py
pipeline.run(
    data_iter,
    loader_file_format="insert_values",
)
```

[Read more →](../dlt-ecosystem/destinations/mssql#fast-loading-with-parquet)

---

## Visualize schemas with `Schema.to_mermaid()`

You can now export any dlt schema as a **Mermaid diagram** for quick visualization in documentation, pull requests, or onboarding materials.

```py
schema_mermaid = pipeline.default_schema.to_mermaid()
```

![mermaid](https://storage.googleapis.com/dlt-blog-images/release-highlights/mermaid.jpg)

Schemas can also be exported from the CLI and rendered natively in tools like GitHub Markdown and Notion.

[Read more →](../general-usage/dataset-access/view-dlt-schema#export-to-mermaid)

---

## Snowflake clustering key improvements

Snowflake destinations now support **updating clustering keys** using column hints.

Clustering changes are applied when a table alteration is triggered (for example, when a new column is added), making it easier to tune clustering for large tables without recreating them.

Example:

```py
@dlt.resource(table_name="events")
def events():
    yield {"event_id": 1, "country": "DE"}

events.apply_hints(columns=[{"name": "event_id", "cluster": True}])
pipeline.run(events())
```

[Read more →](../dlt-ecosystem/destinations/snowflake#supported-column-hints)

---

## Shout-out to new contributors

Big thanks to our newest contributors:

* [@zjacom](https://github.com/zjacom) — #3294
* [@JayJai04](https://github.com/JayJai04) — #3300
* [@anair123](https://github.com/anair123) — #3304
* [@martinibach](https://github.com/martinibach) — #3309
* [@hello-world-bfree](https://github.com/hello-world-bfree) — #3318
* [@tahamuzammil100](https://github.com/tahamuzammil100) — #3314
* [@timH6502](https://github.com/timH6502) — #3307
* [@wrussell1999](https://github.com/wrussell1999) — #3331
* [@luqmansen](https://github.com/luqmansen) — #3332

---

**Full release notes**

[View the complete list of changes →](https://github.com/dlt-hub/dlt/releases)