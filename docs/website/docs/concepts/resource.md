---
title: Resource
description: Explanation of what a dlt resource is
keywords: [resource, api endpoint, dlt.resource]
---

# Resource

## Declare a resource

A [resource](../glossary.md#resource) is a function that yields data. To create a resource, we add the `@dlt.resource` decorator to that function

Commonly used arguments:
- `name` The name of the table generated by this resource. Defaults to decorated function name.
- `write_disposition` How should the data be loaded at destination? Currently supported: `append`, `replace` and `merge`. Defaults to `append.`

Example:

```python
@dlt.resource(name='table_name', write_disposition='replace')
def generate_rows():
	for i in range(10):
		yield {'id':i, 'example_string':'abc'}
```

To get the data of a resource, we could do

```python

for row in generate_rows():
		print row

for row in sql_source().resources.get('table_users')
		print(row)

```

Typically, resources are declared and grouped with related resources within a [source](source.md) function.

### Define schema
`dlt` will generate [schema](schema.md) for tables associated with resources from the resource's. You can modify the generation process by using the table and column hints. Resource decorator accepts following arguments:
1. `table_name` the name of the table if different from resource name
2. `primary_key` and `merge_key` define name of the columns (compound keys are allowed) that will receive those hints. Used in [incremental loading](../customization/incremental-loading.md)
3. `columns` let's you define one or more columns, including the data types, nullability and other hints. The column definition is a `TypedDict`: `TTableSchemaColumns`. In example below, we tell `dlt` that column `tags` (containing a list of tags) in `user` table should have type `complex` which means that it will be loaded as JSON/struct and not as child table.

```python
@dlt.resource(name="user", columns={"tags": {"data_type": "complex"}})
def get_users():
  ...
# the `table_schema` method gets table schema generated by a resource
print(get_users().table_schema())
```
Note that you can pass dynamic hints which are functions that take the data item as input and return a hint value. This let's you create table and column schemas depending on the data. See example in next section.

### Dispatch data to many tables

You can load data to many tables from a single resource. The most common case is a stream of events of different types, each with different data schema. To deal with this, you can use `table_name` argument on `dlt.resource`. You could pass the table name as a function with the data item as an argument and the `table_name` string as a return value.

For example, a resource that loads GitHub repository events wants to send 'issue', 'pull request', and 'comment` events to separate tables. The type of the event is in the "type" field.

```python
# send item to a table with name item["type"]
@dlt.resource(table_name=lambda event: event['type'])
def repo_events() -> Iterator[TDataItems]:
    yield item

# the `table_schema` method gets table schema generated by a resource and takes optional
# data item to evaluate dynamic hints
print(repo_events().table_schema({"type": "WatchEvent", id=...}))
```

In more advanced cases, you can dispatch data to different tables directly in the code of the resource function:
```python
@dlt.resource
def repo_events() -> Iterator[TDataItems]:
    # mark the "item" to be sent to table with name item["type"]
    yield dlt.mark.with_table_name(item, item["type"])
```

### Parametrize a resource

You can add arguments to your resource functions like to any other. Below we parametrize our `generate_row` resource to generate the number of rows we request

```python
@dlt.resource(name='table_name', write_disposition='replace')
def generate_rows(nr):
	for i in range(nr):
		yield {'id':i, 'example_string':'abc'}

for row in generate_rows(10):
		print row

for row in generate_rows(20):
		print row
```

You can mark some of resource arguments as configuration and [credentials](../customization/credentials.md) values so `dlt` can pass them automatically to your functions.

### Feeding data from one resource into another

You can feed data from a resource into another one. The most common case is when you have an API that returns a list of objects (ie. users) in one endpoint and user details in another. You can deal with this by declaring a resource that obtains a list of users and another resource that receives items from the list and downloads the profiles.

```python
@dlt.resource(write_disposition="replace")
def users(limit=None):
    for u in _get_users(limit):
        yield u

# feed data from users as user_item below, all transformers must have at least one argument that will receive data from the parent resource
@transformer(data_from=users)
def users_details(user_item):
  for detail in _get_details(user_item["user_id"]):
    yield detail

# just load the user_details. dlt figures out dependencies for you.
pipeline.run(user_details)

# you can be more explicit and use a pipe operator. with it you can create dynamic pipelines where the dependencies are set at run time and resources are parametrized ie. below we want to load only 100 users from `users` endpoint
pipeline.run(users(limit=100) | user_details)
```

## Customize resources
### Filter, transform and pivot data
You can attach any number of transformations that are evaluated on item per item basis to your resource. The available transformation types:
* map - transform the data item (`resource.add_map`)
* filter - filter the data item (`resource.add_filter`)
* yield map - a map that returns iterator (so single row may generate many rows - `resource.add_yield_map`)

Example:
We have a resource that loads a list of users from an api endpoint. We want to customize it so:
1. we remove users with `user_id` == "me"
2. we anonymize user data

Here's our resource:

```python
import dlt

@dlt.resource(write_disposition="replace")
def users():
    ...
    users = requests.get(...)
    ...
    yield users

```
Here's our script that defines transformations and loads the data.

```python
from pipedrive import users

def anonymize_user(user_data):
    user_data["user_id"] = hash_str(user_data["user_id"])
    user_data["user_email"] = hash_str(user_data["user_email"])
    return user_data

# add the filter and anonymize function to users resource and enumerate
for user in users().add_filter(lambda user: user["user_id"] != "me").add_map(anonymize_user):
  print(user)

```

### Modify schema
You can change the schema of a resource, be it standalone or as a part of a source. Look for method named `apply_hints` which takes the same arguments as resource decorator. Obviously you should call this method before data is extracted from the resource.

## Load resources

You can pass individual resources or list of resources to the `dlt.pipeline` object. The resources loaded outside the source context, will be added to the [default schema](schema.md) of the pipeline.

Example using the `generate_rows` resource above:
```python

pipeline = dlt.pipeline(pipeline_name="rows_pipeline", destination="duckdb", dataset_name="rows_data")
# load individual resource
pipeline.run(generate_rows(10))
# load a list of resources
pipeline.run([generate_rows(10), generate_rows(20)])
```

### Do a full refresh
To do a full refresh of an `append` or `merge` resources you temporarily change the write disposition to replace. You can use `apply_hints` method of a resource or just provide alternative write disposition when loading:
```python
p.run(merge_source(), write_disposition="replace")
```

