---
title: Getting Started
description: quick start with dlt
keywords: [getting started, quick start, basics]
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';


# Getting Started

## Overview

`dlt` is an open-source library that you can add to your Python scripts to load data
from various and often messy data sources into well-structured, live datasets.
Below we give you a preview how you can get data from APIs, files, Python objects or
pandas dataframes and move it into a local or remote database, data lake or a vector data store.

Let's get started!

## Installation

Install dlt using pip:

```bash
pip install dlt
```

Command above just installs library core, in example below we use `duckdb` as a destination so let's add it:

```bash
pip install "dlt[duckdb]"
```

:::tip
Use clean virtual environment for your experiments! Here are [detailed instructions](../reference/installation).
:::

## Quick start

Let's load a list of Python objects (dictionaries) into `duckdb` and inspect the created dataset:

```python
import dlt

data = [
    {'id': 1, 'name': 'Alice'},
    {'id': 2, 'name': 'Bob'}
]

pipeline = dlt.pipeline(
    pipeline_name='quick_start',
    destination='duckdb',
    dataset_name='mydata'
)
load_info = pipeline.run(data, table_name="users")
print(load_info)
```

Save this python script with the name `quick_start_pipeline.py` and run the following command:

```bash
python3 quick_start_pipeline.py
```

The output should look like:

```bash
Pipeline quick_start completed in 0.59 seconds
1 load package(s) were loaded to destination duckdb and into dataset mydata_20230818012044
The duckdb destination used duckdb:////home/user-name/quick_start/quick_start.duckdb location to store data
Load package 1692364844.460054 is LOADED and contains no failed jobs
```

Now explore your data!

You may need to install `pandas` and `streamlit`.

To see the schema of your created database, run Streamlit command:

```bash
dlt pipeline <pipeline_name> show
```

For example above pipeline name is “quick_start”, so run:

```bash
dlt pipeline quick_start show
```

[This command](../reference/command-line-interface#show-tables-and-data-in-the-destination) generates
and launches a simple Streamlit app that you can use to inspect the schemas and data in the
destination.

![Streamlit Explore data](img/streamlit1.png)
Streamlit Explore data. Schema and data for a test pipeline “quick_start”.

:::tip
`dlt` works in Jupyter Notebook! Do the above in [Colab Demo.](https://dlthub.com/docs/getting-started/try-in-colab)
:::

Learn more:
- What is a data pipeline?
  [General usage: Pipeline.](../general-usage/pipeline)
- [Walkthrough: Create a pipeline](../walkthroughs/create-a-pipeline).
- [Walkthrough: Run a pipeline.](../walkthroughs/run-a-pipeline)
- How to configure DuckDB?
  [Destinations: DuckDB.](../dlt-ecosystem/destinations/duckdb)
- [The full list of available destinations.](../dlt-ecosystem/destinations/)
- [Exploring the data](../dlt-ecosystem/visualizations/exploring-the-data).
- What happens after loading?
  [Understanding the tables](../dlt-ecosystem/visualizations/understanding-the-tables).
- Access your data with [SQL](https://dlthub.com/docs/dlt-ecosystem/transformations/sql) or [Pandas](https://dlthub.com/docs/dlt-ecosystem/transformations/sql)

## Load your data

### Load data from a variety of sources

Use dlt to load practically any data you deal with in your Python scripts into a dataset.
The library will create/update tables, infer data types and deal with nested data automatically:

<Tabs
  groupId="source-type"
  defaultValue="json"
  values={[
    {"label": "from json", "value": "json"},
    {"label": "from CSV", "value": "csv"},
    {"label": "from API", "value": "api"},
    {"label": "from Database", "value":"database"}
]}>
  <TabItem value="json">

```python
import json
import dlt

with open("test.json", 'r') as file:
    data = json.load(file)

pipeline = dlt.pipeline(
	pipeline_name='from_json',
	destination='duckdb',
	dataset_name='mydata',
)
# dlt works with lists of dicts, so wrap data to the list
load_info = pipeline.run([data], table_name="json_data")
print(load_info)
```

  </TabItem>
  <TabItem value="csv">

```python
import dlt
import pandas as pd

df = pd.read_csv("test.csv")
data = df.to_dict(orient='records')

pipeline = dlt.pipeline(
	pipeline_name='from_csv',
	destination='duckdb',
	dataset_name='mydata',
)
load_info = pipeline.run(data, table_name="csv_data")
print(load_info)
```

  </TabItem>
  <TabItem value="api">

```python
import dlt
import requests

# url to request dlt-hub followers
url = f"https://api.github.com/users/dlt-hub/followers"
# make the request and return the json
data = requests.get(url).json()

pipeline = dlt.pipeline(
	pipeline_name='from_api',
	destination='duckdb',
	dataset_name='mydata',
)
# dlt works with lists of dicts, so wrap data to the list
load_info = pipeline.run([data], table_name="followers")
print(load_info)
```

  </TabItem>
  <TabItem value="database">

:::tip

Use our verified [sql database source](https://dlthub.com/docs/dlt-ecosystem/verified-sources/sql_database)
to sync your databases with warehouses, data lakes, or vector stores.

:::

```python
import dlt
import duckdb

db_name = "path_to_your_db"  # Replace this with the path to your DuckDB database file
sql_query = "SELECT * FROM your_table"  # Replace with your SQL query

# connect to the DuckDB
con = duckdb.connect(db_name)
# execute SQL query and fetch result
result = con.execute(query).fetch_df()
# close the connection
con.close()
data = result.to_dict(orient="records")

pipeline = dlt.pipeline(
    pipeline_name='from_database',
    destination='duckdb',
    dataset_name='mydata',
)
load_info = pipeline.run(data)
print(load_info)
```

  </TabItem>
</Tabs>

### Append or replace your data

Run any of the previous examples twice to notice that each time a copy of the data is added to your tables.
We call this load mode `append`. It is very useful when i.e. you have a new folder created daily with `json` file logs, and you want to ingest them.

Perhaps this is not what you want to do in the examples above.
For example, if the CSV file is updated, how we can refresh it in the database?
One method is to tell `dlt` to replace the data in existing tables by using `write_disposition`:

```python
import dlt

data = [
    {'id': 1, 'name': 'Alice'},
    {'id': 2, 'name': 'Bob'}
]

pipeline = dlt.pipeline(
    pipeline_name='replace_data',
    destination='duckdb',
    dataset_name='mydata',
)
load_info = pipeline.run(data, table_name="users", write_disposition="replace")
print(load_info)
```

Run this script twice to see that `users` table still contains only one copy of your data.

:::tip
What if you added a new column to your CSV?
`dlt` will migrate your tables!
See the `replace` mode and table migration in action in our [Colab Demo](https://colab.research.google.com/drive/1H6HKFi-U1V4p0afVucw_Jzv1oiFbH2bu#scrollTo=e4y4sQ78P_OM)
:::

Learn more:

- [Full load - how to replace your data](https://dlthub.com/docs/general-usage/full-loading)
- [Append, replace and merge your tables](https://dlthub.com/docs/general-usage/incremental-loading).


## Declare loading behavior

You can finetune the loading process by decorating Python functions with `@dlt.resource`.

### Load only new data (incremental loading)

We can supercharge the GitHub API example above and get only issues that were created since last load.
Instead of using `replace` write_disposition and downloading all issues each time the pipeline is run, we do the following:

```python
import dlt
import requests


@dlt.resource(table_name="issues", write_disposition="append")
def get_issues(
    created_at = dlt.sources.incremental("created_at", initial_value="1970-01-01T00:00:00Z")
):
    # url to request dlt-hub issues
    url = f"https://api.github.com/repos/dlt-hub/dlt/issues"

    while True:
        response = requests.get(url)
        page_items = response.json()

        if len(page_items) == 0:
            break
        yield page_items

        if "next" not in response.links:
            break
        url = response.links["next"]["url"]

        # stop requesting pages if the last element was already older than initial value
        # note: incremental will skip those items anyway, we just do not want to use the api limits
        if created_at.start_out_of_range:
            break


pipeline = dlt.pipeline(
	pipeline_name='github_issues',
	destination='duckdb',
	dataset_name='mydata',
)
# dlt works with lists of dicts, so wrap data to the list
load_info = pipeline.run(get_issues)
print(load_info)
```

We request issues for dlt-hub/dlt repository ordered by **created_at** descending and yield them page by page in `get_issues` generator function.

We use the `@dlt.resource` decorator to declare table name to which data will be loaded and write disposition, which is `append`.

We also use `dlt.sources.incremental` to track `created_at` field present in each issue to filter only the newly created ones.

Now run the script. It loads all the issues from our repo to `duckdb`. Run it again, and you can see that no issues got added (if no issues were created in the meantime).

Now you can run this script on a daily schedule and each day you’ll load only issues created after the time of the previous pipeline run.

:::tip
Between pipeline runs, `dlt` keeps the state in the same database it loaded data.
Peek into that state, the tables loaded and get other information with:

```shell
dlt pipeline -v github_issues info
```
:::

Learn more:

- Declare your [resources](../general-usage/resource) and group them in [sources](../general-usage/source) using Python decorators.
- [Set up "last value" incremental loading.](../general-usage/incremental-loading#incremental-loading-with-last-value)
- [Inspect pipeline after loading.](../walkthroughs/run-a-pipeline#4-inspect-a-load-process)
- [`dlt` command line interface.](../reference/command-line-interface)

### Update and deduplicate your data

The script above finds new issues and adds them to the database.
It will ignore any updates to issue text, emoji reactions etc.
Get always fresh content of all the issues: combine incremental load with `merge` write disposition,
like in the script below.

```python
import dlt
import requests


@dlt.resource(
    table_name="issues",
    write_disposition="merge",
    primary_key="id",
)
def get_issues(
    updated_at = dlt.sources.incremental("updated_at", initial_value="1970-01-01T00:00:00Z")
):
    # url to request dlt-hub issues
    url = f"https://api.github.com/repos/dlt-hub/dlt/issues?since={updated_at.last_value}"

    while True:
        response = requests.get(url)
        page_items = response.json()

        if len(page_items) == 0:
            break
        yield page_items

        if "next" not in response.links:
            break
        url = response.links["next"]["url"]


pipeline = dlt.pipeline(
	pipeline_name='github_issues_merge',
	destination='duckdb',
	dataset_name='mydata',
)
# dlt works with lists of dicts, so wrap data to the list
load_info = pipeline.run(get_issues)
print(load_info)
```

Above we add `primary_key` hint that tells `dlt` how to identify the issues in the database to find duplicates which content it will merge.

Note that we now track the `updated_at` field - so we filter in all issues **updated** since the last pipeline run (which also includes newly created ones).

Also pay attention how we use **since** [GitHub API](https://docs.github.com/en/rest/issues/issues?apiVersion=2022-11-28#list-repository-issues)
and `updated_at.last_value` to tell GitHub which issues we are interested in. `updated_at.last_value` holds the last `updated_at` value from the previous run.

Learn more:

- [You can do way more with merge.](../general-usage/incremental-loading#merge-incremental-loading)