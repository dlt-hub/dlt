---
title: Introduction
description: Introduction to dlt
keywords: [introduction, who, what, how]
---

# Introduction

![dlt pacman](/img/dlt-pacman.gif)

## What is `dlt`?


`dlt` is an open-source library that enables you to create a data
[pipeline](general-usage/glossary.md#pipeline) in a Python script. To use it, `pip install dlt` and
then `import dlt`. Once set up, it will automatically load any
[source](general-usage/glossary.md#source) (e.g. an API) into a live dataset stored in the
[destination](general-usage/glossary.md#destination) of your choice.

To try it out, install `dlt` with DuckDB using `pip install dlt[duckdb]` and run the following
example:

<!--SNIPSTART intro_snippet -->
```py
   import dlt
   import requests
   # Create a dlt pipeline that will load
   # chess player data to the DuckDB destination
   pipeline = dlt.pipeline(
       pipeline_name='chess_pipeline',
       destination='duckdb',
       dataset_name='player_data'
   )
   # Grab some player data from Chess.com API
   data = []
   for player in ['magnuscarlsen', 'rpragchess']:
       response = requests.get(f'https://api.chess.com/pub/player/{player}')
       response.raise_for_status()
       data.append(response.json())
   # Extract, normalize, and load the data
   info = pipeline.run(data, table_name='player')
   print(info)
```
<!--SNIPEND-->

[How it works](reference/explainers/how-dlt-works.md): `dlt` extracts data from a
[source](general-usage/glossary.md#source), inspects its structure to create a
[schema](general-usage/glossary.md#schema), structures, normalizes and verifies the data, and then
loads the data into a [destination](general-usage/glossary.md#destination).

## Who is `dlt` for?

### Why does the data team love `dlt`?

- Automated maintenance - with schema inference and evolution and alerts, and with short declarative
code, maintenance becomes simple.
- Run anywhere - Run on Airflow, serverless functions, anywhere that python goes data flows. No
special requirements, scales on micro and large infra alike.
- `dlt` offers a user-friendly, declarative interface that removes knowledge obstacles for beginners
while empowering senior professionals.

### By automating a large core of data engineering work, `dlt` solves different problems for different people

- For organisations: `dlt` unleashes a tidal wave of transformation around usage and curation of
datasets.
- For tech leads: `dlt` is a game changer that enables all your team quickly build "masterpiece-grade"
pipelines faster and more robust than they would build the usual throwaway code.
- For engineers, `dlt` is a groundbreaking force that streamlines their work, banishes errors,
notifies contract changes, leaving a trail of awe and amazement in its wake.
- For data scientists and analysts, `dlt` is the light on the path to data discovery, structuring and
usage.
- For data producers, `dlt` serves as an all-in-one "data contract and pipeline solution," that
enables delivering governed data.

Read more about use cases by persona here: [User guides](user-guides).

## Getting started with `dlt`

1. Play with the
[Google Colab demo](https://colab.research.google.com/drive/1NfSB1DpwbbHX9_t5vlalBTf13utwpMGx?usp=sharing).
This is the simplest way you could use `dlt`.
1. Read the docs on [building a pipeline, warehouse or lake with `dlt`](getting-started) or
[using the existing pipelines](dlt-ecosystem/verified-sources).
1. Send the [user guides](user-guides) to the rest of your team and discuss use cases.
1. Ask us on
[Slack](https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g)
if you have any questions about use cases or the library.

## Become part of the `dlt` community

1. Give the library a ‚≠ê and check out the code on [GitHub](https://github.com/dlt-hub/dlt).
1. Ask questions and share how you use the library on
[Slack](https://join.slack.com/t/dlthub-community/shared_invite/zt-1slox199h-HAE7EQoXmstkP_bTqal65g).
1. Report problems and make feature requests [here](https://github.com/dlt-hub/dlt/issues/new).